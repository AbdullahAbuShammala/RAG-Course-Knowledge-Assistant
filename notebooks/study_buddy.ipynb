{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zabo0\\miniconda3\\envs\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs found: ['Core Components In RAG.pdf', 'Data Ingestion And Parsing Techniques.pdf', 'intro_to_rag.pdf', 'Vector Embeddings And Vector Databases.pdf', 'Vector Stores Vs Vector Databases.pdf']\n",
      "\n",
      "Loaded 7 pages from Core Components In RAG.pdf\n",
      "First page metadata: {'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:24:26+03:00', 'source': 'pdf\\\\Core Components In RAG.pdf', 'file_path': 'pdf\\\\Core Components In RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:24:26+03:00', 'trapped': '', 'modDate': \"D:20251019202426+03'00'\", 'creationDate': \"D:20251019202426+03'00'\", 'page': 0}\n",
      "\n",
      "First 500 chars:\n",
      "Section 2: Core Components in RAG \n",
      "Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith \n",
      "Section Number: 2 \n",
      "Total Videos: 2 \n",
      "Date Created: 2024 \n",
      " \n",
      "Video 1: Data Ingestion and Parsing \n",
      "Video Order: 1/2 \n",
      "Topics: Document ingestion, Pre-processing, Chunking, Embeddings, Vector databases, \n",
      "Similarity search, Cosine/Euclidean distance \n",
      "Difficulty: Beginner \n",
      "Content \n",
      "Hello guys. \n",
      "So we are going to continue the discussion of Retrieval-Augmented Generation (RAG). In \n",
      "this specific vid\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "# Get first PDF file\n",
    "pdf_folder = \"pdf\"\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "print(f\"PDFs found: {pdf_files}\")\n",
    "\n",
    "# Load first PDF\n",
    "first_pdf_path = os.path.join(pdf_folder, pdf_files[0])\n",
    "loader = PyMuPDFLoader(first_pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"\\nLoaded {len(docs)} pages from {pdf_files[0]}\")\n",
    "print(f\"First page metadata: {docs[0].metadata}\")\n",
    "print(f\"\\nFirst 500 chars:\\n{docs[0].page_content[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 0}, page_content='Section 3: Data Ingestion and Data Parsing Techniques \\nCourse: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith \\nSection Number: 3 \\nTotal Videos: 9 \\nDate Created: 2024 \\n \\nVideo 1: Document Structure in LangChain \\nVideo Order: 1/9 \\nTopics: LangChain document structure, Page content vs. metadata, Document loaders, \\nText splitters, Project setup notes, Why metadata matters \\nDifficulty: Beginner \\nContent \\nHello guys. \\nSo we are going to continue the discussion with respect to data ingestion. Already in our \\nprevious video, we created our virtual environment and set up the project structure. \\nNow, in this video and in the upcoming series of videos, we are going to focus on data \\ningestion and parsing. \\nSince we are using LangChain for data ingestion, the data will arrive in different source \\nformats: PDFs, text files, Word documents, and more. I will try to show you multiple \\ntechniques to read these files with the recent LangChain version 0.3. \\nOne very important point: as soon as we read documents, LangChain expects us to \\nconvert that data into a Document structure. I’ve included a diagram in the resources \\nsection titled “LangChain Document Structure”—this is a very important concept. For \\nevery document we read and plan to store in a vector database, we include key \\ncomponents like page_content and metadata (the metadata is a dictionary). This is the \\nstructure we will follow: read the data → convert it to the Document structure → (later) \\nembed and store in a vector store. This step also lets us enrich content with additional \\nmetadata. \\nHere’s the idea: \\n• \\npage_content: a string containing the main text we want to embed and search. \\n• \\nmetadata: a dictionary with additional information (source, author, page, date, etc.) \\nthat helps retrieval and filtering.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 1}, page_content='There are many document loader techniques available in LangChain. We will focus on \\nboth custom and in-built techniques because parsing is a very important skill. \\nLet’s start. \\n \\nIntroduction to Data Ingestion \\nData ingestion means reading different kinds of data. For this, we’ll import some libraries. \\nIf a library like pandas is missing, install it (for example: uv add pandas or your chosen \\npackage manager). As we add libraries, your pyproject.toml (or requirements) will update. If \\nany code doesn’t work, double-check the installed versions vs. the versions I use—\\nmismatches can cause breaking changes over time. \\nNext, we’ll import the basic types (List, Dict, Any) and pandas, and then the Document \\nstructure and text splitters from LangChain 0.3. For splitters, we will look at: \\n• \\nRecursiveCharacterTextSplitter \\n• \\nCharacterTextSplitter \\n• \\nTokenTextSplitter \\nWe’ll use each and understand the differences as we go ahead. After setting this up, we’ll \\njust print a small message like “setup completed” so you know the environment is ready. \\n \\nUnderstanding the LangChain Document Structure \\nThis structure will play a very important role when you start building RAG pipelines. \\nInitially, whenever you read any data, you need to convert it into Document objects. From \\nthe diagram, there are two key parts: \\n• \\npage_content → the main text \\n• \\nmetadata → additional information about that text \\nLet’s create a simple Document conceptually (code will come later): \\n• \\npage_content: \"This is the main text content that will be embedded and searched.\" \\n• \\nmetadata: a dictionary like: \\no source: \"example.txt\" \\no page_number: 1'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 2}, page_content='o author: \"Krish\" \\no date_created: \"2024-01-01\" \\no (any other relevant fields you want) \\nIf you print the document, you’ll see both parts: \\n• \\nAccess content via doc.page_content \\n• \\nAccess metadata via doc.metadata \\nWhy metadata is crucial: \\n• \\nFiltering & search: restrict results to a source, author, section, page range, etc. \\n• \\nTracking sources: show the user where the answer came from. \\n• \\nProviding context in responses: include citation details or section names. \\n• \\nDebugging & auditing: trace how a result was produced. \\nWhen we store content with metadata in our vector database, we can answer additional \\nquestions like “Who is the author?” because that information travels with each chunk. For \\nadvanced RAG, metadata becomes even more powerful. \\n \\nFrom Loaders to Documents \\nWhen we use LangChain document loaders (for PDFs, text, Word docs, websites, etc.), \\nthe return type is Document (or a list of Documents). That means, after loading, you \\nalready have objects with page_content and metadata. You can inspect their types and \\nfields directly, and you’re ready for chunking and embedding next. \\nRemember the ingestion diagram: after loading a document, we typically pass it to a \\ndocument/text splitter to break it into chunks. We will use different splitters and compare \\nthem in practice.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 3}, page_content='Video 2: Ingesting and Parsing Text Data Using Document Loaders \\nVideo Order: 2/9 \\nTopics: Text files, TextLoader, DirectoryLoader, Document structure (page_content & \\nmetadata), UTF-8 encoding, glob patterns, pros/cons \\nDifficulty: Beginner \\nContent \\nHello guys. \\nSo we are going to continue the discussion with respect to RAG. Already in our previous \\nvideo, we have understood the entire document structure inside data ingestion. So first of \\nall, one type of data source files that we are going to read is a text file. \\nWhat we are going to do here is the simplest case: reading .txt files. You can refer to the \\nearlier diagram for the document data structure—how the page_content is created, how \\nmetadata is created. We also wrote some code and understood that there are two \\nimportant fields in each Document: \\n• \\npage_content \\n• \\nmetadata \\nDocument is a structure provided by LangChain. Anything that we read from supported \\ndata sources is converted into one or more Document objects, each with a page_content \\nand metadata. \\nLet me now show you how to read text files. This is the simplest case you’ll see. \\n \\nCreating Sample Text Files (Setup) \\nImagine some of your data is present inside .txt files. First, we’ll create a simple directory \\nand sample text files. \\n• \\nImport os. \\n• \\nUse os.makedirs() to create a directory inside the project where we’re doing data \\nparsing. \\n• \\nCreate a data/ folder, and inside it a text_files/ folder. \\n• \\nUse exist_ok=True so it won’t error if the folder already exists. \\nAfter creating the folders, we’ll create sample text files using a small dictionary of \\n{file_path: content} pairs, for example:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 4}, page_content='• \\ndata/text_files/python_intro.txt \\n• \\ndata/text_files/machine_learning_basics.txt \\nWe’ll iterate over the dictionary and, for each (file_path, content) pair: \\n• \\nopen(file_path, \"w\", encoding=\"utf-8\") and write the content. \\n• \\nPrint a message like “Sample text file created.” \\nAt the end, you should have two files in data/text_files/: \\n• \\npython_intro.txt \\n• \\nmachine_learning_basics.txt \\n \\nReading a Single Text File with TextLoader \\nThe first method is reading a single file using TextLoader. \\n• \\nImport TextLoader (commonly from langchain_community.document_loaders \\nimport TextLoader). \\n• \\nCreate a loader: loader = TextLoader(\"data/text_files/python_intro.txt\", \\nencoding=\"utf-8\"). \\n• \\nCall documents = loader.load(). \\nAs discussed, the return type is a list of Document objects. If you print type(documents), \\nyou’ll see it’s a list. Each item looks like: \\n• \\nDocument(page_content=..., metadata={\"source\": \\n\"data/text_files/python_intro.txt\"}) \\nSo even with the built-in loader, you get initial metadata automatically (e.g., source path). \\nYou can inspect the contents like this: \\n• \\nNumber of documents loaded \\n• \\nPreview first 100 characters: documents[0].page_content[:100] \\n• \\nShow metadata: documents[0].metadata \\nThis confirms that a .txt file read via TextLoader yields a list of Documents, each with \\npage_content and metadata.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 5}, page_content='Reading All Text Files in a Directory with DirectoryLoader \\nNow let’s read multiple files directly from a directory using DirectoryLoader. This is \\nsuitable when you have many text files to ingest. \\n• \\nImport DirectoryLoader from langchain_community.document_loaders. \\n• \\nCreate the loader: \\no directory_path = \"data/text_files\" \\no Use a glob pattern to match files, e.g., glob=\"**/*.txt\" (recursive pattern: any \\nsubfolder, any .txt file). \\no Set loader_cls=TextLoader to specify the loader for each matched file. \\no Use loader_kwargs={\"encoding\": \"utf-8\"} so each text file is read with UTF-8. \\no Optionally show_progress=True to visualize the loading progress. \\n• \\nCall documents = directory_loader.load(). \\nThen, iterate over the results and print details: \\n• \\nlen(documents) \\n• \\nFor each doc in documents: \\no doc.metadata.get(\"source\") \\no len(doc.page_content) (number of characters in the content) \\nYou’ll see output similar to: \\n• \\nDocument 1 → source=data/text_files/python_intro.txt, length=<chars> \\n• \\nDocument 2 → source=data/text_files/machine_learning_basics.txt, length=<chars> \\n \\nDirectoryLoader: Characteristics, Advantages, Disadvantages \\nAdvantages \\n• \\nLoad multiple files at once. \\n• \\nSupports glob patterns for flexible matching (**/*.txt). \\n• \\nProgress tracking with show_progress=True. \\n• \\nRecursive directory scanning.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 6}, page_content='Disadvantages \\n• \\nTypically assume files matching the glob are of the same type (e.g., all .txt) for a \\nsingle loader class. \\n• \\nLimited per-file error handling. \\n• \\nCan be memory intensive for very large directories (many or large text files) since it \\nscans and loads many files. \\n \\nSummary of What We Did \\n• \\nCreated sample text files programmatically. \\n• \\nRead a single text file via TextLoader → got a list[Document] with page_content and \\nmetadata. \\n• \\nRead multiple text files via DirectoryLoader using a glob pattern and TextLoader as \\nloader_cls. \\n• \\nSaw how metadata (like source) is populated automatically and why it’s helpful later \\nin retrieval and filtering. \\n• \\nDiscussed the pros/cons of DirectoryLoader.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 7}, page_content='Video 3: Text Splitting Techniques \\nVideo Order: 3/9 \\nTopics: Text splitters, CharacterTextSplitter, RecursiveCharacterTextSplitter, \\nTokenTextSplitter, Chunk size, Chunk overlap, Separators, Context window limits \\nDifficulty: Beginner \\nContent \\nHello guys. \\nSo we are going to continue the discussion with respect to our data ingestion pipeline. \\nAlready in our previous video, we have seen how to read a .txt file and convert it into a \\nDocument using the document loaders available in LangChain. \\nNow, let me go back to the RAG architecture diagram we saw earlier. In the data ingestion \\nphase, we may have different files: PDFs, web pages, text files, etc. After reading a file, the \\nreturn type is a Document (or list of Documents). Then we apply something called a \\ndocument splitter (also called a text splitter). The main goal is to split the document \\ninto chunks. \\nIn this video, I’m going to discuss some of the text splitting strategies available in \\nLangChain. As we go ahead, we’ll also look at more techniques when we move toward \\nadvanced RAG. \\nThe idea of text splitting is simple: we split the loaded documents into smaller chunks \\nbecause LLMs have context window limitations. Smaller, coherent chunks help retrieval \\nand improve downstream answer quality. \\n \\nImporting Text Splitters \\nWe will import the splitters from langchain.text_splitter (or the updated module path as per \\nLangChain 0.3): \\n• \\nCharacterTextSplitter \\n• \\nRecursiveCharacterTextSplitter \\n• \\nTokenTextSplitter \\nWe will print our loaded documents (list of Document). Then we’ll take one Document (e.g., \\ndocuments[0]) and access doc.page_content to apply the different splitters.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 8}, page_content='Method 1: CharacterTextSplitter \\nThe CharacterTextSplitter performs splitting based on characters using a specified \\nseparator. \\n• \\nSeparator: for example, a newline (\"\\\\n\"). \\n• \\nChunk size: e.g., 200 characters (max characters per chunk). \\n• \\nChunk overlap: e.g., 20 characters (the last 20 chars of one chunk repeat at the \\nstart of the next). \\n• \\nLength function: standard len. \\nWe create the splitter and call either split_text(text) (if we’re splitting a string) or \\nsplit_documents([doc]) (if we want Document-aware splitting). Here we’ll illustrate \\nsplit_text(text). \\n• \\nPrint len(char_chunks) to see how many chunks were created. \\n• \\nPrint a few chunks (e.g., char_chunks[0], char_chunks[1]) to inspect content. \\nIf the separator is a newline and our text contains natural line breaks, you may observe \\nclean splits with little to no visible overlap in the printed chunks (even if chunk_overlap is \\nset), because the splitter respects the separator boundaries. \\n \\nMethod 2: RecursiveCharacterTextSplitter (Recommended) \\nThe RecursiveCharacterTextSplitter is the most recommended general-purpose splitter. \\nIt tries multiple separators in order (e.g., \"\\\\n\\\\n\", \"\\\\n\", space, etc.) and recursively backs off \\nto smaller units when larger separators don’t fit the chunk size. \\n• \\nProvide a list of separators (e.g., [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]). \\n• \\nSet chunk_size=200, chunk_overlap=20, length_function=len. \\nCall split_text(text) and inspect results: \\n• \\nYou may see a different number of chunks (e.g., 6 instead of 4) compared to the \\nsimple character splitter, because the recursive method respects document \\nstructure and splits more intelligently. \\n• \\nIf your text has many clean breakpoints (headings, blank lines), you may not see the \\noverlap in the printed output, even though chunk_overlap is set; that’s just because \\nthe splits landed on clean boundaries.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 9}, page_content='To demonstrate overlap clearly, use text without natural breakpoints and restrict the \\nseparator list (e.g., only a space separator), with smaller chunk_size like 80 and \\nchunk_overlap=20. Then print consecutive chunks to see repeated phrases (e.g., “…and it \\nis also” appearing at the end of one chunk and the beginning of the next). \\n \\nMethod 3: TokenTextSplitter \\nThe TokenTextSplitter splits by tokens (not just raw characters). Depending on the \\ntokenizer, spaces and punctuation are accounted for in token counts. \\n• \\nExample settings: chunk_size=50 tokens, chunk_overlap=10 tokens. \\n• \\nCall split_text(text) and inspect the produced chunks. \\nThis method is often slower than character-based splitters but is useful when working with \\ntoken-limited models (you want chunks measured in tokens, not characters). \\n \\nMaking Overlap Visible (Examples) \\nIf you didn’t see overlap earlier, there are a few reasons: \\n• \\nChunk size and overlap may be relatively small. \\n• \\nYour separators caused clean splits (e.g., splitting at headings/blank lines). \\nTo force visible overlap: \\n• \\nReduce chunk_size. \\n• \\nIncrease chunk_overlap. \\n• \\nLimit separators (e.g., only spaces) so splits are more likely to cross mid-sentence. \\nThen print adjacent chunks to see repeated phrases across boundaries: \\n• \\nExample: chunk end “…is even longer than” and the next chunk starts with “…is even \\nlonger than…”. \\n \\nWhen to Use Which Splitter \\nCharacterTextSplitter \\n• \\nPros: Simple, predictable; good for structured text with clear delimiters.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 10}, page_content='• \\nCons: May break mid-sentence if separators are not aligned. \\n• \\nUse when: Your text has clear, consistent delimiters and you want fast, \\nstraightforward splitting. \\nRecursiveCharacterTextSplitter (default choice) \\n• \\nPros: Respects text structure; tries multiple separators; best general-purpose \\nsplitter. \\n• \\nCons: Slightly more complex; may take more time than simple character splits. \\n• \\nUse when: Default for most text; you want structure-aware splitting and good chunk \\nquality. \\nTokenTextSplitter \\n• \\nPros: Measures chunks by tokens; aligns with LLM token limits. \\n• \\nCons: Slower; depends on tokenizer behavior. \\n• \\nUse when: Working with strict token constraints or models where token budgeting \\nis critical. \\n \\nWhy Chunking Matters (Retrieval Perspective) \\nAt the end of ingestion, these chunks are embedded and stored in a vector database. \\nDuring retrieval, a query may match content spread across multiple overlapping chunks. \\nOverlap increases the chance that all relevant context is captured by the top-k retrieved \\nchunks. This leads to better grounding and answer quality in the generation phase.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 11}, page_content='Video 4: Ingestion and Parsing PDF Documents \\nVideo Order: 4/9 \\nTopics: PDF ingestion, PyPDFLoader, PyMuPDFLoader, UnstructuredPDFLoader (intro), \\nDocuments (page_content & metadata), Page-level info, Loader comparison \\nDifficulty: Beginner \\nContent \\nHello guys! \\nSuper excited that we have now completed at least one specific part wherein we are able to \\nread from a text document or text file, and then we are also able to split them into chunks. \\nRight now it’s time to go ahead and show you how we can load PDF files, because this is \\none of the most common use cases. Many companies have a huge amount of PDFs, so it is \\nnecessary that you know how to read a PDF with LangChain. \\nInside my data/ folder, I have uploaded a pdf/ folder, and there is a file named \\nattention.pdf. This specific PDF is a research paper. We will read this PDF, convert it into \\nDocument objects, and then see how we can apply splitting techniques over the extracted \\ntext. \\nFirst, I’ll create another notebook/file for this part, e.g., data_parsing_pdf.ipynb, because \\nPDF handling is a very common real-world requirement. \\n \\nLoading PDFs with LangChain (Approaches) \\nWe will import loaders from langchain_community.document_loaders and demonstrate \\nthree approaches (we will implement two now and introduce the third for later): \\n1. PyPDFLoader (often recommended, simple and reliable) \\n2. PyMuPDFLoader (fast, strong text extraction; supports images) \\n3. UnstructuredPDFLoader (we’ll cover later, useful for messy PDFs) \\n \\nMethod 1: PyPDFLoader \\nPyPDFLoader is a common and recommended way to read PDFs. \\n• \\nCreate a loader with the file path, e.g., data/pdf/attention.pdf. \\n• \\nCall .load() to get a list of Document objects, one per page (typically).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 12}, page_content='• \\nEach Document contains: \\no page_content → extracted text for that page \\no metadata → fields like page, source, creator, producer, author, timestamps, \\netc. \\nAfter loading, print diagnostics: \\n• \\nNumber of pages (e.g., 15 pages loaded) \\n• \\nPage 1 preview → first 100 characters \\n• \\nMetadata → show how much information is automatically captured \\nThis is an easy way to read a PDF and immediately get page-oriented Documents you can \\nlater split and embed. \\n \\nMethod 2: PyMuPDFLoader (fitz) \\nPyMuPDFLoader uses the pymupdf (fitz) engine under the hood. \\n• \\nYou may need to install pymupdf first. \\n• \\nCreate the loader with the same file, call .load(). \\n• \\nInspect the returned Documents and metadata. \\nWhy consider PyMuPDFLoader? \\n• \\nGenerally fast \\n• \\nOften robust text extraction \\n• \\nSupports image extraction and richer PDF features \\nIf you see import errors, install the dependency (e.g., pymupdf) and re-run. \\n \\n(Later) Method 3: UnstructuredPDFLoader \\nWe will discuss UnstructuredPDFLoader in a later video when we dive into cleaning and \\nadvanced PDF processing. It is helpful for: \\n• \\nComplex layouts \\n• \\nPDFs with images, tables, headers/footers'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 13}, page_content='• \\nNeeding heuristic segmentation before chunking \\n \\nQuick Look at the PDF Example \\nThe example PDF is the classic research paper “Attention Is All You Need.” Using the \\nloaders above, we obtain page-wise Documents. In the metadata, you’ll typically see fields \\nlike creator, producer, modDate, path, and the current page number. In the page_content, \\nyou’ll see the extracted text. This structure prepares the content for splitting and \\nembedding. \\n \\nLoader Comparison (Cheat Sheet) \\nPyPDFLoader \\n• \\nPros: Simple, reliable; good for most PDFs; preserves page numbers; basic text \\nextraction \\n• \\nCons: Can struggle with image-heavy or weirdly encoded PDFs \\n• \\nUse when: You want a straightforward loader that works well on standard PDFs \\nPyMuPDFLoader \\n• \\nPros: Fast; strong text extraction; image extraction support \\n• \\nCons: Requires pymupdf dependency; sometimes different extraction quirks \\n• \\nUse when: Speed matters or you need better extraction on tricky PDFs \\nUnstructuredPDFLoader (preview) \\n• \\nPros: Powerful heuristic parsing for messy layouts \\n• \\nCons: Heavier dependency stack; more configuration \\n• \\nUse when: You need advanced, layout-aware preprocessing before chunking'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 14}, page_content='Video 5: Handling Common PDF Issues \\nVideo Order: 5/9 \\nTopics: PDF parsing challenges, whitespace & newline cleanup, ligature fixes, OCR \\nmention, metadata enrichment, SmartPDFProcessor class, recursive splitting, empty-page \\nhandling, robust ingestion pipeline \\nDifficulty: Beginner → Intermediate \\nContent \\nHello guys. \\nSo we are going to continue our discussion with respect to data parsing for PDFs. Already \\nin our previous video, we discussed two important libraries for reading PDFs—\\nPyPDFLoader and PyMuPDFLoader—and we saw how to read a PDF file, extract details, \\nand inspect some of the metadata. \\nIn this video, we’re going to discuss common PDF challenges that you need to handle. The \\npurpose is simple: if your PDF has plain text, parsing is easy. But PDFs often store text in \\ncomplex ways (tables, columns, scanned images), include formatting artifacts (excess \\nwhitespace, broken lines, hyphenation), or contain special/encoded characters (like \\nligatures). We need to clean these issues so the downstream chunking, embedding, and \\nretrieval are effective. \\n \\nWhy PDFs Are Tricky \\n• \\nTables & multi-column layouts can scramble text order. \\n• \\nScanned PDFs require OCR to extract text. \\n• \\nExtraction artifacts: stray newlines (\\\\n), page headers/footers, hyphenations, \\nbroken words. \\n• \\nSpecial characters/ligatures: fi, fl, etc., due to font encodings. \\n• \\nWhitespace noise: multiple spaces or irregular spacing. \\nWe’ll look at a simple cleaning function to normalize raw text after extraction. \\n \\nExample: Raw vs Cleaned Text \\nSuppose a raw page contains something like:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 15}, page_content='\"Company Financial Report\\\\n\\\\n\\\\nThe financial performance for fiscal year 2024 shows \\nsignificant growth in profitability. Revenue increased by 25%. ...\" \\nwith lots of extra spaces/newlines and encoded ligatures. \\nA cleaning function can: \\n• \\nSplit on whitespace and rejoin with single spaces (normalizing spacing). \\n• \\nReplace ligatures like fi → fi, fl → fl. \\n• \\n(Later) Add more rules for hyphenation, headers/footers, etc. \\nAfter cleaning, the text becomes a single, readable paragraph with correct characters, \\nready for splitting and embedding. \\n \\nBuilding a Reusable Processor (Class Design) \\nTo make this robust, we’ll create a class called SmartPDFProcessor that handles PDF \\nloading, cleaning, smart chunking, and metadata enrichment. \\nConstructor (__init__) \\n• \\nParameters: chunk_size=1000, chunk_overlap=100. \\n• \\nInitializes a RecursiveCharacterTextSplitter with: \\no chunk_size \\no chunk_overlap \\no separators=[\" \"] (split primarily on spaces for consistent mid-sentence \\nhandling) \\nPrivate method: _clean_text(text: str) -> str \\n• \\nNormalizes whitespace (e.g., \\' \\'.join(text.split())). \\n• \\nReplaces common ligatures/encoded characters (e.g., fi→fi, fl→fl). \\n• \\nReturns cleaned text. (Extend later for OCR/table handling.) \\nPublic method: process_pdf(pdf_path: str) -> List[Document] \\n1. Load the PDF via PyPDFLoader(pdf_path).load() to get page-wise Documents. \\n2. Iterate pages with enumerate(pages).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 16}, page_content='3. Clean each page’s page_content with _clean_text. \\n4. Skip empty pages (e.g., if len(clean_text.strip()) < 50). \\n5. Create chunks with self.text_splitter.create_documents([clean_text], metadata=...). \\no Enrich each chunk’s metadata using the original page.metadata plus: \\n▪ \\npage_number \\n▪ \\ntotal_pages \\n▪ \\nchunk_method (e.g., \"recursive_space\") \\n▪ \\nchar_count \\n6. Append all per-page chunks to a processed_chunks list and return it. \\nThis design ensures that every chunk in your vector database carries useful metadata for \\nfiltering, tracing sources, and improving retrieval quality. \\n \\nRunning the Processor \\n• \\nInstantiate: processor = SmartPDFProcessor(). \\n• \\nCall: smart_chunks = processor.process_pdf(\"data/pdf/attention.pdf\"). \\n• \\nExpect a message like: Processed into 49 smart chunks (depends on your \\nchunking settings and the PDF content). \\n• \\nInspect metadata for a few chunks to confirm fields like page_number, total_pages, \\nchunk_method, source, etc., are present. \\n \\nWhy Metadata Matters \\nWhen these chunks are embedded and stored in your vector DB, metadata allows you to: \\n• \\nFilter by page_number, section, or source. \\n• \\nProvide citations (page and document). \\n• \\nImprove retrieval precision (e.g., restrict to particular sections/pages in follow-up \\nqueries). \\n• \\nDebug/audit the pipeline when results look off.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 17}, page_content='Video 6: Ingestion and Data Parsing Word Documents \\nVideo Order: 6/9 \\nTopics: Word documents, Docx2txtLoader, UnstructuredWordDocumentLoader, Elements \\nmode, Metadata, Chunking strategy \\nDifficulty: Beginner → Intermediate \\nContent \\nHello guys. \\nSo we are going to continue the discussion with respect to data parsing techniques. \\nAlready in our previous video, we finished the task of data parsing PDFs and saw which \\ncommon functionalities we can use to handle text data extracted from PDFs. We also \\nshowed how to create a class and how we’ll keep implementing each functionality as we go \\nahead. \\nAt the end of the day, we are still focusing on the main goal: read data from a data source, \\napply splitting, convert it into chunks, and while creating these chunks, add metadata. \\nLater, we will see the advantages of this metadata during retrieval. \\nIn this specific video, under data ingestion, we will create one more file (e.g., \\ndata_parsing_doc.ipynb) and focus on Word documents. Your problem statements may \\ninclude data in .docx format—project proposals, reports, HR policies, etc. Here’s an \\nexample: a project proposal for a RAG implementation. I’ve created a simple document \\nwith an executive summary, job responsibilities, and a few structured sections. We’ll \\nread this Word doc, extract content, add metadata, and (optionally) chunk it. \\nLet’s go step by step. \\n \\nWord Document Processing Overview \\nFor Word document processing, we’ll use two important loaders from \\nlangchain_community.document_loaders: \\n1. Docx2txtLoader — quick text extraction from .docx \\n2. UnstructuredWordDocumentLoader — structure-aware parsing using the \\nelements pipeline \\nYou may also see Python libraries like python-docx and docx2txt in the environment for \\nworking with .docx directly, but here we’ll focus on the LangChain loaders so the output is \\nalready in Document form (page_content + metadata).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 18}, page_content='Method 1: Docx2txtLoader (Simple Text Extraction) \\n• \\nCreate a Word doc in data/word_files/ named proposal.docx (e.g., a RAG \\nimplementation proposal with objectives, scope, milestones). \\n• \\nUse Docx2txtLoader to read it: \\no loader = Docx2txtLoader(\"data/word_files/proposal.docx\") \\no docs = loader.load() → returns a list[Document] \\n• \\nPrint diagnostics: \\no len(docs) → how many Document objects were returned (often 1 for simple \\ndocs) \\no docs[0].page_content[:100] → preview content \\no docs[0].metadata → see fields like source (file path), etc. \\nIf everything is fine, you should see content like: Executive Summary … and metadata \\nshowing the file path. This confirms the loader converted the Word file into a Document \\nwith content and metadata. \\n \\nMethod 2: UnstructuredWordDocumentLoader (Elements Mode) \\nFor more complex .docx files with headings, lists, tables, and mixed formatting, use \\nUnstructuredWordDocumentLoader. \\n• \\nInitialize with mode=\"elements\" to preserve structural elements: \\no un_loader = \\nUnstructuredWordDocumentLoader(\"data/word_files/proposal.docx\", \\nmode=\"elements\") \\no un_docs = un_loader.load() \\n• \\nThis may take longer on first run (heavier processing). The result is a list[Document] \\nwhere each item may correspond to an element: \\no Example elements: Title, NarrativeText, ListItem, Table, etc. \\n• \\nPrint diagnostics:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 19}, page_content='o len(un_docs) \\no For the first few elements, print doc.metadata and doc.page_content \\nYou might see output like “Loaded 20 elements” with metadata including source, \\nfile_directory, file_name, last_modified, languages, category, etc. The category reflects the \\nelement type (Title, NarrativeText, etc.). Tables are often parsed as separate elements—very \\nhandy for downstream processing. \\n \\nWhat About Chunking? \\nWhen mode=\"elements\" is used, the loader already segments the document into \\nmeaningful pieces. You can: \\n• \\nUse elements as-is as chunks (each element is small and coherent), or \\n• \\nFurther apply a text splitter (e.g., RecursiveCharacterTextSplitter) to long elements \\n(e.g., long NarrativeText) with settings like chunk_size=500, chunk_overlap=50, and \\ninject metadata such as section, category, element_index, source, etc. \\nThis two-level approach (elements → splitter) gives excellent control and often better \\nretrieval quality. \\n \\nExample Workflow (Putting It Together) \\n1. Load the .docx using one of the loaders above. \\n2. Inspect the returned Documents → check page_content and metadata. \\n3. Normalize/Clean the text if needed (whitespace, ligatures rare in .docx but can \\nnormalize newlines). \\n4. Chunk: \\no If using Docx2txtLoader, run a splitter over the full text. \\no If using UnstructuredWordDocumentLoader with mode=\"elements\", treat \\neach element as a base chunk and optionally split further. \\n5. Enrich Metadata for each chunk (e.g., source, category, heading, element_id, \\nchar_count). \\n6. Store the chunks in your vector database.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 20}, page_content='Notes on Performance & Trade-offs \\n• \\nDocx2txtLoader \\no Pros: Fast, simple, minimal dependencies \\no Cons: Loses structural context (headings, lists, tables) \\no Use when: You just need text quickly from clean .docx \\n• \\nUnstructuredWordDocumentLoader (elements) \\no Pros: Preserves structure; identifies element categories; captures tables as \\nseparate elements \\no Cons: Slower; heavier dependencies \\no Use when: You care about document structure, need table extraction, or \\nwant element-aware chunking \\n \\nMetadata Matters (again!) \\nAs with PDFs, rich metadata on Word-derived chunks improves retrieval and explainability: \\n• \\nFilter by category (Title vs NarrativeText vs Table) \\n• \\nTrack source and file_name for citations \\n• \\nUse heading or inferred section titles for section-aware retrieval \\n \\nWrap-up \\nIn this video, we learned how to ingest Word documents using Docx2txtLoader and \\nUnstructuredWordDocumentLoader. We saw how elements mode can yield multiple \\nDocument elements with structural metadata, which is great for high-quality chunking. \\nFrom here, you can apply splitters as needed, enrich metadata, embed, and store in your \\nvector DB. \\nIn the next video, we’ll discuss CSV and Excel files—how to read them and prepare the \\ncontent for splitting and embeddings. \\nThank you. Take care.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 21}, page_content='Section 3: Data Ingestion and Data Parsing Techniques \\nCourse: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith \\nSection Number: 3 \\nTotal Videos: TBD \\nDate Created: 2024 \\n \\nVideo 7: Parsing CSV and Excel Files \\nVideo Order: 7/9 \\nTopics: Structured data, CSVLoader, UnstructuredCSVLoader, Pandas, Excel processing, \\nUnstructuredExcelLoader, Custom CSV processing, Metadata enrichment \\nDifficulty: Beginner → Intermediate \\nContent \\nHello guys. \\nSo we are going to continue the discussion with respect to data parsing and data \\ningestion. Already in our previous video, we have seen how to parse Word docs. And \\nwhenever we talk about all the previous parsing techniques that we have learnt, it is mostly \\nfor unstructured data, right? So in this particular video, we are going to go ahead and see \\nhow we can ingest and parse structured data like CSV and Excel files. \\nOkay. \\nSo with respect to this, the first thing is that I will just go ahead and quickly create one more \\nfile. So let’s say here I will go ahead and write csv_excel_parsing.ipynb. And this will \\nbasically be my fourth file. Okay I will go ahead and select the kernel quickly. I will create a \\nmarkdown and let me just go ahead and write CSV and Excel files—right—and this is \\nnothing but my structured data. \\nSee, the main aim of creating this is that I really want to show you each and every \\ntechnique of parsing from different data sources. Okay. So that tomorrow, if you’re working \\non anything—right—if there is a CSV file, if there is some kind of Excel file, you should know \\nwhat you’re actually doing or how to probably go ahead and ingest that and do the normal \\nparsing and convert that into a document structure. \\nOkay. \\nSo the first thing that I’m going to import is nothing but, since we are going to work with CSV \\nor Excel file, I think pandas will definitely be required. So I will go ahead and install \\npandas. Okay. So right now pandas is not there. But let’s see whether LangChain is also \\ncoming or not. Okay.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 22}, page_content='So this is the problem that I’m facing. And you may also face whenever you create a new \\nfile—right—at that point of time, the libraries will not get loaded and you will not be able to \\nquickly get all the suggestion, like how we used to get it over here. Okay, so for this you just \\nneed to go ahead and restart it once again. Restart your kernel completely. Okay. And once \\nyou restart your kernel I think you should be able to see it again. \\nOkay. \\nSo first of all what I will do, I will quickly go ahead and see whether in my requirements.txt I \\nhave pandas or not. So here pandas is not there. I will go ahead and import pandas. \\nPandas. So here what I will do I will quickly write uv add -r requirements.txt. Okay, so \\npandas has been imported already I see okay somewhere pandas may be there and now I \\nwill quickly go ahead and write import pandas. Okay, so let’s see. This is executing perfectly \\nfine. \\nOkay. \\nSo with respect to this, if you know how to use pandas, I think that will be more than \\nsufficient in order to start, right. One more thing. What I will do, I will just go ahead to my \\nfolder location where I’m writing the code. I will again open up VS Code. Okay, I’ll open the \\nsame project in VS Code. There is a reason why I’m doing it and I’ll just let you know in \\nsome time. Okay? Why I am actually doing that right? So first of all what I will do, I will just \\nclose this. I will just close—or let me just close it quickly from here, okay. And then I will \\njust go ahead and open my VS Code. \\nNow you’ll be able to see that here. I will go ahead and select my kernel. So all the kernels \\nwill be getting loaded quickly. Yeah perfect. So I have loaded my kernel. Now you can see \\nthat my pandas is basically coming. See whenever things are not getting loaded, just try to \\nclose your VS Code or try to restart your kernel. I restarted my kernel. That thing also did not \\nwork. So with respect to every video, you know, I definitely will tell you some or the other \\nthings over here. So don’t get bored with respect to that, okay? \\nThen I’m going to go ahead and import the os because this kind of suggestions I will not be \\ngetting like—when I say suggestions, the highlighting will not be coming up. And if \\nhighlighting is not coming up like you’ll not understand where you’re making a spelling \\nmistakes and all. And then later on in the debugging section you’ll be doing all those things. \\nOkay. \\nNow quickly what I will do, I will go ahead and make one directory. So since we are working \\nwith CSV and Excel file, what I will do first task is basically to create these files. Okay. Then \\nI will show you how you can go ahead and read that file and do the parsing also. So first of \\nall what I’m doing I’m creating a structured_files folder inside my data folder. So I will just'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 23}, page_content='go ahead and execute it. So if you see over here inside my data there is a structured_files \\nfolder, okay. \\nNow inside this folder I will go ahead and create a simple DataFrame. Okay. So if you know \\npandas simple DataFrame: see product—this is my column name. These are my data that \\nis present inside the column. Another column—these are my data; price—these are my \\ndata; and description—these are my data, right. And this I’m trying to convert into a \\nDataFrame, and I’m saving that DataFrame in a file which is called as products.csv. \\nSo if you know about at least some of the data pre-processing techniques, if you have used \\nit, I think this is the most simple way of creating a CSV file. Okay. So I’ll quickly go ahead \\nwith this because these are some basic things which you should know in Python. Okay. \\nNow inside my data folder, if I go ahead and see there is a structured_files folder and there \\nis a CSV file, and this is how my CSV file looks like. So one CSV file we have actually created \\nit. And now similarly I will go ahead and create one Excel file also. Okay. \\nNow for Excel file I will again take some data and probably print it in over here. So let’s say \\nthis is one important functionality which is called as ExcelWriter inside pandas. So here is \\nmy inventory.xlsx as a writer. And I’m trying to convert this into Excel. I’m using a \\nsheet_name=\"products\" and index=False. So here is my entire summary data which I’m \\ngoing to put inside this particular Excel file. And then I’m taking the summary data, \\nconverting into a DataFrame and finally converting into an Excel with this particular sheet \\nname and writer and index=False where I don’t take my separate column name. Right. \\nWhatever column is specifically given over here that only it will be taken. \\nSo this code actually helps you to create some kind of Excel file over here. So if you just go \\nahead and execute this. So here, No module named openpyxl is coming. So what I will do, \\nI will just go ahead and write it: openpyxl. So this is a library that we are going to specifically \\nuse it. So I will open my command prompt, uv add -r requirements.txt. Okay. So here you \\ncan see openpyxl (and et-xmlfile) is got loaded. Right. So I will go to this and again execute \\nthis. Now I think it should work. \\nSo at the end of the day you will be able to see that along with my CSV file there will also be \\nan XLSX file—right—Excel file. So two important files I have created: products.csv and \\ninventory.xlsx. Very simple. I’ve just used pandas. One function is pd.ExcelWriter where we \\nhave to just give the sheet name and I will be able to create it. Right. And these are my \\ninformation that is present over here. \\nNow let’s start working on CSV processing. Right. So here we are going to specifically go \\nahead and work on how, if we are reading a CSV file, how should we go ahead and read it? \\nAnd how should we basically convert that into a Document? Okay.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 24}, page_content='Again, from LangChain there is an inbuilt function which is called as CSVLoader. So I will \\ngo ahead and import from langchain_community.document_loaders import CSVLoader. \\nOkay. And then from langchain_community.document_loaders import \\nUnstructuredCSVLoader. So these two specific libraries we are going to use in order to load \\na CSV file. Okay. \\nSo first path is nothing but CSVLoader. And we will just try to see how to work with the \\nmethod one. So in the method one you can see CSVLoader (row-based Documents). I’m \\nusing the CSVLoader; I have to give my path for the CSV file. We have to use encoding as \\nUTF-8 and here additional you can give delimiter and quotechar. So delimiter I’m actually \\ngiving , since it is CSV (comma-separated file). We can give this. Quote character is nothing \\nbut quotes \". \\nThen we are using csv_loader.load() and I’m printing all the information like what are my \\ndocuments and all. If you want I can also go ahead and print all the CSV documents. Okay \\ncsv_docs. So let’s go ahead and print this. So here you can see this is one Document \\nmetadata—all the information. page_content—you can see everything is available inside \\nthat page content. Right. Yeah one. Then again my another Document metadata \\nautomatically I’m able to read it. So based on row by row—I think it is row by row—yeah. \\nSee row=2, row=1. All this information is here. \\nSo row=2 is one Document. Okay. So one Document. So if you see this there are total how \\nmany rows? There are five rows. One, two, three, four, five. Right. So (0, 1, 2, 3, 4). Right. \\nAnd if you go ahead and see over here that many number of Documents you’ll be able to \\nfind out. Right. And here you can see row=4 and all the information—\"1080p with noise \\ncancellation\"—is there. Here you can see 1080p with noise cancellation: these are my \\ninformation of the columns. Quickly you can see that once we are reading this, \\nautomatically row by row it is converting into Documents. Okay. And the first Document \\nis this information. The metadata information is also here. And here you can see, since it is \\nreading a CSV file, automatically this function is so good that it is adding a metadata as \\nrow=0. \\nSee, at the end of the day, you can also go ahead and create your own custom Document \\nwith additional metadata information. So this is important that you should know how to \\nread something and how you can go ahead and customize this. I can also do this \\ncustomization. Right. At the end of the day, I’m getting all the format in the Document. I can \\ngo ahead and add more metadata if I want, which I’ve already shown you before also, \\nright, with respect to different different things.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 25}, page_content='So this was one. The second technique that you can specifically use is one more \\nUnstructuredCSVLoader. Okay. So UnstructuredCSVLoader—what we can actually do \\nand how we can actually read it—that is what we are basically going to discuss now. Okay. \\nNow quickly, let’s do one thing. Let’s try to see that before going to unstructured, you may \\nbe thinking that, \"Krish, can we add more additional metadata information and all?\" So for \\nthat what I will do, I will create a custom CSV processing. Right. And let’s see this. Okay. \\nSo I’m going to copy and paste this code—see Custom CSV Processing. So for this I will be \\nrequiring List. So I will go ahead and write from typing import List. And then I will also go \\nahead and import my Document. See I don’t need to by-heart everything like where exactly \\nit is or not. Okay. But it is good to know all these things. But it’s okay—with practice you will \\nget to know from where you are importing all the things, right? So from typing import List \\nand this one. \\nSee here what we are trying to do is that we are creating our custom CSV processing for \\nbetter control. So here we have created a function process_csv_intelligently(file_path) -> \\nList[Document]. I’ll just give the file path and this will return a list of Documents. Okay. So \\nit is going to return a list of Documents where we are also going to do the chunking—each \\nand everything—okay. Automatically the chunking, parsing along with the metadata \\ninformation in each and everything. \\nSo first of all I’m reading the CSV file path. I’ve created an empty documents. Strategy 1 is \\n“one Document per row with structured content.” So what I will do, I will iterate through \\nevery row. And for that we use df.iterrows(). Okay, now what we are basically going to do is \\nthat I’m providing some of the content information over here. Right? This is my content \\ninformation. Now see next step is that I’m creating the Document. So for the Document I \\nwill be using this content. This content is coming from where? See it is coming from this \\ninformation—right—row by row. All this specific information I’m going to put inside this \\ncontent. So this becomes my page_content. \\nThen metadata. So metadata I have my file_path. If you see file path is here. Then \\nrow_index from this index. You can get row_product, row_category, row_price and \\nproduct_info. The same thing this CSVLoader is actually doing. And here you can see that \\nhow easily we have actually written our own custom function for the CSV processing. And \\nthat is how you just go ahead and, you know, create your own information with respect to \\nthis write-up. When you want some more additional information, you can just go ahead and \\nwrite it. \\nOkay.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 26}, page_content='Now I definitely don’t want to stop it over here—like you can do it from your own. Right. So \\nonce I execute this, you will be able to see that this CSV processing is there. Now, if I just \\ncall this function and if I give my path—my path, what path it is? Let’s see this path. So let’s \\nsay I give this path my CSV path, okay, and I give it over here. Now what output I’m actually \\ngoing to get? Okay. So I’ll just go ahead and write return documents. Okay. \\nNow, if I go ahead and execute this so you can see over here all the information is over here. \\nrow=0, row=1, row=2, row=3, row=4—all the information with respect to this is there, okay. \\nSo I hope you got an idea about how you can do the custom CSV processing. At the end of \\nthe day, this is what you really want to create and how you want to create it. As you \\nunderstand multiple use cases, you’ll get to know about it. \\nOkay, now there may also be one more scenario, where I also want to probably go ahead \\nand create a summary, something like that. Right. That I will show you as we go ahead. But \\njust to make sure to show you the two differences like CSV processing strategies that we \\nhave discussed till now—you know: row-based and whether you should go ahead with this \\ncustom one—I’ll just write the differences so that you get to know about this. Okay. \\nSo here—CSV Processing Strategies: \\n• \\nRow-based (CSVLoader) → Simple: one row → one Document; good for record \\nlookups; loses table context. \\n• \\nIntelligent/custom processing → Preserves relationships, create summaries, rich \\nmetadata, better for Q&A. Because here you can add more context, more \\ninformation. In short, when you’re adding more context, here you can also say that, \\n\"Hey, does this CSV have some other relationship with some other CSV file also?\" \\nRight. So that context can be put up in this metadata information. So based on the \\nuse cases, you think whether you need to probably go ahead with using CSVLoader \\nor intelligent processing. \\nOkay. \\nNow coming to the Excel processing—again, guys, when we say coding, we should not \\nrestrict ourself in one thing. I should definitely show you multiple ways how to do this. \\nSo now let’s go ahead with Excel processing. For Excel processing I will show you one \\ntechnique. So here you can just go ahead and see this, okay. And you’ll be able to \\nunderstand it anyway. \\nMethod 1: Using pandas for full control \\n• \\nCreate a function process_excel_with_pandas(file_path) -> List[Document].'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 27}, page_content='• \\nRead the Excel file with pd.ExcelFile(file_path). \\n• \\nFor sheet_name in excel.sheets (or excel.sheet_names), load each DataFrame. \\n• \\nBuild a sheet_content string representing the table (you can format as CSV/TSV or \\nMarkdown if you want). \\n• \\nCreate a Document with page_content=sheet_content and metadata including: \\nsource, sheet_name, n_rows, n_cols. \\n• \\nAppend all to a documents list and return. \\nOnce I execute this quickly I can just go ahead and print things. Okay. So here I’m giving my \\nexcel_docs, len(excel_docs), each and everything. So here you can see two sheets have \\nbeen processed. So if you go ahead and see my excel_docs this is what is the information \\nthat I have: inventory, product summary—each and every information with respect to this, \\nokay. So it is just reading from the inventory file. \\nMethod 2: UnstructuredExcelLoader \\n• \\nYou can also use UnstructuredExcelLoader whenever you want. \\n• \\nImport: from langchain_community.document_loaders import \\nUnstructuredExcelLoader. \\n• \\nCreate the loader with the file path; call .load(). \\n• \\nAs with other unstructured loaders, this may take time on larger or complex \\nworkbooks. \\n• \\nAdvantages: handles complex Excel features, preserves formatting info. \\n• \\nDisadvantages: requires the unstructured library; slower.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 28}, page_content='Video 8: JSON Files Parsing and Processing \\nVideo Order: 8/9 \\nTopics: JSON ingestion, JSONLoader with jq_schema, Custom JSON parsing, Document \\nstructure, Metadata enrichment, API JSON handling \\nDifficulty: Beginner → Intermediate \\nContent \\nHello guys. \\nSo in this specific video we are going to discuss about JSON parsing and processing. Let’s \\nsay you have JSON data—how do we ingest it, parse it, process it, and finally convert it into \\na Document structure? There may be scenarios where you are communicating with APIs \\nand the responses are in JSON. This becomes a very important use case, and many \\npractical pipelines will involve JSON. \\nFirst, we’ll create some JSON files to work with. \\n \\nCreating Sample JSON Files \\nWe’ll start by importing json and os, and creating a directory data/json_files/ (using \\nos.makedirs(..., exist_ok=True)). \\nFile 1: company_data.json \\nA nested JSON with a structure like: \\n• \\ncompany: \"TechCorp\" \\n• \\nemployees: a list of employee objects (each with id, name, role, skills, projects) \\n• \\nprojects: list of project summaries \\n• \\ndepartments: list of department names \\nWe’ll json.dump(...) this json_data to company_data.json. \\nFile 2: events.json \\nAnother JSON containing a list of event logs with fields like timestamp, event, user_id, \\npage, etc. We’ll save this to events.json. \\nAfter this step, you should have: \\n• \\ndata/json_files/company_data.json \\n• \\ndata/json_files/events.json'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 29}, page_content='Method 1: Using LangChain’s JSONLoader (with jq_schema) \\nWe’ll import JSONLoader from langchain_community.document_loaders. \\nSometimes you don’t want the entire JSON—just a specific subtree (e.g., each employee). \\nJSONLoader supports a jq_schema parameter: a jq expression to select parts of the JSON. \\nExample: Extract each employee object as a separate Document. \\n• \\nInitialize: \\nemployee_loader = JSONLoader( file_path=\"data/json_files/company_data.json\", \\njq_schema=\".employees[]\", text_content=False ) \\n• \\nCall: employee_docs = employee_loader.load() \\nNotes: \\n• \\njq_schema=\".employees[]\" traverses into employees and emits each element. \\n• \\ntext_content=False returns the full JSON object in page_content (serialized), not \\njust plain text. \\n• \\nYou may need to install jq support (the Python package used by the loader); ensure \\nit’s in your environment. \\nResult: \\n• \\nYou’ll see something like: “Loaded 2 employee documents.” \\n• \\nEach Document has: \\no page_content: the serialized JSON for one employee \\no metadata: includes source and other fields \\nThis is perfect when you want one vector per employee for retrieval. \\n \\nMethod 2: Custom JSON Processing (Full Control) \\nWhile the loader is convenient, sometimes you need more control. Let’s build a small \\ncustom function that reads the JSON and emits Documents with rich metadata. \\nFunction sketch: process_company_json(file_path) -> List[Document] \\n1. with open(file_path) as f: data = json.load(f)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 30}, page_content='2. Iterate over data.get(\"employees\", []). \\n3. For each employee, construct a content string (or keep JSON) that summarizes key \\nfields: id, name, role, skills, and a list of projects. \\n4. Create a Document: \\no page_content: the formatted text (or json.dumps(emp, ensure_ascii=False)) \\no metadata: { \"source\": file_path, \"record_type\": \"employee\", \"employee_id\": \\nemp[\"id\"], \"name\": emp[\"name\"], \"projects\": [..], \"skills_count\": \\nlen(emp.get(\"skills\", [])) } \\n5. Append to documents and return the list. \\nThis approach lets you normalize text, rename fields, filter sensitive keys, and inject \\ndomain-specific metadata to improve retrieval, filtering, and analytics later. \\n \\nApplying to Event Logs (Assignment-style Tip) \\nFor events.json, follow a similar pattern: \\n• \\nIterate events, create one Document per event (or per user, grouped). \\n• \\nMetadata could include: timestamp, event, user_id, page, session_id, etc. \\n• \\nConsider time bucketing (e.g., day/week) added as a metadata field for temporal \\nfiltering. \\n• \\nFor groupings, you might aggregate events by user_id or session_id and create one \\nDocument per group with summarized page_content. \\n \\nDocument Structure Recap \\nRegardless of the method, the goal is to convert raw JSON into a consistent Document \\nformat that your RAG pipeline expects: \\n• \\npage_content: the text that will be embedded (either pretty-printed JSON or a \\nhuman-readable summary) \\n• \\nmetadata: key fields for filtering, citations, and tracing (ids, types, timestamps, \\nsource paths)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 31}, page_content='Once you have Documents, you can split (if long), embed, and store them in your vector \\ndatabase. \\n \\nWhen to Use Which Approach? \\n• \\nJSONLoader + jq_schema: Fast, declarative extraction when your target subtree is \\nclear (e.g., employees, orders, items). Great for 1-to-1 record → Document. \\n• \\nCustom processing: When you need transformations, redaction, normalization, \\nrollups/summaries, cross-record context, or custom metadata fields. \\nOften, a hybrid works best: use jq_schema to isolate the right records, then post-process \\neach record into your preferred text/metadata format.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 32}, page_content='Video 9: SQL Databases Parsing and Processing \\nVideo Order: 9/9 \\nTopics: SQLite setup, Creating tables, Inserting records, LangChain SQLDatabase & \\nSQLDatabaseLoader, Converting SQL to Documents, Metadata enrichment, Table joins \\nDifficulty: Beginner → Intermediate \\nContent \\nHello guys. \\nSo we are going to continue the discussion with respect to pre-processing and parsing. \\nAlready in our previous video, we completed JSON pre-processing strategies. We saw \\nmultiple examples—how to create JSON files and data, how to read JSON, and finally \\nconvert it into a Document along with metadata. We also saw how to do custom JSON \\npre-processing to create the Document structure itself. Right. So all these specific things \\nhave been discussed. \\nNow one more problem statement that we are going to work on. See, in data ingestion and \\nparsing, what are we trying to do? If we have a PDF file, a Word doc, a TXT file, a JSON file—\\nwe learned techniques to read those files, extract text/data, and convert it into a \\nDocument structure. In the Document structure, you have two important things: content \\nand metadata. And after this, further you can divide into chunks. Sometimes the inbuilt \\nfunctions you use will also internally divide the data into chunks. \\nIn this particular video, what we will be doing is specifically working with a SQL database \\nas a data source. Because whenever we talk about SQL databases, we are basically talking \\nabout structured data. (If I talk about a NoSQL database like MongoDB, in short we are \\ntalking about JSON—key-value pairs.) From a SQL database, how can we read the data and \\nconvert it into a Document structure? That is what we will see. \\n \\nCreating a Sample SQLite Database \\nFor learning, I’ll make it simple using SQLite. \\n1. Imports & setup \\n• \\nimport sqlite3 \\n• \\nimport os \\n• \\nCreate directory: data/databases/ using os.makedirs(..., exist_ok=True). \\n2. Create database file'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 33}, page_content='• \\nPath: data/databases/company.db \\n• \\nConnection: sqlite3.connect(path) \\n• \\nCursor: con.cursor() \\n3. Create tables \\n• \\nemployees: (id INTEGER PRIMARY KEY, name TEXT, role TEXT, department TEXT, \\nsalary REAL) \\n• \\nprojects: (id INTEGER PRIMARY KEY, name TEXT, status TEXT, budget REAL, lead_id \\nINTEGER) \\n4. Insert sample data \\n• \\nEmployees (list of tuples): \\no (1, \"John Doe\", \"Senior Developer\", \"Engineering\", 120000.0) \\no (2, \"Jane Smith\", \"Data Scientist\", \"AI\", 135000.0) \\no (3, \"Mike Johnson\", \"Project Manager\", \"PMO\", 110000.0) \\no (4, \"Sarah Williams\", \"QA Engineer\", \"Quality\", 90000.0) \\n• \\nProjects (list of tuples): \\no (101, \"RAG System\", \"In Progress\", 250000.0, 1) \\no (102, \"Data Platform\", \"Completed\", 400000.0, 2) \\no (103, \"Mobile App\", \"In Progress\", 180000.0, 3) \\no (104, \"Testing Revamp\", \"Planned\", 120000.0, 2) \\nUse cursor.executemany(...) to insert, con.commit() to save, and close the connection. \\n \\nDatabase Content Exploration (LangChain Utilities) \\nWe’ll use LangChain community utilities: \\n• \\nfrom langchain_community.utilities import SQLDatabase \\n• \\nfrom langchain_community.document_loaders import SQLDatabaseLoader \\nMethod 1: SQLDatabase utility \\n• \\nInitialize: db = SQLDatabase.from_uri(\"sqlite:///data/databases/company.db\")'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 34}, page_content='• \\nExplore: \\no db.get_usable_table_names() → e.g., {employees, projects} \\no db.get_table_info() → schema DDL + sample rows \\nThis quickly confirms the tables, schema, and a few rows. \\n \\nCustom SQL → Document Conversion (with Metadata) \\nOur main aim is reading from the database and turning it into Documents. \\nWe’ll write a helper: \\nsql_to_documents(db_path: str) -> List[Document] \\nSteps: \\n1. con = sqlite3.connect(db_path); cur = con.cursor() \\n2. List tables: cur.execute(\"SELECT name FROM sqlite_master WHERE type=\\'table\\'\"); \\ntables = cur.fetchall() \\n3. For each table: \\no Column info: PRAGMA table_info(table_name) → column names \\no Rows: SELECT * FROM table_name → fetch all \\no Build a page_content string summarizing table name, columns, total \\nrecords, and include a few sample rows for context \\no Create a Document: \\n▪ \\npage_content: assembled summary + samples \\n▪ \\nmetadata: { \"source\": db_path, \"table\": table_name, \"record_count\": \\nlen(rows), \"type\": \"sql_table\" } \\no Append to documents \\n4. (Optional) Relationships: run a join to capture cross-table context, e.g.: \\n5. SELECT e.name AS employee_name, e.role, p.name AS project_name, p.status \\n6. FROM employees e \\n7. JOIN projects p ON e.id = p.lead_id;'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-19T20:54:21+03:00', 'source': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'file_path': 'pdf\\\\Data Ingestion And Parsing Techniques.pdf', 'total_pages': 36, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:54:21+03:00', 'trapped': '', 'modDate': \"D:20251019205421+03'00'\", 'creationDate': \"D:20251019205421+03'00'\", 'page': 35}, page_content='Convert the result into a human-readable relationship Document with metadata like { \\n\"source\": db_path, \"type\": \"employee_project_join\" }. \\n8. Return the documents list. \\nWhen you call it with \"data/databases/company.db\", you’ll see a couple of Documents—\\none per table—and an extra Document for the join. Each Document’s page_content \\nincludes table summaries and sample rows. From here, you can apply a text splitter (e.g., \\nRecursiveCharacterTextSplitter) to produce chunked Documents ready for embeddings. \\n \\nWhy Metadata Matters (again) \\nAdding fields like table, record_count, type, and source enables: \\n• \\nFiltering answers by table or record type \\n• \\nBetter traceability/citations back to the DB \\n• \\nTargeted retrieval (e.g., prefer join-derived context for cross-entity questions) \\n \\nWrap-up \\nIn this video, we: \\n• \\nCreated a SQLite database with employees and projects \\n• \\nInserted sample data \\n• \\nUsed LangChain’s SQL utilities to inspect schema & rows \\n• \\nBuilt a custom SQL → Document pipeline, including a join-based relationship \\nDocument \\n• \\nPrepared the output for chunking → embeddings → vector store \\nThis is a foundational pattern you can adapt to Postgres/MySQL/etc. by switching the DB \\nURI and SQL queries. In the next video, we’ll continue expanding ingestion strategies and \\nwire everything into the embedding/vector database steps. \\nThank you. Take care.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "\n",
      "Page 1 - Last 200 chars:\n",
      "’ll break \n",
      "down what data we ingest, how we clean and split it, how we embed it into vectors, and \n",
      "how we store it in a vector database so that later phases (query + generation) can work \n",
      "effectively.\n",
      "\n",
      "Page 1 - First 200 chars:\n",
      "Section 2: Core Components in RAG \n",
      "Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith \n",
      "Section Number: 2 \n",
      "Total Videos: 2 \n",
      "Date Created: 2024 \n",
      " \n",
      "Video 1: Data Ingestion and Parsing\n",
      "------------------------------\n",
      "\n",
      "Page 2 - Last 200 chars:\n",
      "ine. \n",
      " \n",
      "The Ingestion Pipeline: Step by Step \n",
      "Step 1: Load the Documents \n",
      "Use appropriate loaders per file type to extract text and attach preliminary metadata (e.g., \n",
      "source, filename, section, url).\n",
      "\n",
      "Page 2 - First 200 chars:\n",
      "What is Document Ingestion and Pre-processing? \n",
      "To power the retriever, we first need a vector database filled with vectors that represent \n",
      "our knowledge. This knowledge can come from multiple sources\n",
      "------------------------------\n",
      "\n",
      "Page 3 - Last 200 chars:\n",
      "maDB \n",
      "• \n",
      "FAISS (library) \n",
      "• \n",
      "Pinecone (managed) \n",
      "• \n",
      "DataStax / Cassandra with vector support \n",
      "This database supports similarity search so we can find the most relevant chunks for a \n",
      "given query later.\n",
      "\n",
      "Page 3 - First 200 chars:\n",
      "Step 2: Pre-process the Text \n",
      "Light cleaning (remove boilerplate, headers/footers if noisy, fix line breaks, normalize \n",
      "whitespace, optionally remove very long tables or binary debris) so the chunks a\n",
      "------------------------------\n",
      "\n",
      "✗ Video 2 not in first page\n"
     ]
    }
   ],
   "source": [
    "# Check how content flows across pages\n",
    "print(\"=\"*50)\n",
    "for i in range(min(3, len(docs))):  # First 3 pages\n",
    "    print(f\"\\nPage {i+1} - Last 200 chars:\")\n",
    "    print(docs[i].page_content[-200:])\n",
    "    print(f\"\\nPage {i+1} - First 200 chars:\")\n",
    "    print(docs[i].page_content[:200])\n",
    "    print(\"-\"*30)\n",
    "\n",
    "# Look for video boundaries\n",
    "full_text = docs[0].page_content\n",
    "if \"Video 2:\" in full_text or \"Video Order: 2\" in full_text:\n",
    "    print(\"\\n✓ Multiple videos found in same page\")\n",
    "else:\n",
    "    print(\"\\n✗ Video 2 not in first page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for video markers across all pages...\n",
      "\n",
      "Page 1: Found video marker\n",
      "  Video 1: Data Ingestion and Parsing \n",
      "  Video Order: 1/2 \n",
      "Page 5: Found video marker\n",
      "  Video 2: Query Processing and Output Generation Phase \n",
      "  Video Order: 2/2 \n",
      "\n",
      "Total 'Video Order:' found: 2\n",
      "\n",
      "Video 2 found at character position: 5882\n",
      "Context around Video 2:\n",
      "nto the LLM for the Generation Phase. \n",
      "Thank you! Video 2: Query Processing and Output Generation Phase \n",
      "Video Order: 2/2 \n",
      "Topics: Query processing, Query embedding, Similarity search, Retrieval, Context \n",
      "enrichment, Generation, LLMs (OpenAI, Llama, \n"
     ]
    }
   ],
   "source": [
    "# Find all video markers across all pages\n",
    "print(\"Searching for video markers across all pages...\\n\")\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    if \"Video Order:\" in doc.page_content:\n",
    "        # Find the video header\n",
    "        lines = doc.page_content.split('\\n')\n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines\n",
    "            if \"Video\" in line and \"Order:\" in lines[j+1] if j+1 < len(lines) else False:\n",
    "                print(f\"Page {i+1}: Found video marker\")\n",
    "                print(f\"  {line}\")\n",
    "                print(f\"  {lines[j+1]}\")\n",
    "                break\n",
    "    \n",
    "# Check total content length\n",
    "total_content = \" \".join([doc.page_content for doc in docs])\n",
    "video_count = total_content.count(\"Video Order:\")\n",
    "print(f\"\\nTotal 'Video Order:' found: {video_count}\")\n",
    "\n",
    "# Find where Video 2 starts\n",
    "if \"Video 2:\" in total_content:\n",
    "    video2_index = total_content.find(\"Video 2:\")\n",
    "    print(f\"\\nVideo 2 found at character position: {video2_index}\")\n",
    "    print(\"Context around Video 2:\")\n",
    "    print(total_content[video2_index-50:video2_index+200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCANNING ALL PDFs FOR STRUCTURE PATTERNS\n",
      "\n",
      "============================================================\n",
      "\n",
      "📄 Core Components In RAG.pdf\n",
      "   Pages: 7\n",
      "   Videos found: 2\n",
      "   Structure preview:\n",
      "   Section 2: Core Components in RAG  Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith  Section Number: 2  Total Videos: 2  Date Created: 2024    Video 1: Data Ingestion and Parsing\n",
      "   Patterns: ['Has Section:', 'Has Video:', 'Has Topics:', 'Has Difficulty:']\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Data Ingestion And Parsing Techniques.pdf\n",
      "   Pages: 36\n",
      "   Videos found: 9\n",
      "   Structure preview:\n",
      "   Section 3: Data Ingestion and Data Parsing Techniques  Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith  Section Number: 3  Total Videos: 9  Date Created: 2024    Video 1: Docume\n",
      "   Patterns: ['Has Section:', 'Has Video:', 'Has Topics:', 'Has Difficulty:']\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 intro_to_rag.pdf\n",
      "   Pages: 20\n",
      "   Videos found: 4\n",
      "   Structure preview:\n",
      "   Section 1: Introduction to RAG  Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and Langsmith  Section Number: 1  Total Videos: 4  Date Created: 2024    Video 1: Introduction to RAG  Video Or\n",
      "   Patterns: ['Has Section:', 'Has Video:', 'Has Topics:', 'Has Difficulty:']\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Vector Embeddings And Vector Databases.pdf\n",
      "   Pages: 38\n",
      "   Videos found: 5\n",
      "   Structure preview:\n",
      "   Section 4: Vector Embeddings And Vector Databases  Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith  Section Number: 4  Total Videos: 5  Date Created: 2024    Video 1: Introducti\n",
      "   Patterns: ['Has Section:', 'Has Video:', 'Has Topics:', 'Has Difficulty:']\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Vector Stores Vs Vector Databases.pdf\n",
      "   Pages: 6\n",
      "   Videos found: 0\n",
      "   Structure preview:\n",
      "   Vector Stores Vs Vector Databases  Section: Vector Stores And Vector Databases  Overview  In previous lessons we generated embeddings using models like OpenAI and Hugging Face  and ran semantic search\n",
      "   Patterns: ['Has Section:']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Quick scan of all PDFs to understand their different structures\n",
    "print(\"SCANNING ALL PDFs FOR STRUCTURE PATTERNS\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    print(f\"\\n📄 {pdf_file}\")\n",
    "    print(f\"   Pages: {len(docs)}\")\n",
    "    \n",
    "    # Check for video markers\n",
    "    full_text = \" \".join([doc.page_content for doc in docs])\n",
    "    video_count = full_text.count(\"Video Order:\")\n",
    "    \n",
    "    print(f\"   Videos found: {video_count}\")\n",
    "    \n",
    "    # Check first 300 chars to see structure\n",
    "    print(f\"   Structure preview:\")\n",
    "    print(f\"   {docs[0].page_content[:200].replace(chr(10), ' ')}\")\n",
    "    \n",
    "    # Look for common patterns\n",
    "    patterns = {\n",
    "        \"Has Section:\": \"Section\" in full_text[:500],\n",
    "        \"Has Video:\": \"Video\" in full_text[:500], \n",
    "        \"Has Topics:\": \"Topics:\" in full_text[:500],\n",
    "        \"Has Difficulty:\": \"Difficulty:\" in full_text[:500]\n",
    "    }\n",
    "    print(f\"   Patterns: {[k for k,v in patterns.items() if v]}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT QUALITY ANALYSIS\n",
      "\n",
      "============================================================\n",
      "\n",
      "📄 Core Components In RAG.pdf\n",
      "\n",
      "📊 Quality Metrics:\n",
      "   Total characters: 10,320\n",
      "   Total words: 1,646\n",
      "   Avg words per page: 235\n",
      "\n",
      "⚠️  Potential Issues:\n",
      "   Pages ending mid-sentence: 1/7\n",
      "   Possible code blocks: False\n",
      "   Non-ASCII characters: 58\n",
      "   Double spaces: 0, Triple newlines: 0\n",
      "   Timestamp patterns: 0\n",
      "------------------------------------------------------------\n",
      "\n",
      "📄 Data Ingestion And Parsing Techniques.pdf\n",
      "\n",
      "📊 Quality Metrics:\n",
      "   Total characters: 58,233\n",
      "   Total words: 8,912\n",
      "   Avg words per page: 247\n",
      "\n",
      "⚠️  Potential Issues:\n",
      "   Pages ending mid-sentence: 13/36\n",
      "   Possible code blocks: True\n",
      "   Non-ASCII characters: 444\n",
      "   Double spaces: 0, Triple newlines: 0\n",
      "   Timestamp patterns: 0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Document Quality Analysis\n",
    "import re\n",
    "\n",
    "print(\"DOCUMENT QUALITY ANALYSIS\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for pdf_file in pdf_files[:2]:  # Start with first 2 PDFs\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    full_text = \" \".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    print(f\"\\n📄 {pdf_file}\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    print(\"\\n📊 Quality Metrics:\")\n",
    "    print(f\"   Total characters: {len(full_text):,}\")\n",
    "    print(f\"   Total words: {len(full_text.split()):,}\")\n",
    "    print(f\"   Avg words per page: {len(full_text.split())//len(docs)}\")\n",
    "    \n",
    "    # Check for common issues\n",
    "    print(\"\\n⚠️  Potential Issues:\")\n",
    "    \n",
    "    # 1. Broken sentences across pages?\n",
    "    broken_sentences = 0\n",
    "    for doc in docs[:-1]:\n",
    "        if doc.page_content.strip()[-1] not in '.!?':\n",
    "            broken_sentences += 1\n",
    "    print(f\"   Pages ending mid-sentence: {broken_sentences}/{len(docs)}\")\n",
    "    \n",
    "    # 2. Code blocks?\n",
    "    code_indicators = full_text.count('```') + full_text.count('import ') + full_text.count('def ')\n",
    "    print(f\"   Possible code blocks: {code_indicators > 0}\")\n",
    "    \n",
    "    # 3. Special characters/encoding issues?\n",
    "    weird_chars = len(re.findall(r'[^\\x00-\\x7F]+', full_text))\n",
    "    print(f\"   Non-ASCII characters: {weird_chars}\")\n",
    "    \n",
    "    # 4. Excessive whitespace?\n",
    "    double_spaces = full_text.count('  ')\n",
    "    triple_newlines = full_text.count('\\n\\n\\n')\n",
    "    print(f\"   Double spaces: {double_spaces}, Triple newlines: {triple_newlines}\")\n",
    "    \n",
    "    # 5. Timestamps in transcript?\n",
    "    timestamps = len(re.findall(r'\\d{1,2}:\\d{2}', full_text))\n",
    "    print(f\"   Timestamp patterns: {timestamps}\")\n",
    "    \n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEP DIVE: Data Ingestion And Parsing Techniques.pdf\n",
      "============================================================\n",
      "\n",
      "1️⃣ BROKEN SENTENCE EXAMPLE:\n",
      "\n",
      "Page 2 ends with:\n",
      "... embedded and searched.\" \n",
      "• \n",
      "metadata: a dictionary like: \n",
      "o source: \"example.txt\" \n",
      "o page_number: 1\n",
      "\n",
      "Page 3 starts with:\n",
      "o author: \"Krish\" \n",
      "o date_created: \"2024-01-01\" \n",
      "o (any other relevant fields you want) \n",
      "If you prin...\n",
      "\n",
      "2️⃣ CODE BLOCK EXAMPLE:\n",
      "Found at position 2086:\n",
      " reading different kinds of data. For this, we’ll import some libraries. \n",
      "If a library like pandas is missing, install it (for example: uv add pandas or your chosen \n",
      "package manager). As we add libraries, your pyproject.toml (or requirements) will up\n",
      "\n",
      "3️⃣ NON-ASCII CHARACTERS:\n",
      "\n",
      "Page 1 has: {'“', '→', '”—', '’'}\n"
     ]
    }
   ],
   "source": [
    "# Deep dive into ONE document to understand the issues\n",
    "pdf_path = os.path.join(pdf_folder, \"Data Ingestion And Parsing Techniques.pdf\")\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"DEEP DIVE: Data Ingestion And Parsing Techniques.pdf\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Find the broken sentence issue\n",
    "print(\"\\n1️⃣ BROKEN SENTENCE EXAMPLE:\")\n",
    "for i in range(len(docs)-1):\n",
    "    if not docs[i].page_content.strip().endswith(('.', '!', '?', ':')):\n",
    "        print(f\"\\nPage {i+1} ends with:\")\n",
    "        print(f\"...{docs[i].page_content.strip()[-100:]}\")\n",
    "        print(f\"\\nPage {i+2} starts with:\")\n",
    "        print(f\"{docs[i+1].page_content.strip()[:100]}...\")\n",
    "        break\n",
    "\n",
    "# 2. Find code blocks\n",
    "print(\"\\n2️⃣ CODE BLOCK EXAMPLE:\")\n",
    "full_text = \" \".join([doc.page_content for doc in docs])\n",
    "if \"import \" in full_text:\n",
    "    idx = full_text.find(\"import \")\n",
    "    print(f\"Found at position {idx}:\")\n",
    "    print(full_text[idx-50:idx+200])\n",
    "\n",
    "# 3. Find non-ASCII characters\n",
    "print(\"\\n3️⃣ NON-ASCII CHARACTERS:\")\n",
    "import re\n",
    "for i, doc in enumerate(docs[:3]):  # Check first 3 pages\n",
    "    non_ascii = re.findall(r'[^\\x00-\\x7F]+', doc.page_content)\n",
    "    if non_ascii:\n",
    "        print(f\"\\nPage {i+1} has: {set(non_ascii[:5])}\")  # Show first 5 unique\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello guys 👋\n",
      "SEMANTIC FLOW CHECK\n",
      "============================================================\n",
      "\n",
      "Cross-references found:\n",
      "  Forward refs: 1\n",
      "  Back refs: 0\n",
      "  Cross refs: 0\n",
      "\n",
      "Topic markers found:\n",
      "  'Introduction': 2 times\n",
      "  'Overview': 1 times\n",
      "  'Summary': 1 times\n"
     ]
    }
   ],
   "source": [
    "# Check semantic boundaries - How topics flow\n",
    "print(\"Hello guys 👋\")   # <-- added line\n",
    "print(\"SEMANTIC FLOW CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pdf_path = os.path.join(pdf_folder, \"intro_to_rag.pdf\")\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Check if topics are self-contained or reference each other\n",
    "full_text = \" \".join([doc.page_content for doc in docs[:5]])\n",
    "\n",
    "# Look for references to other parts\n",
    "references = {\n",
    "    \"Forward refs\": len(re.findall(r'(later|next video|upcoming|we will see)', full_text, re.I)),\n",
    "    \"Back refs\": len(re.findall(r'(earlier|previous|as mentioned|we saw)', full_text, re.I)),\n",
    "    \"Cross refs\": len(re.findall(r'(see also|refer to|check out)', full_text, re.I))\n",
    "}\n",
    "\n",
    "print(f\"\\nCross-references found:\")\n",
    "for ref_type, count in references.items():\n",
    "    print(f\"  {ref_type}: {count}\")\n",
    "\n",
    "# Check topic density - are there clear topic shifts?\n",
    "print(f\"\\nTopic markers found:\")\n",
    "topic_markers = ['Introduction', 'Overview', 'Summary', 'Conclusion', 'Step', 'Phase']\n",
    "for marker in topic_markers:\n",
    "    count = full_text.count(marker)\n",
    "    if count > 0:\n",
    "        print(f\"  '{marker}': {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned Core Components In RAG.pdf (7 pages)\n",
      "✅ Cleaned Data Ingestion And Parsing Techniques.pdf (36 pages)\n",
      "✅ Cleaned intro_to_rag.pdf (20 pages)\n",
      "✅ Cleaned Vector Embeddings And Vector Databases.pdf (38 pages)\n",
      "✅ Cleaned Vector Stores Vs Vector Databases.pdf (6 pages)\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Define folder\n",
    "pdf_folder = \"pdf\"\n",
    "\n",
    "# Greeting & closing patterns (expandable)\n",
    "greeting_patterns = [\n",
    "    r\"(?i)\\bhello\\s+guys[.! ]*\", \n",
    "    r\"(?i)\\bhi\\s+(everyone|guys|folks)[.! ]*\",\n",
    "    r\"(?i)\\bhey\\s+(everyone|guys)[.! ]*\"\n",
    "]\n",
    "\n",
    "closing_patterns = [\n",
    "    r\"(?i)\\bthank\\s+you[.! ]*\",\n",
    "    r\"(?i)\\bthanks\\s+(for\\s+watching|everyone|guys)[.! ]*\",\n",
    "    r\"(?i)\\bsee\\s+you\\s+(in\\s+the\\s+next\\s+video|soon)[.! ]*\",\n",
    "    r\"(?i)\\bthat'?s\\s+it\\s+for\\s+(this|today'?s)\\s+(video|lesson)[.! ]*\"\n",
    "]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove greetings and closings.\"\"\"\n",
    "    for pattern in greeting_patterns + closing_patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    # Normalize extra spaces\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Loop through all PDFs and clean their text\n",
    "cleaned_docs = {}\n",
    "\n",
    "for pdf_file in os.listdir(pdf_folder):\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        cleaned_texts = [clean_text(doc.page_content) for doc in docs]\n",
    "        cleaned_docs[pdf_file] = \" \".join(cleaned_texts)\n",
    "\n",
    "        print(f\"✅ Cleaned {pdf_file} ({len(docs)} pages)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Core Components In RAG.pdf: 2 videos extracted\n",
      "✅ Data Ingestion And Parsing Techniques.pdf: 9 videos extracted\n",
      "✅ intro_to_rag.pdf: 4 videos extracted\n",
      "✅ Vector Embeddings And Vector Databases.pdf: 5 videos extracted\n",
      "✅ Vector Stores Vs Vector Databases.pdf: 0 videos extracted\n",
      "\n",
      "📘 SAMPLE DOCUMENTS PREVIEW\n",
      " ======================================================================\n",
      "\n",
      "=== Core Components In RAG.pdf ===\n",
      "\n",
      "🎞️ Video Title: Data Ingestion and Parsing\n",
      "   Video Order: 1/2\n",
      "   Topics: Document ingestion, Pre-processing, Chunking, Embeddings, Vector databases,\n",
      "   Difficulty: Beginner\n",
      "   Section: Core Components in RAG\n",
      "   Section Number: 2\n",
      "   Total Videos: 2\n",
      "   Date Created: 2024\n",
      "   Source File: Core Components In RAG.pdf\n",
      "   Characters: 5357\n",
      "\n",
      "📝 Content Preview:\n",
      "So we are going to continue the discussion of Retrieval-Augmented Generation (RAG). In this specific video, we’ll dive into the core components of a RAG pipeline. By now, you already have an intuition for how RAG works at a high level: we have a Large Language Model (LLM), we augment it with externa...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎞️ Video Title: Query Processing and Output Generation Phase\n",
      "   Video Order: 2/2\n",
      "   Topics: Query processing, Query embedding, Similarity search, Retrieval, Context\n",
      "   Difficulty: Beginner\n",
      "   Section: Core Components in RAG\n",
      "   Section Number: 2\n",
      "   Total Videos: 2\n",
      "   Date Created: 2024\n",
      "   Source File: Core Components In RAG.pdf\n",
      "   Characters: 4103\n",
      "\n",
      "📝 Content Preview:\n",
      "So we are going to continue our discussion with respect to our core components in RAG. We have already discussed about document ingestion and pre-processing. We understood what all specific steps we take in the document ingestion phase, wherein we focus on finding the sources of the data. Then we us...\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Data Ingestion And Parsing Techniques.pdf ===\n",
      "\n",
      "🎞️ Video Title: Document Structure in LangChain\n",
      "   Video Order: 1/9\n",
      "   Topics: LangChain document structure, Page content vs. metadata, Document loaders,\n",
      "   Difficulty: Beginner\n",
      "   Section: Data Ingestion and Data Parsing Techniques\n",
      "   Section Number: 3\n",
      "   Total Videos: 9\n",
      "   Date Created: 2024\n",
      "   Source File: Data Ingestion And Parsing Techniques.pdf\n",
      "   Characters: 4360\n",
      "\n",
      "📝 Content Preview:\n",
      "vs. metadata, Document loaders, Text splitters, Project setup notes, Why metadata matters Difficulty: Beginner Content So we are going to continue the discussion with respect to data ingestion. Already in our previous video, we created our virtual environment and set up the project structure. Now, i...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎞️ Video Title: Ingesting and Parsing Text Data Using Document Loaders\n",
      "   Video Order: 2/9\n",
      "   Topics: Text files, TextLoader, DirectoryLoader, Document structure (page_content &\n",
      "   Difficulty: Beginner\n",
      "   Section: Data Ingestion and Data Parsing Techniques\n",
      "   Section Number: 3\n",
      "   Total Videos: 9\n",
      "   Date Created: 2024\n",
      "   Source File: Data Ingestion And Parsing Techniques.pdf\n",
      "   Characters: 4663\n",
      "\n",
      "📝 Content Preview:\n",
      "So we are going to continue the discussion with respect to RAG. Already in our previous video, we have understood the entire document structure inside data ingestion. So first of all, one type of data source files that we are going to read is a text file. What we are going to do here is the simplest...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎞️ Video Title: Text Splitting Techniques\n",
      "   Video Order: 3/9\n",
      "   Topics: Text splitters, CharacterTextSplitter, RecursiveCharacterTextSplitter,\n",
      "   Difficulty: Beginner\n",
      "   Section: Data Ingestion and Data Parsing Techniques\n",
      "   Section Number: 3\n",
      "   Total Videos: 9\n",
      "   Date Created: 2024\n",
      "   Source File: Data Ingestion And Parsing Techniques.pdf\n",
      "   Characters: 5783\n",
      "\n",
      "📝 Content Preview:\n",
      "So we are going to continue the discussion with respect to our data ingestion pipeline. Already in our previous video, we have seen how to read a .txt file and convert it into a Document using the document loaders available in LangChain. Now, let me go back to the RAG architecture diagram we saw ear...\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== intro_to_rag.pdf ===\n",
      "\n",
      "🎞️ Video Title: Introduction to RAG\n",
      "   Video Order: 1/4\n",
      "   Topics: RAG fundamentals, Retrieval, Augmentation, Generation, Architecture overview\n",
      "   Difficulty: Beginner\n",
      "   Section: Introduction to RAG\n",
      "   Section Number: 1\n",
      "   Total Videos: 4\n",
      "   Date Created: 2024\n",
      "   Source File: intro_to_rag.pdf\n",
      "   Characters: 9893\n",
      "\n",
      "📝 Content Preview:\n",
      "So I'm super excited to start the series of videos on RAG. In this video and in the upcoming series of videos, we are going to understand everything about retrieval augmented generation, a very important topic currently. We will talk about various architectures. We'll talk about how to build a RAG p...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎞️ Video Title: Some Examples and Advantages Of Using RAG\n",
      "   Video Order: 2/4\n",
      "   Topics: RAG workflow, Similarity search, Augmentation details, Real-world examples,\n",
      "   Difficulty: Beginner-Intermediate\n",
      "   Section: Introduction to RAG\n",
      "   Section Number: 1\n",
      "   Total Videos: 4\n",
      "   Date Created: 2024\n",
      "   Source File: intro_to_rag.pdf\n",
      "   Characters: 12404\n",
      "\n",
      "📝 Content Preview:\n",
      "So we are going to continue the discussion with respect to RAG already. We got some idea at least to make you just understand what is RAG and how does RAG actually work. One is retrieval augmented generation. One very important thing that I missed out in the previous video is that whenever you make ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎞️ Video Title: Business Impact Use Cases With RAG\n",
      "   Video Order: 3/4\n",
      "   Topics: Business impact, Cost savings, ROI, Industry adoption, Real case studies\n",
      "   Difficulty: Intermediate\n",
      "   Section: Introduction to RAG\n",
      "   Section Number: 1\n",
      "   Total Videos: 4\n",
      "   Date Created: 2024\n",
      "   Source File: intro_to_rag.pdf\n",
      "   Characters: 2836\n",
      "\n",
      "📝 Content Preview:\n",
      "So we are going to continue our discussion with respect to RAG. Already in our previous video, you know, I gave some of the points where I told that how JPMorgan were able to, you know, save $150 million annually using RAG and all. So what I thought is that why RAG actually matters, you know, and wh...\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Vector Embeddings And Vector Databases.pdf ===\n",
      "\n",
      "🎞️ Video Title: Introduction To Embeddings And Vector Databases\n",
      "   Video Order: 1/5\n",
      "   Topics: Embedding fundamentals, Vector databases vs traditional databases, Similarity\n",
      "   Difficulty: Beginner → Intermediate\n",
      "   Section: Vector Embeddings And Vector Databases\n",
      "   Section Number: 4\n",
      "   Total Videos: 5\n",
      "   Date Created: 2024\n",
      "   Source File: Vector Embeddings And Vector Databases.pdf\n",
      "   Characters: 15621\n",
      "\n",
      "📝 Content Preview:\n",
      "search, Cosine similarity, Feature representations, Model choices (OpenAI & Hugging Face) — Overview — So we are going to continue the discussion with respect to Rag. — Recap: Ingestion & Splitting — In our previous video we have seen various data ingestion and parsing techniques. Uh, and the last o...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎞️ Video Title: Visualization Of Embedding And Cosine Similarity\n",
      "   Video Order: 2/5\n",
      "   Topics: Visualizing embeddings, 2D plots, Cosine similarity, Interpreting similarity scores,\n",
      "   Difficulty: Beginner → Intermediate\n",
      "   Section: Vector Embeddings And Vector Databases\n",
      "   Section Number: 4\n",
      "   Total Videos: 5\n",
      "   Date Created: 2024\n",
      "   Source File: Vector Embeddings And Vector Databases.pdf\n",
      "   Characters: 11880\n",
      "\n",
      "📝 Content Preview:\n",
      "High-dimensional embeddings — Overview — So we are going to continue the discussion with respect to embeddings. Now, uh, already we have understood some of the theoretical explanation about what exactly embeddings is, what exactly embedding models is. And, uh, you know, we also got a brief idea abou...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎞️ Video Title: Creating Your First Embeddings With HuggingFace Embedding Models\n",
      "   Video Order: 3/5\n",
      "   Topics: Hugging Face overview, Sentence Transformers, LangChain integrations, Embed\n",
      "   Difficulty: Beginner → Intermediate\n",
      "   Section: Vector Embeddings And Vector Databases\n",
      "   Section Number: 4\n",
      "   Total Videos: 5\n",
      "   Date Created: 2024\n",
      "   Source File: Vector Embeddings And Vector Databases.pdf\n",
      "   Characters: 17925\n",
      "\n",
      "📝 Content Preview:\n",
      "query vs. embed documents, Model choices & dimensions — Overview — So we are going to continue the discussion with the topic that is embeddings. And in this specific video we are going to create our first embeddings. Now I hope everybody has got an idea what exactly embeddings is. You know, at the e...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_folder = \"pdf\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove greetings, closings, normalize whitespace.\"\"\"\n",
    "    patterns = [\n",
    "        r\"(?i)\\bhello\\s+guys[.! ]*\",\n",
    "        r\"(?i)\\bhi\\s+(everyone|guys|folks)[.! ]*\",\n",
    "        r\"(?i)\\bthank\\s+you[.! ]*\",\n",
    "        r\"(?i)\\bthanks\\s+(for\\s+watching|everyone|guys)[.! ]*\",\n",
    "        r\"(?i)\\bsee\\s+you\\s+(in\\s+the\\s+next\\s+video|soon)[.! ]*\",\n",
    "        r\"(?i)\\bthat'?s\\s+it\\s+for\\s+(this|today'?s)\\s+(video|lesson)[.! ]*\",\n",
    "    ]\n",
    "    for p in patterns:\n",
    "        text = re.sub(p, \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_section_metadata(text):\n",
    "    \"\"\"Extract metadata at the section level.\"\"\"\n",
    "    section_meta = {}\n",
    "    patterns = {\n",
    "        \"section_name\": r\"Section\\s*\\d*:\\s*(.*?)(?:Course|Section\\s*Number|Total|Date|$)\",\n",
    "        \"section_number\": r\"Section\\s*Number:\\s*(\\d+)\",\n",
    "        \"total_videos\": r\"Total\\s*Videos:\\s*(\\d+)\",\n",
    "        \"course_name\": r\"Course:\\s*(.*?)(?:Section|$)\",\n",
    "        \"date_created\": r\"Date\\s*Created:\\s*(\\d+)\",\n",
    "    }\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            section_meta[key] = match.group(1).strip()\n",
    "    return section_meta\n",
    "\n",
    "\n",
    "def extract_video_metadata(text):\n",
    "    \"\"\"Extract metadata inside a single video block.\"\"\"\n",
    "    meta = {}\n",
    "    patterns = {\n",
    "        \"video_title\": r\"Video\\s*\\d+:\\s*([^\\n]*)\",\n",
    "        \"video_order\": r\"Video\\s*Order:\\s*([^\\n]*)\",\n",
    "        \"topics\": r\"Topics:\\s*([^\\n]*)\",\n",
    "        \"difficulty\": r\"Difficulty:\\s*([^\\n]*)\",\n",
    "    }\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            meta[key] = match.group(1).strip()\n",
    "    return meta\n",
    "\n",
    "\n",
    "def strip_metadata_from_content(text):\n",
    "    \"\"\"\n",
    "    Remove any leading metadata section (Video title, Topics, Difficulty, etc.)\n",
    "    including the 'Content' label if present.\n",
    "    \"\"\"\n",
    "    # Find the 'Content' keyword and remove everything before it\n",
    "    content_match = re.search(r\"(?i)\\bcontent\\b[:\\-]?\", text)\n",
    "    if content_match:\n",
    "        text = text[content_match.end():]\n",
    "    else:\n",
    "        # Fallback: remove any lines starting with metadata-like keywords\n",
    "        lines = text.splitlines()\n",
    "        cleaned_lines = []\n",
    "        skip_keywords = [\"video\", \"video order\", \"topics\", \"difficulty\"]\n",
    "        for line in lines:\n",
    "            if any(re.match(fr\"(?i)^{kw}\", line.strip()) for kw in skip_keywords):\n",
    "                continue\n",
    "            cleaned_lines.append(line)\n",
    "        text = \"\\n\".join(cleaned_lines)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def load_pdf_videos(pdf_path):\n",
    "    \"\"\"Extract structured Document objects from PDF (only videos).\"\"\"\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    full_text = \" \".join([doc.page_content for doc in docs])\n",
    "    section_meta = extract_section_metadata(full_text)\n",
    "    section_meta[\"source_file\"] = os.path.basename(pdf_path)\n",
    "\n",
    "    # Split by video markers and skip the first (section header)\n",
    "    video_blocks = re.split(r\"(?=Video\\s*\\d+:)\", full_text)[1:]\n",
    "\n",
    "    video_docs = []\n",
    "    for block in video_blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "        meta = extract_video_metadata(block)\n",
    "        text = clean_text(strip_metadata_from_content(block))\n",
    "        metadata = {**section_meta, **meta, \"doc_type\": \"video\"}\n",
    "        video_docs.append(Document(page_content=text, metadata=metadata))\n",
    "\n",
    "    return video_docs\n",
    "\n",
    "\n",
    "# --- PROCESS ALL PDFs ---\n",
    "all_video_docs = []\n",
    "for pdf_file in os.listdir(pdf_folder):\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        video_docs = load_pdf_videos(pdf_path)\n",
    "        all_video_docs.extend(video_docs)\n",
    "        print(f\"✅ {pdf_file}: {len(video_docs)} videos extracted\")\n",
    "\n",
    "\n",
    "# --- PREVIEW SAMPLE DOCUMENTS ---\n",
    "print(\"\\n📘 SAMPLE DOCUMENTS PREVIEW\\n\", \"=\"*70)\n",
    "by_source = {}\n",
    "for doc in all_video_docs:\n",
    "    src = doc.metadata[\"source_file\"]\n",
    "    by_source.setdefault(src, []).append(doc)\n",
    "\n",
    "for src, docs in by_source.items():\n",
    "    print(f\"\\n=== {src} ===\")\n",
    "    for d in docs[:3]:\n",
    "        print(f\"\\n🎞️ Video Title: {d.metadata.get('video_title', 'N/A')}\")\n",
    "        print(f\"   Video Order: {d.metadata.get('video_order', 'N/A')}\")\n",
    "        print(f\"   Topics: {d.metadata.get('topics', 'N/A')}\")\n",
    "        print(f\"   Difficulty: {d.metadata.get('difficulty', 'N/A')}\")\n",
    "        print(f\"   Section: {d.metadata.get('section_name', 'N/A')}\")\n",
    "        print(f\"   Section Number: {d.metadata.get('section_number', 'N/A')}\")\n",
    "        print(f\"   Total Videos: {d.metadata.get('total_videos', 'N/A')}\")\n",
    "        print(f\"   Date Created: {d.metadata.get('date_created', 'N/A')}\")\n",
    "        print(f\"   Source File: {d.metadata.get('source_file', 'N/A')}\")\n",
    "        print(f\"   Characters: {len(d.page_content)}\")\n",
    "        print(f\"\\n📝 Content Preview:\\n{d.page_content[:300]}...\")\n",
    "        print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'section_name': 'Core Components in RAG', 'section_number': '2', 'total_videos': '2', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Core Components In RAG.pdf', 'video_title': 'Data Ingestion and Parsing', 'video_order': '1/2', 'topics': 'Document ingestion, Pre-processing, Chunking, Embeddings, Vector databases,', 'difficulty': 'Beginner', 'doc_type': 'video'}, page_content='So we are going to continue the discussion of Retrieval-Augmented Generation (RAG). In this specific video, we’ll dive into the core components of a RAG pipeline. By now, you already have an intuition for how RAG works at a high level: we have a Large Language Model (LLM), we augment it with external knowledge stored in a vector database, and the LLM uses retrieved context from that database to generate better answers. At a glance, when I provide an input to a plain LLM, it just generates an output from its internal knowledge. In RAG, however, the LLM is connected to a vector database. We store information in that database as vectors (numerical representations). When we send a question, we first retrieve relevant information from the vector database, pass that retrieved context to the LLM, and then the LLM generates a summarized, grounded output using that context. From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1. Document Ingestion Phase 2. Query Processing Phase 3. Generation Phase This video focuses on Phase 1: Document Ingestion and Pre-processing. We’ll break down what data we ingest, how we clean and split it, how we embed it into vectors, and how we store it in a vector database so that later phases (query + generation) can work effectively. What is Document Ingestion and Pre-processing? To power the retriever, we first need a vector database filled with vectors that represent our knowledge. This knowledge can come from multiple sources: company policies, internal documents, PDFs, Word docs, CSVs, websites, databases, images (with extracted text), and more. The ingestion pipeline’s job is to: • Load data from these sources • Pre-process the raw text (cleaning, normalization) • Split documents into chunks • Embed each chunk into a vector • Store the vectors (with metadata) in a vector database That’s the core of Phase 1. Common Data Sources Your data might be: • PDFs • Word Documents (.docx) • Text files (.txt) • Spreadsheets (CSV/Excel) • Web pages (scraped or via loaders) • Databases (records turned into text fields) We will read all of these using loaders, normalize them into a consistent text representation, and then pass them through a standardized pipeline. The Ingestion Pipeline: Step by Step Step 1: Load the Documents Use appropriate loaders per file type to extract text and attach preliminary metadata (e.g., source, filename, section, url). Step 2: Pre-process the Text Light cleaning (remove boilerplate, headers/footers if noisy, fix line breaks, normalize whitespace, optionally remove very long tables or binary debris) so the chunks are clean and meaningful. Step 3: Split into Chunks We don’t store entire documents as a single vector. We split documents into smaller chunks (e.g., ~500 tokens with some overlap). This matters because every LLM has a context window limit—only so much text can be fed to the model at once. Smaller, coherent chunks improve both retrieval accuracy and downstream answer quality. • Think of chunks as paragraphs or sections sized for retrieval. • Overlap (e.g., 50–100 tokens) helps preserve continuity across chunk boundaries. Why chunking is necessary: If you try to hand the LLM a thousand-page book as context, it won’t fit. Even if it did, the relevant part might be buried. Chunking allows the retriever to pinpoint just the passages most relevant to the question, keeping prompts concise and precise. Step 4: Embed Each Chunk We pass each chunk to an embedding model that converts text into a vector (a list of numbers). For example, a sentence might turn into a vector like [0.6, 0.5, 0.4, 0.1, 0.7, …]. These vectors capture semantic meaning, enabling similarity comparisons. • You can use OpenAI embeddings or Hugging Face open-source models. • The embedding model’s job: map semantically similar text to nearby points in vector space. Step 5: Store in a Vector Database We store each vector along with its metadata (e.g., source, section, page_number, chunk_id, timestamp, keywords). Popular vector stores include: • ChromaDB • FAISS (library) • Pinecone (managed) • DataStax / Cassandra with vector support This database supports similarity search so we can find the most relevant chunks for a given query later. Similarity Search and Distance Metrics Once vectors are stored, the retriever uses similarity search to find chunks related to a query. Common distance/similarity measures: • Cosine similarity (most common for embeddings) • Euclidean distance Given a user question like, “What is an LLM?”, the retriever embeds the question into a vector and searches for the nearest vectors in the database. The top-k matching chunks are returned as context for the LLM. End-to-End View of Phase 1 1. Data Sources → PDFs, docs, CSVs, websites, databases 2. Loaders + Cleaning → normalize and prepare text 3. Split into Chunks → manageable, overlapping units 4. Embed Chunks → text → vectors using an embedding model 5. Store in Vector DB → vectors + metadata (for retrieval) When this pipeline completes, you have a ready-to-query vector database. In the next video, we’ll cover Query Processing: how the system embeds an incoming user question, how it performs the similarity search, how we select and format the retrieved context, and how that context flows into the LLM for the Generation Phase.'),\n",
       " Document(metadata={'section_name': 'Core Components in RAG', 'section_number': '2', 'total_videos': '2', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Core Components In RAG.pdf', 'video_title': 'Query Processing and Output Generation Phase', 'video_order': '2/2', 'topics': 'Query processing, Query embedding, Similarity search, Retrieval, Context', 'difficulty': 'Beginner', 'doc_type': 'video'}, page_content='So we are going to continue our discussion with respect to our core components in RAG. We have already discussed about document ingestion and pre-processing. We understood what all specific steps we take in the document ingestion phase, wherein we focus on finding the sources of the data. Then we use document splitter to convert that into chunks. Then with the help of an embedding model, we convert those chunks into vectors, and we store these vectors in the vector database. So in this video we are going to focus on the second important step—that is, the query processing phase. Initially here, we are focusing more on the theoretical concepts, but as we go ahead we will implement each and everything, from the data ingestion pipeline to the query processing phase to the generation phase. Then we will try to build better RAG architectures by using these concepts. Now in the second important step, we are really going to focus on something called a query. If you see over here, these are the steps that we are going to do: the query processing phase. In the query processing phase, what all things will we be learning as we go ahead? Till now you know that from our data ingestion pipeline, my vector database is ready. Let’s say I draw a vector database—this is the vector database that is ready. You know how this vector database has been created: we used the data ingestion pipeline. In that pipeline, we had our source data, then we did chunking, then we converted chunks into vectors, and finally we stored all of this into the vector database. Now, in the query processing phase, we first of all take an input query. So here I will be having my input query. For example, the input is: “What is RAG?” If all the information stored in the vector database is related to RAG, the system will operate as follows. Step 1: Embed the input query The input query text (e.g., “What is RAG?”) will be converted into a vector using embedding models. After this step, we obtain a numeric vector representation—some numbers like 0.3, 0.4, 0.6, etc., up to the embedding dimension. This is the initial first phase of query processing: converting text to vectors. Step 2: Retrieve similar chunks from the vector database Once we have the query vector, we send a request to the vector database and apply similarity-based search. Techniques include cosine similarity and Euclidean distance, among others. The main aim is to find the most relevant chunks that match the query vector. Based on this search, we retrieve multiple chunks—say Document 1, Document 2, Document 3—that best match the input query. These are the retrieved results returned by the vector database. Step 3: Build the augmented context These retrieved results are then used to form a context for the LLM. With Retrieval-Augmented Generation, we often enrich the retrieved context by adding metadata (source, section, page, timestamps, etc.) or applying light transformations so that the LLM receives cleaner, more targeted information. This enrichment helps the LLM produce a more accurate and grounded output. At the end of query processing, we have: original query + retrieved chunks (+ enriched metadata/context). This bundle is what we pass forward to the next stage—the Generation Phase. Generation Phase (overview) In the generation phase, whatever context we have built from the retriever is passed to the LLM. We can use different LLMs: OpenAI models, Llama (open source), Google Gemini, and others. The LLM receives (original query + enriched context) and then produces a summarized output—a final answer phrased naturally, like how a human would write. This is how the entire pipeline of a RAG system looks in practice: • Document Ingestion & Pre-processing → build the vector database from sources • Query Processing → embed query, retrieve the most relevant chunks, enrich context • Generation → feed query + context to the LLM and produce the final answer With this, we generate an output and return it to the user. The best part is that the LLM can summarize and structure the answer in a very natural, conversational way.'),\n",
       " Document(metadata={'section_name': 'Data Ingestion and Data Parsing Techniques', 'section_number': '3', 'total_videos': '9', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Data Ingestion And Parsing Techniques.pdf', 'video_title': 'Document Structure in LangChain', 'video_order': '1/9', 'topics': 'LangChain document structure, Page content vs. metadata, Document loaders,', 'difficulty': 'Beginner', 'doc_type': 'video'}, page_content='vs. metadata, Document loaders, Text splitters, Project setup notes, Why metadata matters Difficulty: Beginner Content So we are going to continue the discussion with respect to data ingestion. Already in our previous video, we created our virtual environment and set up the project structure. Now, in this video and in the upcoming series of videos, we are going to focus on data ingestion and parsing. Since we are using LangChain for data ingestion, the data will arrive in different source formats: PDFs, text files, Word documents, and more. I will try to show you multiple techniques to read these files with the recent LangChain version 0.3. One very important point: as soon as we read documents, LangChain expects us to convert that data into a Document structure. I’ve included a diagram in the resources section titled “LangChain Document Structure”—this is a very important concept. For every document we read and plan to store in a vector database, we include key components like page_content and metadata (the metadata is a dictionary). This is the structure we will follow: read the data → convert it to the Document structure → (later) embed and store in a vector store. This step also lets us enrich content with additional metadata. Here’s the idea: • page_content: a string containing the main text we want to embed and search. • metadata: a dictionary with additional information (source, author, page, date, etc.) that helps retrieval and filtering. There are many document loader techniques available in LangChain. We will focus on both custom and in-built techniques because parsing is a very important skill. Let’s start. Introduction to Data Ingestion Data ingestion means reading different kinds of data. For this, we’ll import some libraries. If a library like pandas is missing, install it (for example: uv add pandas or your chosen package manager). As we add libraries, your pyproject.toml (or requirements) will update. If any code doesn’t work, double-check the installed versions vs. the versions I use—\\nmismatches can cause breaking changes over time. Next, we’ll import the basic types (List, Dict, Any) and pandas, and then the Document structure and text splitters from LangChain 0.3. For splitters, we will look at: • RecursiveCharacterTextSplitter • CharacterTextSplitter • TokenTextSplitter We’ll use each and understand the differences as we go ahead. After setting this up, we’ll just print a small message like “setup completed” so you know the environment is ready. Understanding the LangChain Document Structure This structure will play a very important role when you start building RAG pipelines. Initially, whenever you read any data, you need to convert it into Document objects. From the diagram, there are two key parts: • page_content → the main text • metadata → additional information about that text Let’s create a simple Document conceptually (code will come later): • page_content: \"This is the main text content that will be embedded and searched.\" • metadata: a dictionary like: o source: \"example.txt\" o page_number: 1 o author: \"Krish\" o date_created: \"2024-01-01\" o (any other relevant fields you want) If you print the document, you’ll see both parts: • Access content via doc.page_content • Access metadata via doc.metadata Why metadata is crucial: • Filtering & search: restrict results to a source, author, section, page range, etc. • Tracking sources: show the user where the answer came from. • Providing context in responses: include citation details or section names. • Debugging & auditing: trace how a result was produced. When we store content with metadata in our vector database, we can answer additional questions like “Who is the author?” because that information travels with each chunk. For advanced RAG, metadata becomes even more powerful. From Loaders to Documents When we use LangChain document loaders (for PDFs, text, Word docs, websites, etc.), the return type is Document (or a list of Documents). That means, after loading, you already have objects with page_content and metadata. You can inspect their types and fields directly, and you’re ready for chunking and embedding next. Remember the ingestion diagram: after loading a document, we typically pass it to a document/text splitter to break it into chunks. We will use different splitters and compare them in practice.'),\n",
       " Document(metadata={'section_name': 'Data Ingestion and Data Parsing Techniques', 'section_number': '3', 'total_videos': '9', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Data Ingestion And Parsing Techniques.pdf', 'video_title': 'Ingesting and Parsing Text Data Using Document Loaders', 'video_order': '2/9', 'topics': 'Text files, TextLoader, DirectoryLoader, Document structure (page_content &', 'difficulty': 'Beginner', 'doc_type': 'video'}, page_content='So we are going to continue the discussion with respect to RAG. Already in our previous video, we have understood the entire document structure inside data ingestion. So first of all, one type of data source files that we are going to read is a text file. What we are going to do here is the simplest case: reading .txt files. You can refer to the earlier diagram for the document data structure—how the page_content is created, how metadata is created. We also wrote some code and understood that there are two important fields in each Document: • page_content • metadata Document is a structure provided by LangChain. Anything that we read from supported data sources is converted into one or more Document objects, each with a page_content and metadata. Let me now show you how to read text files. This is the simplest case you’ll see. Creating Sample Text Files (Setup) Imagine some of your data is present inside .txt files. First, we’ll create a simple directory and sample text files. • Import os. • Use os.makedirs() to create a directory inside the project where we’re doing data parsing. • Create a data/ folder, and inside it a text_files/ folder. • Use exist_ok=True so it won’t error if the folder already exists. After creating the folders, we’ll create sample text files using a small dictionary of {file_path: content} pairs, for example: • data/text_files/python_intro.txt • data/text_files/machine_learning_basics.txt We’ll iterate over the dictionary and, for each (file_path, content) pair: • open(file_path, \"w\", encoding=\"utf-8\") and write the content. • Print a message like “Sample text file created.” At the end, you should have two files in data/text_files/: • python_intro.txt • machine_learning_basics.txt Reading a Single Text File with TextLoader The first method is reading a single file using TextLoader. • Import TextLoader (commonly from langchain_community.document_loaders import TextLoader). • Create a loader: loader = TextLoader(\"data/text_files/python_intro.txt\", encoding=\"utf-8\"). • Call documents = loader.load(). As discussed, the return type is a list of Document objects. If you print type(documents), you’ll see it’s a list. Each item looks like: • Document(page_content=..., metadata={\"source\": \"data/text_files/python_intro.txt\"}) So even with the built-in loader, you get initial metadata automatically (e.g., source path). You can inspect the contents like this: • Number of documents loaded • Preview first 100 characters: documents[0].page_content[:100] • Show metadata: documents[0].metadata This confirms that a .txt file read via TextLoader yields a list of Documents, each with page_content and metadata. Reading All Text Files in a Directory with DirectoryLoader Now let’s read multiple files directly from a directory using DirectoryLoader. This is suitable when you have many text files to ingest. • Import DirectoryLoader from langchain_community.document_loaders. • Create the loader: o directory_path = \"data/text_files\" o Use a glob pattern to match files, e.g., glob=\"**/*.txt\" (recursive pattern: any subfolder, any .txt file). o Set loader_cls=TextLoader to specify the loader for each matched file. o Use loader_kwargs={\"encoding\": \"utf-8\"} so each text file is read with UTF-8. o Optionally show_progress=True to visualize the loading progress. • Call documents = directory_loader.load(). Then, iterate over the results and print details: • len(documents) • For each doc in documents: o doc.metadata.get(\"source\") o len(doc.page_content) (number of characters in the content) You’ll see output similar to: • Document 1 → source=data/text_files/python_intro.txt, length=<chars> • Document 2 → source=data/text_files/machine_learning_basics.txt, length=<chars> DirectoryLoader: Characteristics, Advantages, Disadvantages Advantages • Load multiple files at once. • Supports glob patterns for flexible matching (**/*.txt). • Progress tracking with show_progress=True. • Recursive directory scanning. Disadvantages • Typically assume files matching the glob are of the same type (e.g., all .txt) for a single loader class. • Limited per-file error handling. • Can be memory intensive for very large directories (many or large text files) since it scans and loads many files. Summary of What We Did • Created sample text files programmatically. • Read a single text file via TextLoader → got a list[Document] with page_content and metadata. • Read multiple text files via DirectoryLoader using a glob pattern and TextLoader as loader_cls. • Saw how metadata (like source) is populated automatically and why it’s helpful later in retrieval and filtering. • Discussed the pros/cons of DirectoryLoader.'),\n",
       " Document(metadata={'section_name': 'Data Ingestion and Data Parsing Techniques', 'section_number': '3', 'total_videos': '9', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Data Ingestion And Parsing Techniques.pdf', 'video_title': 'Text Splitting Techniques', 'video_order': '3/9', 'topics': 'Text splitters, CharacterTextSplitter, RecursiveCharacterTextSplitter,', 'difficulty': 'Beginner', 'doc_type': 'video'}, page_content='So we are going to continue the discussion with respect to our data ingestion pipeline. Already in our previous video, we have seen how to read a .txt file and convert it into a Document using the document loaders available in LangChain. Now, let me go back to the RAG architecture diagram we saw earlier. In the data ingestion phase, we may have different files: PDFs, web pages, text files, etc. After reading a file, the return type is a Document (or list of Documents). Then we apply something called a document splitter (also called a text splitter). The main goal is to split the document into chunks. In this video, I’m going to discuss some of the text splitting strategies available in LangChain. As we go ahead, we’ll also look at more techniques when we move toward advanced RAG. The idea of text splitting is simple: we split the loaded documents into smaller chunks because LLMs have context window limitations. Smaller, coherent chunks help retrieval and improve downstream answer quality. Importing Text Splitters We will import the splitters from langchain.text_splitter (or the updated module path as per LangChain 0.3): • CharacterTextSplitter • RecursiveCharacterTextSplitter • TokenTextSplitter We will print our loaded documents (list of Document). Then we’ll take one Document (e.g., documents[0]) and access doc.page_content to apply the different splitters. Method 1: CharacterTextSplitter The CharacterTextSplitter performs splitting based on characters using a specified separator. • Separator: for example, a newline (\"\\\\n\"). • Chunk size: e.g., 200 characters (max characters per chunk). • Chunk overlap: e.g., 20 characters (the last 20 chars of one chunk repeat at the start of the next). • Length function: standard len. We create the splitter and call either split_text(text) (if we’re splitting a string) or split_documents([doc]) (if we want Document-aware splitting). Here we’ll illustrate split_text(text). • Print len(char_chunks) to see how many chunks were created. • Print a few chunks (e.g., char_chunks[0], char_chunks[1]) to inspect content. If the separator is a newline and our text contains natural line breaks, you may observe clean splits with little to no visible overlap in the printed chunks (even if chunk_overlap is set), because the splitter respects the separator boundaries. Method 2: RecursiveCharacterTextSplitter (Recommended) The RecursiveCharacterTextSplitter is the most recommended general-purpose splitter. It tries multiple separators in order (e.g., \"\\\\n\\\\n\", \"\\\\n\", space, etc.) and recursively backs off to smaller units when larger separators don’t fit the chunk size. • Provide a list of separators (e.g., [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]). • Set chunk_size=200, chunk_overlap=20, length_function=len. Call split_text(text) and inspect results: • You may see a different number of chunks (e.g., 6 instead of 4) compared to the simple character splitter, because the recursive method respects document structure and splits more intelligently. • If your text has many clean breakpoints (headings, blank lines), you may not see the overlap in the printed output, even though chunk_overlap is set; that’s just because the splits landed on clean boundaries. To demonstrate overlap clearly, use text without natural breakpoints and restrict the separator list (e.g., only a space separator), with smaller chunk_size like 80 and chunk_overlap=20. Then print consecutive chunks to see repeated phrases (e.g., “…and it is also” appearing at the end of one chunk and the beginning of the next). Method 3: TokenTextSplitter The TokenTextSplitter splits by tokens (not just raw characters). Depending on the tokenizer, spaces and punctuation are accounted for in token counts. • Example settings: chunk_size=50 tokens, chunk_overlap=10 tokens. • Call split_text(text) and inspect the produced chunks. This method is often slower than character-based splitters but is useful when working with token-limited models (you want chunks measured in tokens, not characters). Making Overlap Visible (Examples) If you didn’t see overlap earlier, there are a few reasons: • Chunk size and overlap may be relatively small. • Your separators caused clean splits (e.g., splitting at headings/blank lines). To force visible overlap: • Reduce chunk_size. • Increase chunk_overlap. • Limit separators (e.g., only spaces) so splits are more likely to cross mid-sentence. Then print adjacent chunks to see repeated phrases across boundaries: • Example: chunk end “…is even longer than” and the next chunk starts with “…is even longer than…”. When to Use Which Splitter CharacterTextSplitter • Pros: Simple, predictable; good for structured text with clear delimiters. • Cons: May break mid-sentence if separators are not aligned. • Use when: Your text has clear, consistent delimiters and you want fast, straightforward splitting. RecursiveCharacterTextSplitter (default choice) • Pros: Respects text structure; tries multiple separators; best general-purpose splitter. • Cons: Slightly more complex; may take more time than simple character splits. • Use when: Default for most text; you want structure-aware splitting and good chunk quality. TokenTextSplitter • Pros: Measures chunks by tokens; aligns with LLM token limits. • Cons: Slower; depends on tokenizer behavior. • Use when: Working with strict token constraints or models where token budgeting is critical. Why Chunking Matters (Retrieval Perspective) At the end of ingestion, these chunks are embedded and stored in a vector database. During retrieval, a query may match content spread across multiple overlapping chunks. Overlap increases the chance that all relevant context is captured by the top-k retrieved chunks. This leads to better grounding and answer quality in the generation phase.'),\n",
       " Document(metadata={'section_name': 'Data Ingestion and Data Parsing Techniques', 'section_number': '3', 'total_videos': '9', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Data Ingestion And Parsing Techniques.pdf', 'video_title': 'Ingestion and Parsing PDF Documents', 'video_order': '4/9', 'topics': 'PDF ingestion, PyPDFLoader, PyMuPDFLoader, UnstructuredPDFLoader (intro),', 'difficulty': 'Beginner', 'doc_type': 'video'}, page_content='Super excited that we have now completed at least one specific part wherein we are able to read from a text document or text file, and then we are also able to split them into chunks. Right now it’s time to go ahead and show you how we can load PDF files, because this is one of the most common use cases. Many companies have a huge amount of PDFs, so it is necessary that you know how to read a PDF with LangChain. Inside my data/ folder, I have uploaded a pdf/ folder, and there is a file named attention.pdf. This specific PDF is a research paper. We will read this PDF, convert it into Document objects, and then see how we can apply splitting techniques over the extracted text. First, I’ll create another notebook/file for this part, e.g., data_parsing_pdf.ipynb, because PDF handling is a very common real-world requirement. Loading PDFs with LangChain (Approaches) We will import loaders from langchain_community.document_loaders and demonstrate three approaches (we will implement two now and introduce the third for later): 1. PyPDFLoader (often recommended, simple and reliable) 2. PyMuPDFLoader (fast, strong text extraction; supports images) 3. UnstructuredPDFLoader (we’ll cover later, useful for messy PDFs) Method 1: PyPDFLoader PyPDFLoader is a common and recommended way to read PDFs. • Create a loader with the file path, e.g., data/pdf/attention.pdf. • Call .load() to get a list of Document objects, one per page (typically). • Each Document contains: o page_content → extracted text for that page o metadata → fields like page, source, creator, producer, author, timestamps, etc. After loading, print diagnostics: • Number of pages (e.g., 15 pages loaded) • Page 1 preview → first 100 characters • Metadata → show how much information is automatically captured This is an easy way to read a PDF and immediately get page-oriented Documents you can later split and embed. Method 2: PyMuPDFLoader (fitz) PyMuPDFLoader uses the pymupdf (fitz) engine under the hood. • You may need to install pymupdf first. • Create the loader with the same file, call .load(). • Inspect the returned Documents and metadata. Why consider PyMuPDFLoader? • Generally fast • Often robust text extraction • Supports image extraction and richer PDF features If you see import errors, install the dependency (e.g., pymupdf) and re-run. (Later) Method 3: UnstructuredPDFLoader We will discuss UnstructuredPDFLoader in a later video when we dive into cleaning and advanced PDF processing. It is helpful for: • Complex layouts • PDFs with images, tables, headers/footers • Needing heuristic segmentation before chunking Quick Look at the PDF Example The example PDF is the classic research paper “Attention Is All You Need.” Using the loaders above, we obtain page-wise Documents. In the metadata, you’ll typically see fields like creator, producer, modDate, path, and the current page number. In the page_content, you’ll see the extracted text. This structure prepares the content for splitting and embedding. Loader Comparison (Cheat Sheet) PyPDFLoader • Pros: Simple, reliable; good for most PDFs; preserves page numbers; basic text extraction • Cons: Can struggle with image-heavy or weirdly encoded PDFs • Use when: You want a straightforward loader that works well on standard PDFs PyMuPDFLoader • Pros: Fast; strong text extraction; image extraction support • Cons: Requires pymupdf dependency; sometimes different extraction quirks • Use when: Speed matters or you need better extraction on tricky PDFs UnstructuredPDFLoader (preview) • Pros: Powerful heuristic parsing for messy layouts • Cons: Heavier dependency stack; more configuration • Use when: You need advanced, layout-aware preprocessing before chunking'),\n",
       " Document(metadata={'section_name': 'Data Ingestion and Data Parsing Techniques', 'section_number': '3', 'total_videos': '9', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Data Ingestion And Parsing Techniques.pdf', 'video_title': 'Handling Common PDF Issues', 'video_order': '5/9', 'topics': 'PDF parsing challenges, whitespace & newline cleanup, ligature fixes, OCR', 'difficulty': 'Beginner → Intermediate', 'doc_type': 'video'}, page_content='So we are going to continue our discussion with respect to data parsing for PDFs. Already in our previous video, we discussed two important libraries for reading PDFs—\\nPyPDFLoader and PyMuPDFLoader—and we saw how to read a PDF file, extract details, and inspect some of the metadata. In this video, we’re going to discuss common PDF challenges that you need to handle. The purpose is simple: if your PDF has plain text, parsing is easy. But PDFs often store text in complex ways (tables, columns, scanned images), include formatting artifacts (excess whitespace, broken lines, hyphenation), or contain special/encoded characters (like ligatures). We need to clean these issues so the downstream chunking, embedding, and retrieval are effective. Why PDFs Are Tricky • Tables & multi-column layouts can scramble text order. • Scanned PDFs require OCR to extract text. • Extraction artifacts: stray newlines (\\\\n), page headers/footers, hyphenations, broken words. • Special characters/ligatures: fi, fl, etc., due to font encodings. • Whitespace noise: multiple spaces or irregular spacing. We’ll look at a simple cleaning function to normalize raw text after extraction. Example: Raw vs Cleaned Text Suppose a raw page contains something like: \"Company Financial Report\\\\n\\\\n\\\\nThe financial performance for fiscal year 2024 shows significant growth in profitability. Revenue increased by 25%. ...\" with lots of extra spaces/newlines and encoded ligatures. A cleaning function can: • Split on whitespace and rejoin with single spaces (normalizing spacing). • Replace ligatures like fi → fi, fl → fl. • (Later) Add more rules for hyphenation, headers/footers, etc. After cleaning, the text becomes a single, readable paragraph with correct characters, ready for splitting and embedding. Building a Reusable Processor (Class Design) To make this robust, we’ll create a class called SmartPDFProcessor that handles PDF loading, cleaning, smart chunking, and metadata enrichment. Constructor (__init__) • Parameters: chunk_size=1000, chunk_overlap=100. • Initializes a RecursiveCharacterTextSplitter with: o chunk_size o chunk_overlap o separators=[\" \"] (split primarily on spaces for consistent mid-sentence handling) Private method: _clean_text(text: str) -> str • Normalizes whitespace (e.g., \\' \\'.join(text.split())). • Replaces common ligatures/encoded characters (e.g., fi→fi, fl→fl). • Returns cleaned text. (Extend later for OCR/table handling.) Public method: process_pdf(pdf_path: str) -> List[Document] 1. Load the PDF via PyPDFLoader(pdf_path).load() to get page-wise Documents. 2. Iterate pages with enumerate(pages). 3. Clean each page’s page_content with _clean_text. 4. Skip empty pages (e.g., if len(clean_text.strip()) < 50). 5. Create chunks with self.text_splitter.create_documents([clean_text], metadata=...). o Enrich each chunk’s metadata using the original page.metadata plus: ▪ page_number ▪ total_pages ▪ chunk_method (e.g., \"recursive_space\") ▪ char_count 6. Append all per-page chunks to a processed_chunks list and return it. This design ensures that every chunk in your vector database carries useful metadata for filtering, tracing sources, and improving retrieval quality. Running the Processor • Instantiate: processor = SmartPDFProcessor(). • Call: smart_chunks = processor.process_pdf(\"data/pdf/attention.pdf\"). • Expect a message like: Processed into 49 smart chunks (depends on your chunking settings and the PDF content). • Inspect metadata for a few chunks to confirm fields like page_number, total_pages, chunk_method, source, etc., are present. Why Metadata Matters When these chunks are embedded and stored in your vector DB, metadata allows you to: • Filter by page_number, section, or source. • Provide citations (page and document). • Improve retrieval precision (e.g., restrict to particular sections/pages in follow-up queries). • Debug/audit the pipeline when results look off.'),\n",
       " Document(metadata={'section_name': 'Data Ingestion and Data Parsing Techniques', 'section_number': '3', 'total_videos': '9', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Data Ingestion And Parsing Techniques.pdf', 'video_title': 'Ingestion and Data Parsing Word Documents', 'video_order': '6/9', 'topics': 'Word documents, Docx2txtLoader, UnstructuredWordDocumentLoader, Elements', 'difficulty': 'Beginner → Intermediate', 'doc_type': 'video'}, page_content='So we are going to continue the discussion with respect to data parsing techniques. Already in our previous video, we finished the task of data parsing PDFs and saw which common functionalities we can use to handle text data extracted from PDFs. We also showed how to create a class and how we’ll keep implementing each functionality as we go ahead. At the end of the day, we are still focusing on the main goal: read data from a data source, apply splitting, convert it into chunks, and while creating these chunks, add metadata. Later, we will see the advantages of this metadata during retrieval. In this specific video, under data ingestion, we will create one more file (e.g., data_parsing_doc.ipynb) and focus on Word documents. Your problem statements may include data in .docx format—project proposals, reports, HR policies, etc. Here’s an example: a project proposal for a RAG implementation. I’ve created a simple document with an executive summary, job responsibilities, and a few structured sections. We’ll read this Word doc, extract content, add metadata, and (optionally) chunk it. Let’s go step by step. Word Document Processing Overview For Word document processing, we’ll use two important loaders from langchain_community.document_loaders: 1. Docx2txtLoader — quick text extraction from .docx 2. UnstructuredWordDocumentLoader — structure-aware parsing using the elements pipeline You may also see Python libraries like python-docx and docx2txt in the environment for working with .docx directly, but here we’ll focus on the LangChain loaders so the output is already in Document form (page_content + metadata). Method 1: Docx2txtLoader (Simple Text Extraction) • Create a Word doc in data/word_files/ named proposal.docx (e.g., a RAG implementation proposal with objectives, scope, milestones). • Use Docx2txtLoader to read it: o loader = Docx2txtLoader(\"data/word_files/proposal.docx\") o docs = loader.load() → returns a list[Document] • Print diagnostics: o len(docs) → how many Document objects were returned (often 1 for simple docs) o docs[0].page_content[:100] → preview content o docs[0].metadata → see fields like source (file path), etc. If everything is fine, you should see content like: Executive Summary … and metadata showing the file path. This confirms the loader converted the Word file into a Document with content and metadata. Method 2: UnstructuredWordDocumentLoader (Elements Mode) For more complex .docx files with headings, lists, tables, and mixed formatting, use UnstructuredWordDocumentLoader. • Initialize with mode=\"elements\" to preserve structural elements: o un_loader = UnstructuredWordDocumentLoader(\"data/word_files/proposal.docx\", mode=\"elements\") o un_docs = un_loader.load() • This may take longer on first run (heavier processing). The result is a list[Document] where each item may correspond to an element: o Example elements: Title, NarrativeText, ListItem, Table, etc. • Print diagnostics: o len(un_docs) o For the first few elements, print doc.metadata and doc.page_content You might see output like “Loaded 20 elements” with metadata including source, file_directory, file_name, last_modified, languages, category, etc. The category reflects the element type (Title, NarrativeText, etc.). Tables are often parsed as separate elements—very handy for downstream processing. What About Chunking? When mode=\"elements\" is used, the loader already segments the document into meaningful pieces. You can: • Use elements as-is as chunks (each element is small and coherent), or • Further apply a text splitter (e.g., RecursiveCharacterTextSplitter) to long elements (e.g., long NarrativeText) with settings like chunk_size=500, chunk_overlap=50, and inject metadata such as section, category, element_index, source, etc. This two-level approach (elements → splitter) gives excellent control and often better retrieval quality. Example Workflow (Putting It Together) 1. Load the .docx using one of the loaders above. 2. Inspect the returned Documents → check page_content and metadata. 3. Normalize/Clean the text if needed (whitespace, ligatures rare in .docx but can normalize newlines). 4. Chunk: o If using Docx2txtLoader, run a splitter over the full text. o If using UnstructuredWordDocumentLoader with mode=\"elements\", treat each element as a base chunk and optionally split further. 5. Enrich Metadata for each chunk (e.g., source, category, heading, element_id, char_count). 6. Store the chunks in your vector database. Notes on Performance & Trade-offs • Docx2txtLoader o Pros: Fast, simple, minimal dependencies o Cons: Loses structural context (headings, lists, tables) o Use when: You just need text quickly from clean .docx • UnstructuredWordDocumentLoader (elements) o Pros: Preserves structure; identifies element categories; captures tables as separate elements o Cons: Slower; heavier dependencies o Use when: You care about document structure, need table extraction, or want element-aware chunking Metadata Matters (again!) As with PDFs, rich metadata on Word-derived chunks improves retrieval and explainability: • Filter by category (Title vs NarrativeText vs Table) • Track source and file_name for citations • Use heading or inferred section titles for section-aware retrieval Wrap-up In this video, we learned how to ingest Word documents using Docx2txtLoader and UnstructuredWordDocumentLoader. We saw how elements mode can yield multiple Document elements with structural metadata, which is great for high-quality chunking. From here, you can apply splitters as needed, enrich metadata, embed, and store in your vector DB. In the next video, we’ll discuss CSV and Excel files—how to read them and prepare the content for splitting and embeddings. Take care. Section 3: Data Ingestion and Data Parsing Techniques Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith Section Number: 3 Total Videos: TBD Date Created: 2024'),\n",
       " Document(metadata={'section_name': 'Data Ingestion and Data Parsing Techniques', 'section_number': '3', 'total_videos': '9', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Data Ingestion And Parsing Techniques.pdf', 'video_title': 'Parsing CSV and Excel Files', 'video_order': '7/9', 'topics': 'Structured data, CSVLoader, UnstructuredCSVLoader, Pandas, Excel processing,', 'difficulty': 'Beginner → Intermediate', 'doc_type': 'video'}, page_content='So we are going to continue the discussion with respect to data parsing and data ingestion. Already in our previous video, we have seen how to parse Word docs. And whenever we talk about all the previous parsing techniques that we have learnt, it is mostly for unstructured data, right? So in this particular video, we are going to go ahead and see how we can ingest and parse structured data like CSV and Excel files. Okay. So with respect to this, the first thing is that I will just go ahead and quickly create one more file. So let’s say here I will go ahead and write csv_excel_parsing.ipynb. And this will basically be my fourth file. Okay I will go ahead and select the kernel quickly. I will create a markdown and let me just go ahead and write CSV and Excel files—right—and this is nothing but my structured data. See, the main aim of creating this is that I really want to show you each and every technique of parsing from different data sources. Okay. So that tomorrow, if you’re working on anything—right—if there is a CSV file, if there is some kind of Excel file, you should know what you’re actually doing or how to probably go ahead and ingest that and do the normal parsing and convert that into a document structure. Okay. So the first thing that I’m going to import is nothing but, since we are going to work with CSV or Excel file, I think pandas will definitely be required. So I will go ahead and install pandas. Okay. So right now pandas is not there. But let’s see whether LangChain is also coming or not. Okay. So this is the problem that I’m facing. And you may also face whenever you create a new file—right—at that point of time, the libraries will not get loaded and you will not be able to quickly get all the suggestion, like how we used to get it over here. Okay, so for this you just need to go ahead and restart it once again. Restart your kernel completely. Okay. And once you restart your kernel I think you should be able to see it again. Okay. So first of all what I will do, I will quickly go ahead and see whether in my requirements.txt I have pandas or not. So here pandas is not there. I will go ahead and import pandas. Pandas. So here what I will do I will quickly write uv add -r requirements.txt. Okay, so pandas has been imported already I see okay somewhere pandas may be there and now I will quickly go ahead and write import pandas. Okay, so let’s see. This is executing perfectly fine. Okay. So with respect to this, if you know how to use pandas, I think that will be more than sufficient in order to start, right. One more thing. What I will do, I will just go ahead to my folder location where I’m writing the code. I will again open up VS Code. Okay, I’ll open the same project in VS Code. There is a reason why I’m doing it and I’ll just let you know in some time. Okay? Why I am actually doing that right? So first of all what I will do, I will just close this. I will just close—or let me just close it quickly from here, okay. And then I will just go ahead and open my VS Code. Now you’ll be able to see that here. I will go ahead and select my kernel. So all the kernels will be getting loaded quickly. Yeah perfect. So I have loaded my kernel. Now you can see that my pandas is basically coming. See whenever things are not getting loaded, just try to close your VS Code or try to restart your kernel. I restarted my kernel. That thing also did not work. So with respect to every video, you know, I definitely will tell you some or the other things over here. So don’t get bored with respect to that, okay? Then I’m going to go ahead and import the os because this kind of suggestions I will not be getting like—when I say suggestions, the highlighting will not be coming up. And if highlighting is not coming up like you’ll not understand where you’re making a spelling mistakes and all. And then later on in the debugging section you’ll be doing all those things. Okay. Now quickly what I will do, I will go ahead and make one directory. So since we are working with CSV and Excel file, what I will do first task is basically to create these files. Okay. Then I will show you how you can go ahead and read that file and do the parsing also. So first of all what I’m doing I’m creating a structured_files folder inside my data folder. So I will just go ahead and execute it. So if you see over here inside my data there is a structured_files folder, okay. Now inside this folder I will go ahead and create a simple DataFrame. Okay. So if you know pandas simple DataFrame: see product—this is my column name. These are my data that is present inside the column. Another column—these are my data; price—these are my data; and description—these are my data, right. And this I’m trying to convert into a DataFrame, and I’m saving that DataFrame in a file which is called as products.csv. So if you know about at least some of the data pre-processing techniques, if you have used it, I think this is the most simple way of creating a CSV file. Okay. So I’ll quickly go ahead with this because these are some basic things which you should know in Python. Okay. Now inside my data folder, if I go ahead and see there is a structured_files folder and there is a CSV file, and this is how my CSV file looks like. So one CSV file we have actually created it. And now similarly I will go ahead and create one Excel file also. Okay. Now for Excel file I will again take some data and probably print it in over here. So let’s say this is one important functionality which is called as ExcelWriter inside pandas. So here is my inventory.xlsx as a writer. And I’m trying to convert this into Excel. I’m using a sheet_name=\"products\" and index=False. So here is my entire summary data which I’m going to put inside this particular Excel file. And then I’m taking the summary data, converting into a DataFrame and finally converting into an Excel with this particular sheet name and writer and index=False where I don’t take my separate column name. Right. Whatever column is specifically given over here that only it will be taken. So this code actually helps you to create some kind of Excel file over here. So if you just go ahead and execute this. So here, No module named openpyxl is coming. So what I will do, I will just go ahead and write it: openpyxl. So this is a library that we are going to specifically use it. So I will open my command prompt, uv add -r requirements.txt. Okay. So here you can see openpyxl (and et-xmlfile) is got loaded. Right. So I will go to this and again execute this. Now I think it should work. So at the end of the day you will be able to see that along with my CSV file there will also be an XLSX file—right—Excel file. So two important files I have created: products.csv and inventory.xlsx. Very simple. I’ve just used pandas. One function is pd.ExcelWriter where we have to just give the sheet name and I will be able to create it. Right. And these are my information that is present over here. Now let’s start working on CSV processing. Right. So here we are going to specifically go ahead and work on how, if we are reading a CSV file, how should we go ahead and read it? And how should we basically convert that into a Document? Okay. Again, from LangChain there is an inbuilt function which is called as CSVLoader. So I will go ahead and import from langchain_community.document_loaders import CSVLoader. Okay. And then from langchain_community.document_loaders import UnstructuredCSVLoader. So these two specific libraries we are going to use in order to load a CSV file. Okay. So first path is nothing but CSVLoader. And we will just try to see how to work with the method one. So in the method one you can see CSVLoader (row-based Documents). I’m using the CSVLoader; I have to give my path for the CSV file. We have to use encoding as UTF-8 and here additional you can give delimiter and quotechar. So delimiter I’m actually giving , since it is CSV (comma-separated file). We can give this. Quote character is nothing but quotes \". Then we are using csv_loader.load() and I’m printing all the information like what are my documents and all. If you want I can also go ahead and print all the CSV documents. Okay csv_docs. So let’s go ahead and print this. So here you can see this is one Document metadata—all the information. page_content—you can see everything is available inside that page content. Right. Yeah one. Then again my another Document metadata automatically I’m able to read it. So based on row by row—I think it is row by row—yeah. See row=2, row=1. All this information is here. So row=2 is one Document. Okay. So one Document. So if you see this there are total how many rows? There are five rows. One, two, three, four, five. Right. So (0, 1, 2, 3, 4). Right. And if you go ahead and see over here that many number of Documents you’ll be able to find out. Right. And here you can see row=4 and all the information—\"1080p with noise cancellation\"—is there. Here you can see 1080p with noise cancellation: these are my information of the columns. Quickly you can see that once we are reading this, automatically row by row it is converting into Documents. Okay. And the first Document is this information. The metadata information is also here. And here you can see, since it is reading a CSV file, automatically this function is so good that it is adding a metadata as row=0. See, at the end of the day, you can also go ahead and create your own custom Document with additional metadata information. So this is important that you should know how to read something and how you can go ahead and customize this. I can also do this customization. Right. At the end of the day, I’m getting all the format in the Document. I can go ahead and add more metadata if I want, which I’ve already shown you before also, right, with respect to different different things. So this was one. The second technique that you can specifically use is one more UnstructuredCSVLoader. Okay. So UnstructuredCSVLoader—what we can actually do and how we can actually read it—that is what we are basically going to discuss now. Okay. Now quickly, let’s do one thing. Let’s try to see that before going to unstructured, you may be thinking that, \"Krish, can we add more additional metadata information and all?\" So for that what I will do, I will create a custom CSV processing. Right. And let’s see this. Okay. So I’m going to copy and paste this code—see Custom CSV Processing. So for this I will be requiring List. So I will go ahead and write from typing import List. And then I will also go ahead and import my Document. See I don’t need to by-heart everything like where exactly it is or not. Okay. But it is good to know all these things. But it’s okay—with practice you will get to know from where you are importing all the things, right? So from typing import List and this one. See here what we are trying to do is that we are creating our custom CSV processing for better control. So here we have created a function process_csv_intelligently(file_path) -> List[Document]. I’ll just give the file path and this will return a list of Documents. Okay. So it is going to return a list of Documents where we are also going to do the chunking—each and everything—okay. Automatically the chunking, parsing along with the metadata information in each and everything. So first of all I’m reading the CSV file path. I’ve created an empty documents. Strategy 1 is “one Document per row with structured content.” So what I will do, I will iterate through every row. And for that we use df.iterrows(). Okay, now what we are basically going to do is that I’m providing some of the content information over here. Right? This is my content information. Now see next step is that I’m creating the Document. So for the Document I will be using this content. This content is coming from where? See it is coming from this information—right—row by row. All this specific information I’m going to put inside this content. So this becomes my page_content. Then metadata. So metadata I have my file_path. If you see file path is here. Then row_index from this index. You can get row_product, row_category, row_price and product_info. The same thing this CSVLoader is actually doing. And here you can see that how easily we have actually written our own custom function for the CSV processing. And that is how you just go ahead and, you know, create your own information with respect to this write-up. When you want some more additional information, you can just go ahead and write it. Okay. Now I definitely don’t want to stop it over here—like you can do it from your own. Right. So once I execute this, you will be able to see that this CSV processing is there. Now, if I just call this function and if I give my path—my path, what path it is? Let’s see this path. So let’s say I give this path my CSV path, okay, and I give it over here. Now what output I’m actually going to get? Okay. So I’ll just go ahead and write return documents. Okay. Now, if I go ahead and execute this so you can see over here all the information is over here. row=0, row=1, row=2, row=3, row=4—all the information with respect to this is there, okay. So I hope you got an idea about how you can do the custom CSV processing. At the end of the day, this is what you really want to create and how you want to create it. As you understand multiple use cases, you’ll get to know about it. Okay, now there may also be one more scenario, where I also want to probably go ahead and create a summary, something like that. Right. That I will show you as we go ahead. But just to make sure to show you the two differences like CSV processing strategies that we have discussed till now—you know: row-based and whether you should go ahead with this custom one—I’ll just write the differences so that you get to know about this. Okay. So here—CSV Processing Strategies: • Row-based (CSVLoader) → Simple: one row → one Document; good for record lookups; loses table context. • Intelligent/custom processing → Preserves relationships, create summaries, rich metadata, better for Q&A. Because here you can add more context, more information. In short, when you’re adding more context, here you can also say that, \"Hey, does this CSV have some other relationship with some other CSV file also?\" Right. So that context can be put up in this metadata information. So based on the use cases, you think whether you need to probably go ahead with using CSVLoader or intelligent processing. Okay. Now coming to the Excel processing—again, guys, when we say coding, we should not restrict ourself in one thing. I should definitely show you multiple ways how to do this. So now let’s go ahead with Excel processing. For Excel processing I will show you one technique. So here you can just go ahead and see this, okay. And you’ll be able to understand it anyway. Method 1: Using pandas for full control • Create a function process_excel_with_pandas(file_path) -> List[Document]. • Read the Excel file with pd.ExcelFile(file_path). • For sheet_name in excel.sheets (or excel.sheet_names), load each DataFrame. • Build a sheet_content string representing the table (you can format as CSV/TSV or Markdown if you want). • Create a Document with page_content=sheet_content and metadata including: source, sheet_name, n_rows, n_cols. • Append all to a documents list and return. Once I execute this quickly I can just go ahead and print things. Okay. So here I’m giving my excel_docs, len(excel_docs), each and everything. So here you can see two sheets have been processed. So if you go ahead and see my excel_docs this is what is the information that I have: inventory, product summary—each and every information with respect to this, okay. So it is just reading from the inventory file. Method 2: UnstructuredExcelLoader • You can also use UnstructuredExcelLoader whenever you want. • Import: from langchain_community.document_loaders import UnstructuredExcelLoader. • Create the loader with the file path; call .load(). • As with other unstructured loaders, this may take time on larger or complex workbooks. • Advantages: handles complex Excel features, preserves formatting info. • Disadvantages: requires the unstructured library; slower.'),\n",
       " Document(metadata={'section_name': 'Data Ingestion and Data Parsing Techniques', 'section_number': '3', 'total_videos': '9', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Data Ingestion And Parsing Techniques.pdf', 'video_title': 'JSON Files Parsing and Processing', 'video_order': '8/9', 'topics': 'JSON ingestion, JSONLoader with jq_schema, Custom JSON parsing, Document', 'difficulty': 'Beginner → Intermediate', 'doc_type': 'video'}, page_content='So in this specific video we are going to discuss about JSON parsing and processing. Let’s say you have JSON data—how do we ingest it, parse it, process it, and finally convert it into a Document structure? There may be scenarios where you are communicating with APIs and the responses are in JSON. This becomes a very important use case, and many practical pipelines will involve JSON. First, we’ll create some JSON files to work with. Creating Sample JSON Files We’ll start by importing json and os, and creating a directory data/json_files/ (using os.makedirs(..., exist_ok=True)). File 1: company_data.json A nested JSON with a structure like: • company: \"TechCorp\" • employees: a list of employee objects (each with id, name, role, skills, projects) • projects: list of project summaries • departments: list of department names We’ll json.dump(...) this json_data to company_data.json. File 2: events.json Another JSON containing a list of event logs with fields like timestamp, event, user_id, page, etc. We’ll save this to events.json. After this step, you should have: • data/json_files/company_data.json • data/json_files/events.json Method 1: Using LangChain’s JSONLoader (with jq_schema) We’ll import JSONLoader from langchain_community.document_loaders. Sometimes you don’t want the entire JSON—just a specific subtree (e.g., each employee). JSONLoader supports a jq_schema parameter: a jq expression to select parts of the JSON. Example: Extract each employee object as a separate Document. • Initialize: employee_loader = JSONLoader( file_path=\"data/json_files/company_data.json\", jq_schema=\".employees[]\", text_content=False ) • Call: employee_docs = employee_loader.load() Notes: • jq_schema=\".employees[]\" traverses into employees and emits each element. • text_content=False returns the full JSON object in page_content (serialized), not just plain text. • You may need to install jq support (the Python package used by the loader); ensure it’s in your environment. Result: • You’ll see something like: “Loaded 2 employee documents.” • Each Document has: o page_content: the serialized JSON for one employee o metadata: includes source and other fields This is perfect when you want one vector per employee for retrieval. Method 2: Custom JSON Processing (Full Control) While the loader is convenient, sometimes you need more control. Let’s build a small custom function that reads the JSON and emits Documents with rich metadata. Function sketch: process_company_json(file_path) -> List[Document] 1. with open(file_path) as f: data = json.load(f) 2. Iterate over data.get(\"employees\", []). 3. For each employee, construct a content string (or keep JSON) that summarizes key fields: id, name, role, skills, and a list of projects. 4. Create a Document: o page_content: the formatted text (or json.dumps(emp, ensure_ascii=False)) o metadata: { \"source\": file_path, \"record_type\": \"employee\", \"employee_id\": emp[\"id\"], \"name\": emp[\"name\"], \"projects\": [..], \"skills_count\": len(emp.get(\"skills\", [])) } 5. Append to documents and return the list. This approach lets you normalize text, rename fields, filter sensitive keys, and inject domain-specific metadata to improve retrieval, filtering, and analytics later. Applying to Event Logs (Assignment-style Tip) For events.json, follow a similar pattern: • Iterate events, create one Document per event (or per user, grouped). • Metadata could include: timestamp, event, user_id, page, session_id, etc. • Consider time bucketing (e.g., day/week) added as a metadata field for temporal filtering. • For groupings, you might aggregate events by user_id or session_id and create one Document per group with summarized page_content. Document Structure Recap Regardless of the method, the goal is to convert raw JSON into a consistent Document format that your RAG pipeline expects: • page_content: the text that will be embedded (either pretty-printed JSON or a human-readable summary) • metadata: key fields for filtering, citations, and tracing (ids, types, timestamps, source paths) Once you have Documents, you can split (if long), embed, and store them in your vector database. When to Use Which Approach? • JSONLoader + jq_schema: Fast, declarative extraction when your target subtree is clear (e.g., employees, orders, items). Great for 1-to-1 record → Document. • Custom processing: When you need transformations, redaction, normalization, rollups/summaries, cross-record context, or custom metadata fields. Often, a hybrid works best: use jq_schema to isolate the right records, then post-process each record into your preferred text/metadata format.'),\n",
       " Document(metadata={'section_name': 'Data Ingestion and Data Parsing Techniques', 'section_number': '3', 'total_videos': '9', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Data Ingestion And Parsing Techniques.pdf', 'video_title': 'SQL Databases Parsing and Processing', 'video_order': '9/9', 'topics': 'SQLite setup, Creating tables, Inserting records, LangChain SQLDatabase &', 'difficulty': 'Beginner → Intermediate', 'doc_type': 'video'}, page_content='So we are going to continue the discussion with respect to pre-processing and parsing. Already in our previous video, we completed JSON pre-processing strategies. We saw multiple examples—how to create JSON files and data, how to read JSON, and finally convert it into a Document along with metadata. We also saw how to do custom JSON pre-processing to create the Document structure itself. Right. So all these specific things have been discussed. Now one more problem statement that we are going to work on. See, in data ingestion and parsing, what are we trying to do? If we have a PDF file, a Word doc, a TXT file, a JSON file—\\nwe learned techniques to read those files, extract text/data, and convert it into a Document structure. In the Document structure, you have two important things: content and metadata. And after this, further you can divide into chunks. Sometimes the inbuilt functions you use will also internally divide the data into chunks. In this particular video, what we will be doing is specifically working with a SQL database as a data source. Because whenever we talk about SQL databases, we are basically talking about structured data. (If I talk about a NoSQL database like MongoDB, in short we are talking about JSON—key-value pairs.) From a SQL database, how can we read the data and convert it into a Document structure? That is what we will see. Creating a Sample SQLite Database For learning, I’ll make it simple using SQLite. 1. Imports & setup • import sqlite3 • import os • Create directory: data/databases/ using os.makedirs(..., exist_ok=True). 2. Create database file • Path: data/databases/company.db • Connection: sqlite3.connect(path) • Cursor: con.cursor() 3. Create tables • employees: (id INTEGER PRIMARY KEY, name TEXT, role TEXT, department TEXT, salary REAL) • projects: (id INTEGER PRIMARY KEY, name TEXT, status TEXT, budget REAL, lead_id INTEGER) 4. Insert sample data • Employees (list of tuples): o (1, \"John Doe\", \"Senior Developer\", \"Engineering\", 120000.0) o (2, \"Jane Smith\", \"Data Scientist\", \"AI\", 135000.0) o (3, \"Mike Johnson\", \"Project Manager\", \"PMO\", 110000.0) o (4, \"Sarah Williams\", \"QA Engineer\", \"Quality\", 90000.0) • Projects (list of tuples): o (101, \"RAG System\", \"In Progress\", 250000.0, 1) o (102, \"Data Platform\", \"Completed\", 400000.0, 2) o (103, \"Mobile App\", \"In Progress\", 180000.0, 3) o (104, \"Testing Revamp\", \"Planned\", 120000.0, 2) Use cursor.executemany(...) to insert, con.commit() to save, and close the connection. Database Content Exploration (LangChain Utilities) We’ll use LangChain community utilities: • from langchain_community.utilities import SQLDatabase • from langchain_community.document_loaders import SQLDatabaseLoader Method 1: SQLDatabase utility • Initialize: db = SQLDatabase.from_uri(\"sqlite:///data/databases/company.db\") • Explore: o db.get_usable_table_names() → e.g., {employees, projects} o db.get_table_info() → schema DDL + sample rows This quickly confirms the tables, schema, and a few rows. Custom SQL → Document Conversion (with Metadata) Our main aim is reading from the database and turning it into Documents. We’ll write a helper: sql_to_documents(db_path: str) -> List[Document] Steps: 1. con = sqlite3.connect(db_path); cur = con.cursor() 2. List tables: cur.execute(\"SELECT name FROM sqlite_master WHERE type=\\'table\\'\"); tables = cur.fetchall() 3. For each table: o Column info: PRAGMA table_info(table_name) → column names o Rows: SELECT * FROM table_name → fetch all o Build a page_content string summarizing table name, columns, total records, and include a few sample rows for context o Create a Document: ▪ page_content: assembled summary + samples ▪ metadata: { \"source\": db_path, \"table\": table_name, \"record_count\": len(rows), \"type\": \"sql_table\" } o Append to documents 4. (Optional) Relationships: run a join to capture cross-table context, e.g.: 5. SELECT e.name AS employee_name, e.role, p.name AS project_name, p.status 6. FROM employees e 7. JOIN projects p ON e.id = p.lead_id; Convert the result into a human-readable relationship Document with metadata like { \"source\": db_path, \"type\": \"employee_project_join\" }. 8. Return the documents list. When you call it with \"data/databases/company.db\", you’ll see a couple of Documents—\\none per table—and an extra Document for the join. Each Document’s page_content includes table summaries and sample rows. From here, you can apply a text splitter (e.g., RecursiveCharacterTextSplitter) to produce chunked Documents ready for embeddings. Why Metadata Matters (again) Adding fields like table, record_count, type, and source enables: • Filtering answers by table or record type • Better traceability/citations back to the DB • Targeted retrieval (e.g., prefer join-derived context for cross-entity questions) Wrap-up In this video, we: • Created a SQLite database with employees and projects • Inserted sample data • Used LangChain’s SQL utilities to inspect schema & rows • Built a custom SQL → Document pipeline, including a join-based relationship Document • Prepared the output for chunking → embeddings → vector store This is a foundational pattern you can adapt to Postgres/MySQL/etc. by switching the DB URI and SQL queries. In the next video, we’ll continue expanding ingestion strategies and wire everything into the embedding/vector database steps. Take care.'),\n",
       " Document(metadata={'section_name': 'Introduction to RAG', 'section_number': '1', 'total_videos': '4', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and Langsmith', 'date_created': '2024', 'source_file': 'intro_to_rag.pdf', 'video_title': 'Introduction to RAG', 'video_order': '1/4', 'topics': 'RAG fundamentals, Retrieval, Augmentation, Generation, Architecture overview', 'difficulty': 'Beginner', 'doc_type': 'video'}, page_content='So I\\'m super excited to start the series of videos on RAG. In this video and in the upcoming series of videos, we are going to understand everything about retrieval augmented generation, a very important topic currently. We will talk about various architectures. We\\'ll talk about how to build a RAG pipeline. We\\'ll talk about how to build generative AI with RAG, how to build generative AI applications with RAG. If I talk about the current scenario, more than 80 percentage of the business use cases currently who are working in generative AI and AI, they are building RAG applications. RAG applications. So it is really important for anyone who is specifically interested in generative AI or genetic AI. They need to know how RAG architecture is basically implemented. RAG Architecture Overview Now, with respect to the RAG architecture here, I have shown you one type of architecture where we have three important phases: 1. Document Ingestion phase 2. Query processing phase 3. Generation phase When we talk about retrieval - what exactly is retrieval? We will discuss about what exactly is augmented. Will discuss about what exactly generation is what we are going to discuss about. And just by seeing this architecture, there are so many things that has been provided over here, but our learning pattern will be in such a way that we will learn one topic at a time. We will implement multiple use cases and at the end of the day, we will try to understand each and everything. And how do you build real world applications as we go ahead? Definition of Retrieval Augmented Generation First of all, as usual we will go ahead with the simple definition. What exactly is retrieval augmented generation? Simple definition: It is a powerful technique that enhances AI language models by combining the generation capabilities with external knowledge retrieval. So here you can see again, it is just enhancing the AI language model or any large language model by combining the generation capabilities with external knowledge retrieval. Understanding RAG Through Analogies RAG is like giving an AI assistant access to a library where while it\\'s answering questions, instead of relying solely on what it has learned from training, the AI can look up specific, current or specialized information from external sources before generating its response. Think of it in this way: • Traditional large language models are like students taking a closed book exam. They can only use what they memorized. • RAG enabled models are like students in an open book exam so they can reference materials to provide more accurate details and up to date answers. Basic LLM Limitations Example Now here you can see that whenever we have any LLM model. Let\\'s consider that this is my LLM model. I hope everybody knows about what is LLM. Now in this LLM model, let\\'s consider this our basic LLM model. And this model is trained on data till July 2023. With respect to this particular model, you will be seeing that whenever we give an input to this model, this LLM, since it has been trained with data before July 2023, it will be able to give you some kind of output. So here it will be able to generate some kind of output. But what if I ask a question \"hey what is the current news?\" Like let\\'s say today date I give it right. This LLM will not be able to answer because obviously it is trained on this specific data. Right. Till this specific data itself. So LLM will not be able to answer this. Solution 1: Integration with External Tools So what LLM does is that if we provide LLM integration with some third party tools, third party tools, let\\'s say one of the tool is a web search tool. So let\\'s say I have given some web search tool option over here with respect to this particular integration. Now with this web search, what it will do whenever we give this question, \"what is the current news\", the LLM will be able to make a call to this web search tool and it will be able to get the response. Once it gets the response, the LLM will be able to provide you the output saying that hey, today\\'s news is this. So here the dependency is there that there is some external kind of tool which is nothing but a web search API. And it is able to give you the answer. Solution 2: Company-Specific Information with RAG Now this is perfect right. Now in this particular use cases you will be able to see that we have dependency on external tools. Now, let\\'s say if I go ahead and ask, this is my company XYZ. Now for this particular company I say, \"hey what is the leave policy or what is the recent leave policy of XYZ?\" Let\\'s say inside this company I\\'m an employee XYZ company. I\\'m an employee. And I want to basically know what is the recent leave policy of XYZ. So I asked this particular, let\\'s say this is a chatbot that has got created for this work only. Where I can ask anything company related information and this chatbot will be important because this chatbot is a kind of chatbot, which is important to this particular company because it is being able to provide you some kind of information. Now, in this particular question, the LLM will be dependent on some kind of retriever. So let\\'s say this company information, whatever information is there, the current information, everything. It will be stored in some kind of database. So let\\'s say this is my database. And this database we say it as vector database. Understanding Vector Databases So as we go ahead we will learn more about this. What exactly is vector database. How does it work. But I\\'ll just give you an idea. So this is one kind of database which is called as vector database. Now inside this vector database if you know, vector database usually stores text data in the form of vectors. Now whenever I ask this question \"what is the recent leave policy of XYZ\". So what this will do, it will try to interact with this vector database. Because these all data information is stored already in this vector database. And based on this we will be able to get a specific response. The Three Components of RAG: R-A-G Now this kind of communication when where we are trying to communicate with some kind of vector database. And then we take this data from the vector database as a context. And the LLM will be able to give you the output, the summarized output. So here three important things are basically happening. And that is what RAG is all about. So here I have RAG: • R - Retrieval • A - Augmented • G - Generation Retrieval (R) Retrieval basically means what? Since I am communicating with the database. So this is my vector database. So we are retrieving some content from this particular database. We are retrieving some content from this database. Generation (G) LLM taking this context and it is generating some kind of output. So here it can be a summarized output. It can be any kind of output. So some output is basically coming up. That basically means this LLM along with the context that it got from the retriever is generating something. Augmented (A) Now what does augmented basically mean? Now before I go ahead and talk more about augmentation, now the next question arises how do we go ahead and retrieve from this vector database. So here different metrics will be used like similarity metrics. Similarity metrics or similarity search. There will be a similarity search. So there can be any kind of different metrics. We\\'ll learn more about this. So in short what I\\'m doing is that I am querying a vector database. I\\'m getting the context. And then I\\'m trying to generate a summarized output. Understanding Augmentation in Detail Whenever we are retrieving any content from this particular vector database or any context from this vector database, augmentation basically means that we will try to enrich the retrieved content. Let\\'s say from my vector database I got my query was something like, let\\'s say that I\\'m just trying to probably, let\\'s say from this particular retrieval I got something like this: I say I got an answer like \"revenue increased by 10%\". So let\\'s say in the retrieval, when we are retrieving from the vector database based on a query, we got this result. So when we say augmentation how we are enriching this content you will be able to see. If this is the text that I\\'m getting from the vector database we will try to add more metadata. Now how we will be adding more metadata? Augmented basically means the same thing. So augmented for this specific example will look something like this: Text: \"Revenue increased by 10%\" Source: Tesla Annual Report Date: 2024 The next thing that we are going to get is source. Source is nothing but let\\'s say Tesla. It can be a Tesla annual report. It can be any company report. Let\\'s say I\\'ve asked, let\\'s say the query is related to Tesla. So I can get the source. Then I can go ahead and add date. These all are metadata. Now with the help of this metadata, you know at the end of the day your generation, the summarized output will be really, really, really good. So this way we will be able to very accurately provide an output. Summary of RAG Process So this is what is all about RAG. What exactly Retrieval Augmented Generation basically means: 1. Retrieval is nothing but we are hitting a query. We will try to based on an input query. We will try to search from the vector database. And then it will do a similarity search. Get the documents or get the some kind of content which is matching this particular vector database. 2. Augmentation - Through the augmentation we\\'ll try to enrich the retrieved content. 3. Generation - Finally that enriched content that we are giving it to the LLM in the form of context will generate a summarized output. So this is all about retrieval augmented generation which is nothing but RAG. Now here you\\'ll be able to see that what all things are basically there step by step. We\\'ll try to understand as we go ahead.'),\n",
       " Document(metadata={'section_name': 'Introduction to RAG', 'section_number': '1', 'total_videos': '4', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and Langsmith', 'date_created': '2024', 'source_file': 'intro_to_rag.pdf', 'video_title': 'Some Examples and Advantages Of Using RAG', 'video_order': '2/4', 'topics': 'RAG workflow, Similarity search, Augmentation details, Real-world examples,', 'difficulty': 'Beginner-Intermediate', 'doc_type': 'video'}, page_content='So we are going to continue the discussion with respect to RAG already. We got some idea at least to make you just understand what is RAG and how does RAG actually work. One is retrieval augmented generation. One very important thing that I missed out in the previous video is that whenever you make an input query, and when we say that, hey, we are going to do some kind of search from the vector database. This is something like a similarity search. That basically means based on this input, whatever will be matching that will be available over here will be provided as a context. And then with the help of augmentation, we enrich this retrieved content by adding more metadata. And finally, we give it as a context to the LLM which will be generating the summarized output, which is the part of generation. Breaking Down RAG Components So in short, if I talk about retrieval augmented and generation here: For Retrieval I will basically say, hey, we are finding relevant information. We are finding relevant information from where? From vector databases. And there are various types of vector databases which we will be learning in our course. For Augmentation Now with respect to the augmentation what exactly it is. It is nothing but enhancing which I have already explained. Enhancing context with metadata. And why we are doing this? Because it will actually, our LLM will be able to answer much more accurately based on the input. We will see that you know how things are basically going to happen and all. For Generation And finally, with respect to the generation here, you can see it is producing the answer. So these are the three main important things with respect to any RAG architecture. Why Does RAG Actually Matter? Now let me ask one very important question. And this question is just like everybody will be probably thinking about it. Why does RAG actually matter? You know, why is RAG really helpful? I\\'ll talk about various companies who are able to just do some amazing things with respect to RAG, you know, are they able to save huge amount of money? Bigger companies like JP Morgan and all. And I will probably share you the entire links also if you want. But amazing work with the help of RAG they are actually able to do. Restaurant Chef Analogy Now let\\'s take one simple example and let me explain more about RAG because it is important that you understand with a simple example itself. Let\\'s say, and then we\\'ll try to also compare this with LLM. Let\\'s say there is a restaurant. So I will just try to draw some restaurant over here. So let\\'s say this is my restaurant. And this restaurant over here let\\'s say has a chef. Now you can consider this chef as an LLM. Now let\\'s say that there is one customer and this restaurant is somewhere in some country. So let\\'s say the country A or it is in this country A. Now, let\\'s say in this restaurant there is a person who has come from country B. And he has ordered something. So this person is basically ordering something to the chef. Scenario 1: Chef\\'s Limited Knowledge (Hallucination) If the chef knows how to cook it, obviously chef will directly cook it. But if he or she does not know how to cook it because they may not know by heart. They don\\'t know the recipe of this particular dish. Let\\'s say this dish is from the European side. And let\\'s say this dish is in some country like India. Obviously some of the Indian chefs will not know how to properly prepare this. Why chef is not able to properly go ahead and create this particular dish or prepare this dish? Because it is not trained with this kind of dishes. It is basically trained mostly with country of India dishes itself. It knows how to cook biryani. It knows how to cook Indian food specifically. Now, in this particular scenario, what chef can actually do in order to prepare the food for this person: 1. One thing is that chef can just guess something and he or she can prepare a thing. Prepare the food. So let\\'s say there is some guessing work that is going on. So this person has guessed and prepared the food. And obviously at the end of the day, the food was terrible because obviously the chef does not know how to prepare the food, but he or she has guessed and prepared most terrible food. And it tends to happen. So this scenario is basically called as hallucination. Hallucination because the LLM is not trained with some other data or something new data or recent data, it is trained with some other data itself. But if LLM does not know this, even though it will try to guess something. And then it will probably create some other kind of content. So that is nothing but hallucination. Scenario 2: Honest Admission 2. The second scenario will be that the LLM or chef can directly say that, hey, I don\\'t know. It might directly give you the answer that I don\\'t know. I don\\'t know about this dish. Let\\'s say you go and ask how to prepare this dish. It does not know. So it will say that. Hey, I don\\'t know. Scenario 3: Using External Resources (RAG Approach) 3. The third option is that let\\'s say the chef has some library of books. Some library of books. So what the chef will do, it will go ahead and check the recipe book. And then probably create something. By just following the step. Create something amazing. So this is one option. So similarly let\\'s say this books will be stored somewhere. Similarly in the case of LLM, you know when it comes in this particular scenario, what we can do is that we can bring some database like vector database and store all the text data. Let\\'s say image data. Different kind of data. Media files. Video files in the form of vectors inside this particular vector database. So here inside this vector database we use something called as text embeddings. By using text embeddings we basically convert all these things into vectors. And then the chef how it will be referring to the books. The LLM can refer this vector database. And whatever things are matching based on the search, it will be able to retrieve the result, it will be able to summarize it and it will be able to give the output. So this is more of a kind of example that I really wanted to show you, by just taking an example of chef itself. So here also as I said, three important steps are happening retrieval augmented and generation. Real World Scenario: Customer Support Now let\\'s take one real world scenario so that you get a clear understanding about one of the most problem, important problem statement where RAG is specifically used. One is customer support. So in customer support, let\\'s say I will give you both the scenario. Let\\'s say I have a customer support application. And this is without RAG. And let\\'s say this is the next one example that I want to show you is with RAG. Now I will give you one question. Let\\'s say the customer has asked this question. The customer over here have asked this question. The question is very simple. It says that \"What\\'s your return policy for items bought during the Black Friday sale?\" Without RAG (Generic LLM Response) So if I just have a LLM model, it will try to give up most generic response what is available in the site. Let say it will say: \"Generally most companies offer 30 days return, but policy may vary\" because obviously this LLM model will not have the data inside this particular company where this chatbot is being used. So usually what happens is that if I just take a normal LLM model, if I just take a normal LLM model and use it directly for my customer support, then this problem will happen. When I ask, hey, what\\'s your return policy for items bought during the Black Friday sale? Whatever generic internet information it may have, or whatever previous information it has been trained with, some other information that generic answer you\\'ll be able to get. Generally, most companies offer 30 days policy return, but policy may vary. So here you can see that there are three problems: 1. Generic 2. Unhelpful 3. Unhelpful response in short With RAG (Company-Specific Response) But once you start implementing RAG in your company, once you start implementing RAG, what kind of output you may get for this kind of customer support. Now let\\'s say that I have a AI assistant which uses an LLM plus company data, which is stored in vector database. Now the same information when it is asked. And let\\'s say that this is my real data that I will be having. This is how I will be able to get a response. Customer asks: \"What\\'s your return policy for item bought during the Black Friday sale?\" RAG will probably search from the company policy database and this company policy database is in vector database itself. And then it will pick the right kind of result, the most matching results. Obviously it is going to apply augmentation and enriching the content, and then the LLM will finally summarize it. And you know that every company is also probably change their policies. So based on the recent policy, it will say: \"According to our current policy document, version 3.2, uploaded on November 2024, Black Friday purchases have an extended 60 days return window until 31st.\" This is one example I\\'ve taken up right now. Tell me just by thinking whether this was good weather for a client? Whether this kind of response is good or this kind of response is good? Obviously, this kind of response we are able to get it because here we have integrated our LLM with our own company policy databases, and we are specifically using this. And this vector database will be belonging to one specific company. This is one amazing thing about RAG, you know, again, one really amazing thing that I really want to talk about is like how it is able to create such a very beautiful impact. And this impact, I\\'ll tell you that it has, you know, I did a lot of internet search. Like, who all are specifically now using RAGs and how they are able to do this. Cost Savings and Business Impact I also have created one amazing graph over here. So I\\'ll just talk about it, you know, with respect to the cost savings and all. So some of the cost saving anyhow, I will be giving you this. See, so this was the recent news that we were able to see. And I have actually searched it. If you go ahead and search it, you will be able to even get it in the internet also. 1. JP Morgan - They saved $150 million annually by implementing RAG for research analysts instead of fine tuning. Now, don\\'t worry about what is fine tuning and all. I will talk about it as we go ahead. What are the differences between fine tuning and all before, you know, JP Morgan used to do a lot of fine tuning. There was a lot of cost that is involved in fine tuning. Now. They have completely moved in RAG, you know, they are building the applications with respect to RAG and all. So that\\'s one very good thing that they have saved so much amount of money. 2. Microsoft reported 94% reduction in AI hallucination after implementing RAG in the copilot product. So if you know Microsoft also has a GitHub copilot and all. So initially they were also hallucinating. I told you, right. What is the reason for hallucinating? Hallucinating basically means if the LLM does not know something, it will still try to make up something and give it to you. Even though it does not have that particular knowledge, it is not trained with that kind of data. So that is the problem that usually happens. 3. Bloomberg updates the financial AI assistant hourly with new market data impossible with traditional LLMs. 4. Healthcare companies use RAG to ensure AI responses always cites approved medical resources. You know, so this is with respect to the compliances. And if I talk with respect to different job profiles, people are specifically using all this kind of RAG applications and they\\'re building it for the company itself. So I hope in this particular series of video we understood about RAG. Still, I will discuss about this entire architecture. This architecture is specifically for you. But right now still we have not discussed much. We just discussed about what exactly RAG means. You know, what is retrieval augmented generation. Along with this, we also understood about some of the use cases with respect to customer support. Without implementing RAG, what kind of response I get. And with implementing RAG, what kind of response I get. So I hope you like this particular video. This was it from my side. I\\'ll Take care.'),\n",
       " Document(metadata={'section_name': 'Introduction to RAG', 'section_number': '1', 'total_videos': '4', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and Langsmith', 'date_created': '2024', 'source_file': 'intro_to_rag.pdf', 'video_title': 'Business Impact Use Cases With RAG', 'video_order': '3/4', 'topics': 'Business impact, Cost savings, ROI, Industry adoption, Real case studies', 'difficulty': 'Intermediate', 'doc_type': 'video'}, page_content=\"So we are going to continue our discussion with respect to RAG. Already in our previous video, you know, I gave some of the points where I told that how JPMorgan were able to, you know, save $150 million annually using RAG and all. So what I thought is that why RAG actually matters, you know, and what kind of business impact it is probably going ahead and creating. And let me tell you guys, as I said, more than 80% of the business use cases in many, many companies are related to RAG. They are specifically building genetic AI applications. They are building genetic RAG systems. JPMorgan Case Study So here you can see that, before, you know, JP Morgan used to spend around $200 million per year on fine tuning. And then now, you know, recently, you know, they are just spending around $50 million per year on RAG. So here you can see that they are anyhow saving $150 million annually. For research analysts this is as said. Microsoft Case Study - Accuracy Improvement Then, with respect to the accuracy improvement, you can see Microsoft case studies. If you just go and search for this amazingly in the internet you'll be able to get it. So here 94% reduction in AI hallucination. So before RAG, you know, 34% hallucination was there right now after RAG 2% hallucination is left after implementing RAG in copilot. Bloomberg Case Study - Real-time Updates Here, you'll see that traditional LLM six months training cycle, RAG system, real time updates. It is 24 times faster. You know, this is from the Bloomberg case study. This information has basically come up with respect to compliances. Healthcare Compliance RAG has been heavily used in 100% source attribution, regulatory compliances, complete audit trail approved success. Only, you know, and this is how the RAG adoption trends takes place, you know, and probably till 2026, it is shown that, easily around 85% at least are there going to specifically use over here with respect to RAG adoptions and all. Why RAG Matters - Key Metrics Everything over here with respect to this is given. Why RAG matters: • The average ROI (return of investment) with respect to RAG application is somewhere around 312%. • Implementation time is very, very fast. Within 6 to 8 weeks, you'll be able to implement it. When I talk about implementation, I'm talking about, you know, taking it to the production. Top Industries Adopting RAG Top industries adopting RAG are: 1. Finance 2. E-commerce 3. Healthcare 4. Manufacturing 5. Legal 6. Education So these all are there. And that is the reason why I say that RAG is really, really important. I've trained more than 20,000 plus people. You know, everybody nowadays are just coming up with questions related to RAG. You know how to probably go ahead and implement it. So definitely RAG is a thing that everybody needs to know as we go ahead.\"),\n",
       " Document(metadata={'section_name': 'Introduction to RAG', 'section_number': '1', 'total_videos': '4', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and Langsmith', 'date_created': '2024', 'source_file': 'intro_to_rag.pdf', 'video_title': 'Prompt Engineering Vs Fine Tuning Vs RAG', 'video_order': '4/4', 'topics': 'Comparison of techniques, Pros and cons, Use case selection, When to use each', 'difficulty': 'Intermediate', 'doc_type': 'video'}, page_content='So we are going to continue the discussion with respect to retrieval augmented generation. In this specific video we are going to discuss about this very important topic. Like what are the differences between prompt engineering versus fine tuning versus RAG? Let me tell you guys, because here there are a lot of confusion from many, many people. You know, if there is prompt engineering, why should we go ahead with fine tuning or why should we go ahead? If there is fine tuning, why should we go ahead with RAG? Each and everything we will be specifically talking about. We\\'ll also talk about how prompt engineering works. Fine tuning works, RAG works, and we\\'ll also talk about the pros and cons as we go ahead. Prompt Engineering So first of all let\\'s understand about prompt engineering. You know, so as you all know prompt engineering is more about giving a prompt to a base LLM. When I say base LLM, let\\'s consider that this is one kind of LLM. It can be a OpenAI GPT four or LLM, or it can be llama three. It can be Google Gemini LLM. Let\\'s say if I just go ahead and give a kind of prompt. Let\\'s say the prompt is like \"act as a chef and provide me a recipe\". So if I give this information, so this information is just like a prompt where I\\'m giving a kind of instruction to the LLM to behave in that way. So based on this particular prompt, now whatever input I specifically give or whatever question I actually ask, I say that, hey, \"please tell me how to prepare pizza\". So that is the kind of input. So based on like how a chef will give me an answer, it will go ahead and give you a customized output. How Prompt Engineering Works So with respect to prompt engineering it is very simple. We provide a prompt to our base LLM. And our LLM basically works based on that specific prompt. So here, the first thing is that we provide specific instruction in the prompt. Specific instruction in the prompt. Second, your prompt that you\\'re giving should be very, very structured prompt. It should be a structured prompt with clear context. This is a really important statement. If the context is not clear it will not be able to, the base will not be able to give you a customized output. Then the third important main property is that the model will remain unchanged. The model is same. The same base model. It is not going to change its parameters or anything as such. Pros of Using Prompt Engineering Now let\\'s go ahead and talk about the pros of using this. Pros of using the prompt engineering. So the prompt engineering part. So the first pro over here is that the first major advantage is that no technical expertise is needed. So here just a normal English language. And you will be able to perform prompt engineering. So no expertise technical expertise is needed over here. Second you will be able to get quickly and instant results. Quickly and instant results. Third, here to write a prompt or you know, when compared to fine tuning and RAG, there is no training cost involved. So no training is required. No additional training is required. It is completely for free. You can just go ahead and change the prompt itself. And the LLM should be able to give you a customized output. Fourth important main property is that it works with any LLM. So you can go ahead and give your own prompt based on any kind of LLM, then it should be able to work. Cons of Prompt Engineering Then what are the cons? Because we need to also understand if we know all the advantages. What are the disadvantages. Now with respect to disadvantages here most of the base LLM models you know it will be limited. It will have a limited base knowledge, you know. So in this scenario, since we have a base LLM, it is always limited by models base knowledge. So here we don\\'t have any external information. So whatever model is basically trained with the previous data, only that information I have right now. When we keep on changing prompt, sometime it is there are scenarios that we will get inconsistent results. So let\\'s say if I go ahead and put a prompt on one LLM model, I may get a different result. I may just tweak that particular prompt. Then again I will be getting a different result. So inconsistent result. This is the most important thing. Inconsistent result. Coming to the third important con here. Always remember every model has a token limit restriction. So with respect to that particular point I will go ahead and write it down over here. Token limit restrict complexity. So this is one specific point that I really want to mention. And the fourth important thing is that it can\\'t add new knowledge. That\\'s it because we cannot add it. Here we are specifically using the base LLM. We are modifying the prompt and we are getting the output. When Should We Use Prompt Engineering? Now when should we specifically use prompt engineering? You know, when should we focus on that? See, let\\'s say that you want to quickly go ahead and check, so I\\'ll just go ahead and give a statement what it is best for. So the first point here, I will say that if you are trying to do a small scale application quickly, do it. Try to do it with the help of prompt, because most of the use cases will be able to solve it for them. And definitely this is not suitable for, see, whenever we use some base model, we are always going to get generic answers. So not suitable for any company specific work. Let\\'s say I want to create an AI assistant only for companies. At that point of time. I will definitely not use prompt engineering for that. I will either go ahead with fine tuning or RAG. So here it is. This is also used for generic public task or sorry generic. I\\'m saying public but it is just used for generic purpose task. Generic purpose. All my answers will be mostly generic itself. Small scale application. Third is that I will also be using this for quickly doing my quick prototyping. So this can also be very, very helpful. So these are some of the points that we have specified here in the prompt engineering. We just give the prompt. And that prompt is combined with our base LLM. And whenever we get a given input we usually get a customized output. Fine Tuning Now the second thing is with respect to fine tuning. Now fine tuning why it is specifically necessary you know. So let\\'s say that I have a company where I am planning to create an AI assistant, which should completely behave like how I want, you know, each and every instruction that I am actually giving to the AI assistant. It should be very specific to my company. Let\\'s say if there is a chatbot that I have actually created, an AI assistant that I have created. If I say \"hi\". It should probably reply you back that \"Hi. Welcome to XYZ company. How may I help you? Can I provide you the services of this specific company?\" So here specifically we are focusing on a company domain use case. It is completely like how my company actually wants that AI assistant. Similarly I will be able to create it. How Fine Tuning Works So here how we actually do the fine tuning. Now with respect to fine tuning it is nothing. But here we basically say that we are going to teach the base LLM through training. So let\\'s say that there is a base LLM model. What we do is that we get a new training data. This new training data will be the company training data. And then we try to train the base LLM with this specific training data. You know once we train with this training data, obviously the LLMs usually they use different different architecture. And there there will be weights that weights gets modified. It gets modified. And once we specifically take that modified weights and apply it into our LLM model. It is nothing, but it is called as fine tuned LLM model. So this is nothing. But this is a specialized fine tuned LLM model which will be specifically used by the company for their problem statement. So this is how entirely and again this is a very big process. Fine tuning techniques are there like LoRA LoRA methods and all. But our course syllabus is mainly focused towards RAG you know. So here you\\'ll be able to see that with respect to any companies who are specifically focusing in creating their own chatbot AI assistant, which will behave like what the company specifically want that time we\\'ll be using fine tuning itself. Key Points of How Fine Tuning Works Now first of all, we will try to understand how it works. So for this, the first important thing is that we have to prepare. What are the requirements. So we have to prepare a domain specific training data. Domain specific training data. When I say domain specific training data, that basically means it is nothing but the company data for which we are basically creating the chatbot. Second is that we train model on the data, on company\\'s data. So this is the second important point. Third is that model weights that are changing. They are permanent. They are permanently changed. You know it will again I\\'ll not go back to my base model. So once the training is done based on this new data, the weights will be completely modified. So it basically creates a specialized version. I can say that. Specialized version. So these are some of the important points. Pros of Fine Tuning Now let\\'s talk about the pros and cons. So that you will be able to get a very clear understanding what are the pros and cons with respect to this? So the first pro that I would like to talk about is that whenever you have specific specialized domain knowledge problem statement, at that point of time for a specific company, this will be very, very helpful. So deeply specialized, deeply specialized knowledge. You\\'ll be able to create this kind of chatbot. Second, your chatbot will also have a very consistent behavior since we have trained this with our own data. Third, no prompt engineering needed. Here we specifically don\\'t require any kind of prompt engineering because already my model has been trained with fine tuning data. It can learn new writing styles, you know, new behaviors, and it is always better for a specific domain. Let\\'s say I want to create a chatbot assistant, which is a legal expert. So what I will do, I will specifically train that model with only legal data. So that are some of the important pros that we can probably bring up while we are specifically creating AI assistant or LLMs by fine tuning. Cons of Fine Tuning Now obviously there are pros. They need to be some kind of cons. So what are some of the cons. So let\\'s talk about this. What are some of the cons. So when with respect to the cons the first important thing that I will be talking about it is expensive. So the entire fine tuning process is expensive. Why? Because obviously we will be requiring GPUs in order to train all this specific models. Not suitable for a startup, you know, who are having less funding. And all. Obviously they don\\'t want to spend a lot of monies with respect to fine tuning in GPUs, renting out GPUs itself. Second important thing is that over here, when you are trying to create a chatbot with the help of fine tuning, you require good expertise. So require ML expert or AI expertise. You definitely require this because the task and tree training model is very, very huge. Then regularly again once you start retraining your model. Regularly retraining is required. Definitely. Why? For updates. Once you train your model, retraining is definitely required for updating your model again and again. So these are some of the cons. And that is the reason only companies whenever it is only required they will go ahead with fine tuning the entire models. But they are also companies who will be specifically doing all this stuff. When to Use Fine Tuning Now finally what it is best for? We need to understand that. Let\\'s say I want to go ahead and provide some specific style, specific style in how my chatbot is behaving. I will definitely go in, go ahead with fine tuning my entire AI assistant. Whenever I have huge data, high volume data, I should definitely go ahead with this. Because they are consistent task and all. And third, if I\\'m looking for, you know, where my accuracy should be very, very high. So definitely I will go ahead with fine tuning. So here when accuracy matters. If accuracy does not matter much, if accuracy is critical, then definitely I will go ahead with fine tuning. So these were the differences between prompt engineering and fine tuning. RAG (Retrieval Augmented Generation) Now comes RAG. So so many things we have discussed. RAG simple is like that. You\\'re teaching through retrieval. So here, let me just talk about this RAG. And from this particular diagram, you should be able to clearly understand in a RAG you definitely have some knowledge. External source. It can be a database. It can be APIs. It can be anything as such. Then based on the user query we use the base LLM that is there. Based on this particular query, we retrieve the documents from this particular knowledge base. We retrieve the documents. Once we retrieve the documents, then we summarize that document. We augment that document. And finally we generate that specific response. So here you\\'ll be able to see that there will be something like an external knowledge base where the data is actually put up. It can be APIs, it can be vector databases, it can be anything as such, it can be in-memory databases, it can be documents, it can be anything as such. So from there we retrieve the documents and then based on the user queries. Now this user query, we try to match it up on various different metrics. Usually when I talk about this knowledge database. It is some kind of databases itself, specifically vector database. I\\'m not talking about third party API. Also, I can go ahead and interact with other vector databases which are in the cloud by using APIs and all. But at the end of the day here, what you are trying to do is that you are able to retrieve some documents, and then you are able to enrich that documents, and finally you are able to get the output. So here that is the most important property about RAG. How RAG Works Now let\\'s talk about some of the important scenarios. So first important thing is that how it works. You know first of all all the documents are stored in vector database. So we store documents in vector DB. Because at the end of the day retrieval is actually created from this. Second important point is that we retrieve we retrieve relevant documents relevant docs from DB. From. By using queries or like for each query, you can say for each query. But at the end of the day, similarity search is basically happening from that particular vector db. Retrieve relevant docs for each query. Now, the third important point over here is that it is very, very simple here. You\\'ll be able to see that LLM generates answer from context. Answer from context. So whatever context you specifically get, you are able to generate the answers. Pros of RAG Now let\\'s talk about the pros. So here I have my pros. So the first pros is that see why do we use RAG. Let\\'s say if there if company is having some kind of information that is getting regularly updated, then we cannot just go ahead and do the fine tuning. There are a lot of costs that will be specifically involved. So for this particular scenario, you know, whenever there is an up to date information, always go ahead with RAG. So here I will say that it will always have up to date information info. Second important thing is that no training required. This is the major cost. No training required. We don\\'t have to train the base LLM because there will be huge number of parameters. You know, when we talk about cost effective, it is also very, very cost effective. And based on the techniques of RAG, the accuracy is also very, very high. Accuracy is high but not that high as fine tuning. But I think a good, like right now, the kind of RAGs that we are specifically able to see is quite amazing. So the accuracy is increasing again and again. The fifth important point is that it can also handle private. Private or proprietary data. Proprietary data because everything will be stored in some kind of databases. Cons of RAG Now let\\'s talk about cons. Cons is one thing. Obviously the first con is that this requires an infrastructure setup. Infrastructure setup basically means obviously you need to have a vector database and a and all this specific stuff. The second thing is that sometimes the retrieval the retrieval quality affects results. Like how we are able to retrieve from the vector database that will affect the results. Third there is always a context window limitation. See every LLM models will be having some kind of context window limitations. So based on that only they\\'ll be able to get the context from the retriever itself. There will be a limitation based on different different models. Then other than this, you can also think for what all things it is best for. I will keep this as an assignment, but maximum number of use cases like customer support and all can be usually done. Real time data also can be updated in the vector database and based on that you will be able to generate. Final Summary So that was more about talking about prompt engineering. So guys I have completely explained the differences between prompt engineering, fine tuning and RAG. What I\\'ve done is that one image. Also I\\'ve created it over here. Which method you should use. What are the pros and cons. Each and every thing has been mentioned over here. You can just go ahead and see to this, you know um very easily if you\\'re not able to understand my handwriting, the same thing. I\\'ve written it over here step by step, which much more proper explanation. So I hope you like this particular video. I will Take care.'),\n",
       " Document(metadata={'section_name': 'Vector Embeddings And Vector Databases', 'section_number': '4', 'total_videos': '5', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Vector Embeddings And Vector Databases.pdf', 'video_title': 'Introduction To Embeddings And Vector Databases', 'video_order': '1/5', 'topics': 'Embedding fundamentals, Vector databases vs traditional databases, Similarity', 'difficulty': 'Beginner → Intermediate', 'doc_type': 'video'}, page_content=\"search, Cosine similarity, Feature representations, Model choices (OpenAI & Hugging Face) — Overview — So we are going to continue the discussion with respect to Rag. — Recap: Ingestion & Splitting — In our previous video we have seen various data ingestion and parsing techniques. Uh, and the last one was like if our data is available in a specific databases for those kind of data, which is in the form of structured format, how do we read it? How do we convert that into a document we have already seen? — Where We Are in the RAG Architecture — Now if I go back again to the architecture, right rag architecture, we have completed all these things. We have seen how to do the document splitter and how with the help of text splitter, we are actually converting a document into chunks right now. The next step is that how do we go ahead and use some kind of embedding models and convert that into some embeddings. Right. So this is what we are going to now focus on. Our main aim is that once we get the data in the form of chunks, which all different types of embedding models, we can go ahead and apply. And the idea behind embedding model is very simple whatever documents or data is available, how do we go ahead and convert that into vectors? — What & Why: Embeddings at a Glance — Now in this video I'm just going to go ahead and give you a brief introduction about embeddings and why embeddings can be really, really important and useful and how it is different when we compare it to the traditional databases. So all those things we are going to go ahead and discuss. Okay. — Traditional Databases (Exact Match Retrieval) — Now let's say, uh, if I just take an example of a traditional database. So let's say if I have a traditional database. So this is my traditional database. Now for this traditional database let's consider this. Let's say if this is my traditional database. And since we are going to go ahead and discuss about embeddings here, uh, now what we are specifically going to use is vector databases. And I'll talk about it. Why vector databases will also be important vector database. Now in a traditional database. Let's say that my search term, let's say I want to search something related to Cat. Or let's consider that in this particular database I have some of the items. The items can be let's say I have cat, I have dog, I have kitten, right. And let's say I have puppy. Now these are the information that is available in this specific database. Now whenever I go ahead and search anything related to Cat, then what this will do based on a specific query, we will go ahead and pick up the exact match that is available inside this database. So my output result. When I go ahead and search for Cat, I am going to go ahead and get Cat right. It is very simple because the matching term is nothing but cat. So this is what usually happens in a traditional database. So if you probably go ahead and consider a SQL database, right. MySQL or SQL server. Let's say I go ahead and write a query, select star from employee table where the name is equal to crush. So I'm searching for a record wherein I'm putting a condition saying that the name should be equal to crush, then that specific record will be displayed, right? Similarly, if you have multiple tables in a traditional database, you may go ahead and write a complex query, and based on that particular query, you will be getting a exact match. So this is really important. The purpose of a traditional database like SQL, MySQL, NoSQL. Here you are going to get only exact matches exact matches. — Vector Databases (Semantic Similarity Retrieval) — Right now. What is the importance of a vector databases? We'll try to understand it. And for a vector databases, since we are specifically going to use embeddings. Now what is embeddings? Embeddings is nothing. But here embeddings is a technique wherein if I have any kind of text data, if I apply embeddings on top of it, I will be able to get some vector format or vector representation for this particular data, which is in the form of text. Let's say for this particular text, let's say the text is like Apple okay. So if the text is Apple, and if I apply some embedding techniques in this, I will be getting a vector representation which looks like this. Let's say I'm just considering this. It looks something like this okay. Now this vector representation again for this particular course you should have a basic understanding what is embeddings. Because these all are the part of NLP techniques. But here I will just give you a brief idea about embeddings also. Now whenever I have this kind of representation. So these are nothing but numerical representation. I cannot just go ahead and directly use a word and give it to my model. My model will not be able to understand this. Instead, it will be able to understand some kind of numerical representation. And this numerical representation is nothing but vector representation. Now for storing this vector representation we can use something called as a vector database. So let's say if Apple is something like this, we can store this information over here in the form of a record or a text ID or document ID or something. Right. So we can store this entire vector representation over here. — Similar Meaning vs Exact Match (Cat → Cat & Kitten) — Now let's consider that I have the same thing. So let's say if I have the same data over here. So I'm I'm just going to go ahead and consider, let's say this is my same data that I have. Okay. Let's say if this is oops this is the same data that I have. Okay I'm going to paste it over here. Now what is the main purpose of the vector database is that if I go ahead and search for Cat again Okay, now here it is not going to do the exact match, but instead it will go ahead and find the similar meanings word similar meanings word right. Or it will go ahead and find with respect to cat which are the similar words over here. So over here, if you go ahead and see the result right here with respect to the result, two things you will be able to get. See, cat is obviously cat only kitten. Also we can basically talk about cats right. So here what we are going to get. The result you are going to get is cat and kitten. So a vector represent now why you are getting this two words over here as in the form of results. Because if you consider at the end of the day in a vector databases, we will be storing the information of all the text which is in the form of vector representation. So this cat and kitten will have a similar kind of vector representation because these words are almost similar. So all the similar words. So usually what happens is that in deep learning in NLP we have good embedding models, word embedding models, right. Which are trained with huge amount of data. These are trained with huge amount of data, huge amount of data. And this embedding models main aim is whatever word it has, whatever word it takes, it will be able to provide a vector representation for that word. Vector representation for that word vector representation will be in such a way that similar kind of words, let's say if I take an example of cat and kitten, since both the words are same, the vector representation may look like this .2.3.4, let's say, and kitten may look something like this. .2.4.3. — Cosine Similarity (How We Compare Vectors) — So here you can see these are my vectors right. These are the vector representation for this word cat. And this is for kitten right. You can see that both the vector representation are very much near right. So what we do is that in order to compare whether these two are same or not, we apply something called as cosine similarity. This is just one of the metric. And based on this cosine similarity it actually says that how similar this both words are. I will show you this all in the terms of coding right. But I hope you got a idea about what is vector representation. And with respect to any kind of vector databases where you are specifically applying embedding techniques and storing those vector representation inside the vector database, you will be getting a result based on finding similar meanings or similar match. That is how it is completely different from a traditional database. — Embedding Models (OpenAI & Hugging Face) — Now this vector database is or vector embeddings, right? Vector database is basically means we will be using this databases to store this embeddings. Right. This vector representation. And to get this vector representation we use embedding models. Now there are different different embedding models. We have models from OpenAI. We have open source models like hugging face right hugging face and all. — Intuition via Feature Representation (Movie Analogy) — But now the question arises, Krish, why you are representing in this format, let's say, why you have written point two, .3.4, what this entire words also represent that also will go ahead and discuss. So let's, let's take one, uh, you know, example. I hope everybody has seen the movie of marvels. Right. And many people marvel, right. And, you know, Marvel is having the biggest franchisee in the world, right? So let's consider a movie from Marvel, let's say Marvel. There is a movie of Iron Man, right? Iron man. Now, let's say this Iron Man is there. Now, I may go ahead and create an embedding model, or I may use some kind of embedding model. And this embedding model, whenever it takes this Iron man as a text, it may convert this into a vector. The vector may look something like this. Right. And for this I will just assume some of the features. So these are nothing. But this vector representation is also called as feature representation. Why we say feature representation I'll just talk about it. Feature representation. Let's say my first feature is nothing but action okay. The second feature is nothing but comedy. The third feature is nothing but suspense. Okay, let's consider this three feature. Now you know that Iron Man, whenever we talk about it, is a hardcore action movie, right? So let's say I go ahead and write this vector as 0.95. .95. Okay, so this iron man with respect to action is represented by a vector .95. Iron man in comedy. So there is not much comedy in this specific movie. So I'm just going to give one vector representation as point two. See, this is not how a model is trained, but I'm just trying to give you an idea. What does this feature representation of vector representation basically mean? Then Iron man two suspense, right? Suspense. There are also some kind of suspense, so I'll keep it as point six. Now similarly, let's say there is another movie like Hulk and you know that is also a marvel movie, right? So Marvel, whenever we talk about Hulk in action, it is .96 comedy. Yes, it will be having point four and suspense. Yes, there is a kind of suspense point seven. But let's say that if I take some another movie, another movie basically means Sherlock Holmes. Sherlock Holmes, Sherlock Holmes. Action. There is not much kind of actions. So let's say, yeah, there they may be action, but let's say that I'm giving it as 0.6, right? Comedy. Sherlock Holmes has some kind of comedy suspense. There is a lot of suspense in this, so I'll put it as point nine right now here you can see all this numbers that you are able to see. So the Iron Man is basically represented by this vector Hulk is represented by this vector. And Sherlock Holmes is represented by this. And each of this vector represents this features representation. Right. Like it is a correlation of Iron Man to action movie, iron Man to comedy, Iron Man to suspense. — 2-D Plot Thought Experiment — Right now, let's say if I am seeing a Netflix movie and in the Netflix movie, I go and see an Iron Man movie. Now, after completing Iron Man, which movie do you think Netflix will recommend me? This is just a simple example for my side, so obviously you will be able to see from this vector representation and this vector representation if we go ahead and find the cosine similarity, right. The cosine similarity formula is very simple. It is nothing but Iron man. So let's consider that this is vector A. This is vector B, this is vector C. It is nothing but a dot product of B multiplied by normalization of A. That basically means magnitude of a and magnitude of b. So if we go ahead and apply this specific formula just by seeing this vector representation, don't you think that Iron Man and Hulk are almost similar? So what will Netflix recommend? Netflix will definitely go ahead and recommend Hulk, because if you go ahead and calculate the cosine similarity and if you just go ahead and plot it, plot it with respect to similarity. So let's say I'm plotting it in this two dimension. Okay, I'm obviously there is three dimension, but I'll just try to plot it at two dimension by taking action and comedy. So let's say if this is action, this is comedy, right? So the first plot, you will be able to see Iron Man 0.9 5.2. Right. So 0.950.2. Let's say this is one, two, something like this, right. Or this is this is 101. And this is zero one. This is 0.5. This is 0.5. Right. So 0.92 is nothing but somewhere over here right. So this is Iron Man. Hulk will be able to see 0.96.4. So it will be somewhere over here. So this is Hulk. This is Iron Man. So this is Hulk. This is Iron Man. So just see this how near this is, right? So that basically means this two movies are almost similar. If I consider Sherlock Holmes, it is 0.6, 0.7, 5.6 basically means over here, 0.75 basically means over here. So this is where my Sherlock Holmes comes, Sherlock Holmes come. So the distance is very far When you consider this to distance, it is very near. So if Netflix is showing you Iron Man, then the next movie that is recommended will be Hulk because this is very near to them, right. — What We’ll Do Next — So this is the purpose of using embeddings, right. And that is how when we do a similar meaning search. Right. We get similar things over here based on this vector representation which is near to each other. Right. And this is the entire idea behind embeddings okay. Now in our next video we will try to see again, I don't want to go much into theoretical things because these are some prerequisites that you should already know in NLP. Now in the next video, what I am actually going to do is that we are going to learn about how we can perform different, how we can use different embedding models and convert some data into vectors. Right. And later on we'll also talk about different kind of vector databases we can basically use. Okay. So we will focus on two. type one is open. I will also go ahead and focus on hugging face. Right. So both of them, because you should also know how to use embedding model models which are open source and how to also use paid. Right. So open I will have paid hugging face will be having some of the open source models, but the main idea is that we will take our text data or document and then we will go ahead and use some embedding models. So here I will be having my embedding models. And with the help of this embedding models we will go ahead and convert that into vectors. We will go ahead and convert that into vectors. That is what we are basically going to do. And this I will show you with the help of practical examples as we go ahead. Right. — Wrap-up — But I hope you got a idea about embeddings. What is the difference between traditional databases and vector databases? Uh, yeah, this was it for my side. I will see you all in the next video. Take care.\"),\n",
       " Document(metadata={'section_name': 'Vector Embeddings And Vector Databases', 'section_number': '4', 'total_videos': '5', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Vector Embeddings And Vector Databases.pdf', 'video_title': 'Visualization Of Embedding And Cosine Similarity', 'video_order': '2/5', 'topics': 'Visualizing embeddings, 2D plots, Cosine similarity, Interpreting similarity scores,', 'difficulty': 'Beginner → Intermediate', 'doc_type': 'video'}, page_content=\"High-dimensional embeddings — Overview — So we are going to continue the discussion with respect to embeddings. Now, uh, already we have understood some of the theoretical explanation about what exactly embeddings is, what exactly embedding models is. And, uh, you know, we also got a brief idea about vector databases, right. — Project Setup — So now, uh, what we are going to do is that I'll go ahead and create my second folder. So let's say that I name this folder as vector embedding vector embeddings and databases okay. So we are going to specifically work on two different things now vector embeddings and databases. Um that includes how we go ahead and like like what? Let's say that once we go ahead and read all the documents, you know, then we go ahead and apply some document splitter, we convert that into chunks. And then how with the help of embedding models, we go ahead and convert that into vectors. So over here I will quickly go ahead and create my files. Embedding dot Ipynb. Okay. I will go ahead and select a kernel over here. And then we can go ahead and start. — Quick Definition — So first of all I would like to provide a brief definition about what exactly are embeddings. Okay. So think of embeddings. Um it is a way of translating word into a language that computer understand that is nothing but numbers. So these all numbers are nothing. But we basically say it as vectors okay. And this is how we specifically go ahead and use it. — Plan for This Video — Now over here in our previous example, I had drawn some kind of visualization. Right. Like this. You could see that I have written Iron Man Hulk over here, Sherlock over here. So first of all, what we'll do is that in this specific video, we will go ahead and see that, let's say if I have some of the words right. And for all these specific words, how do we go ahead and just plot it and see that whether it is very near to each other or not. — Imports & Installation — Okay, so first of all, what I'm actually going to do quickly, I will go ahead and import numpy as NP. I'm going to use numpy, and I'm going to also use matplotlib dot pyplot dot pyplot as plt. Okay. And then I'm going to specifically use this two libraries. Let's say if this library is not there you can see matplot is not there. So what I will do I will quickly go ahead and update my requirement dot txt file. So let's say over here I will go ahead and write matplotlib okay. And why I am doing this because I want to visualize the embeddings right. Like how we had seen in this particular diagram. Can we go ahead and visualize? I'll take some 3 to 4 different kind of vector representation of a words. And then we will try to just plot it in this way so that you'll also get an idea about embeddings. You know how exactly it looks in terms of visualization. Now I will go to my command prompt and it is already there. My my environment is already activated. So you also need to make sure that. And I'll write you've add requirement dot txt. So here you can see that already resolved packages. And here matplotlib has got installed right. So matplotlib by parsing all these things have got installed. Now I'm going to go back again to my embeddings and then execute this. Now it should work absolutely fine. Okay. — 2D Example Setup — Now what I'm actually going to do is that I will just take a simplified 2D examples. One very important point is that guys over here for this vector representation I use three feature representation. Right. So if you go ahead and see different different embedding models, they will be having different dimensions with respect to this feature representation. Let's say if I go ahead and use OpenAI some of the embedding model right. This may have 1500 dimensions okay. Hugging face it may have 384 dimensions. So different, different models have different capabilities. And based on that, whenever they take a word, it can convert that word into different dimensions of feature representation. Right. And that is how it is specifically trained. Every model has different capability. Right. It may not be two features or three features. Right. They may have different different features. Right. So uh, in this particular example, I am considering some of the words where I've just given two features. Let's say .8.6. I'm not naming the features itself because we also don't know, like how open I may have trained the embedding models or how hugging face may have trained the embedding models. Right. So I'm considering some of the very common words like cat, kitten. So here you can see .8.6 is the kitten is .75.65. So it is very near to each other. Dog is .7.3, puppy is .65.35 car and truck. It is no way related to this. So what is my plan is that I will try to plot all this words, and it will then give you an idea about like how this word looks like, right? — Plotting the Embeddings (2D) — So for this again it is basic Python. We're going to use plt dot subplots figure size of eight comma six I am reading every items inside this word embedding. So for word comma coordinates right in embedding dot items. And I am taking the coordinates of zero in my x axis. And this one on my y axis with a solid color of 100. Okay. Then uh I'm annotating with this specific information. So this is basic matplotlib visualization. So now I will just go ahead and show you. So here you can see this is my how my simplified word embedding in 2D space looks like here kitten and cat are is almost similar. Puppy and dog is almost similar. Car and truck is almost similar. Okay, so this is an example of vector representation. But in this particular example I've just taken two vectors itself. Right. But as I said with respect to different Embedding models. We definitely get different kind of, um, you know, we basically get different dimensions with respect to the vector representation. — Measuring Similarity (Cosine Similarity) — Now the next step is that, uh, if you go ahead and see this right, I told you, right. Netflix. If iron movie is seen, how does it go ahead and probably suggest like which is should be the next movie, right. And for this we apply cosine similarity. And the basic cosine similarity formula is something like this. It is nothing but vector a dot product of vector B divided by magnitude of A and magnitude of B, right? So we will also go ahead and apply cosine similarity between two vectors. So let's do that. And uh here I'll just go ahead and write something like measuring similarity. Okay. Measuring similarity. And for this we will try to use cosine similarity. If I want. Okay. — Implementing Cosine Similarity — So now quickly I will go ahead and define cosine similarity. And remember one thing about cosine similarity is that if we are using cosine similarity. These are how the outcome looks like. If the result is close to zero or close to one, it is very similar. If the result is close to zero, not related. If the result is close to minus one, it is opposite. Okay, so that is what the cosine similarity gives you an output. As I said, the formula of cosine similarity is a dot b that is nothing but a dot product divided by magnitude of A and magnitude of B, is there? Right? So first of all, what we need to do we will consider two vectors right. So vector one and vector two. And first of all what we are basically going to do. We are going to do a dot product of this. So here I will just go ahead and write. This is my dot product in order to calculate it. And here I will go ahead and write NP dot dot. And let's go ahead and write vector one comma vector two. So this is what we basically do with the dot product. Right. And for doing the dot product we use numpy NP dot dot okay. Then what we do in order to calculate our magnitude of a I will just go ahead and write norm underscore a is equal to in order to calculate it. I also have an inbuilt function in numpy, which is nothing but a dot norm. So if you go ahead and use this particular function, you'll you should be able to get the magnitude of vector A okay. So here we are basically going to get the vector a. Similarly the same formula will go ahead and apply for the vector two okay. So this will basically be my b. So now here I've got my magnitude of a magnitude of B. And finally I need to just go ahead and return dot product dot product divided by divided by norm. Underscore a. With a multiplication of norm underscore B. Remember this dot product is something different with respect to multiplication okay. So in a dot product vector or vector multiplication will happen. Right? For every vector representation or every dimension of the vector, they'll be getting multiplied with this other vector itself, right? So this is what is my entire function of cosine similarity. — Examples (Cat–Kitten vs Cat–Car) — Now we can go ahead and apply this. So let's say that I have some of the example of the vectors over here. The cat vector looks like this. The kitten vector looks like this. The vector looks like this okay. Now if I just go ahead and apply the similarity for cosine similarity, let's say for cosine similarity I go ahead and apply for cat. Let's say cat and kitten kitten kitten similarity I'm going to go ahead and apply okay. So for this I'm going to go ahead and apply my cosine similarity. And I'll give my two vectors okay. So here I'm going to go ahead and give my two vectors. One is cat underscore vector comma. And the next one is nothing but kitten underscore vector okay. Now if I just go ahead and print the cat kitten similarity you should be able to get the output okay. So let's execute this. So here you can see that my output is nothing but 0.996. So already I've told you that if it is close to one, it is very similar. That basically means it's cat and kitten vector are almost similar. Similarly, I can go ahead and apply the different similarity for this. So here I'll go ahead and apply for cosine similarity. And here let's say if I go ahead and take car cat and car vector, then here you'll be able to see that I'm getting point -0.4367. That basically means it is completely like opposite. It has opposite meaning. So obviously car and cat are nowhere related at all. But here we have given some kind of vector representation that matches with this, right? So that is the reason we are able to get this. — Notes on Vector DBs & High Dimensions — So this is how uh you go ahead and apply a cosine similarity. Uh, or you just can define a function with respect to cosine similarity and do this, um, similar like internally in the vector databases. Also this all functionalities will be used that we will go ahead and see it as we go ahead. But yes, this was just an idea about visualization of embeddings. And then we have also spoken about how do you find the similarity between two different vectors. When we talk about vector databases, we will still talk about different, um, you know, different, uh, similarity algorithms that we are going to implement. Uh, there is also something called as similarity search internally. You know, this cosine similarity may get applied. But yes, I will just go ahead and talk about it as we go ahead. But this is just an example of how you can go ahead and visualize some vectors. Obviously it is not. It is difficult to visualize vectors that are greater than three. Right? I cannot like in some of the algorithms, it will give you 1500 dimensions of vector representation that we cannot go ahead and plot it. But if you just apply some principal component analysis where you do dimensionality reduction, convert it into two vectors, then all the words will look something like this. — Wrap-up — So two different thing. Amazing things we learned in this particular video. One is how do you visualize it and how do you go ahead and measure the similarity. So yes, this was it from my side. I'll\"),\n",
       " Document(metadata={'section_name': 'Vector Embeddings And Vector Databases', 'section_number': '4', 'total_videos': '5', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Vector Embeddings And Vector Databases.pdf', 'video_title': 'Creating Your First Embeddings With HuggingFace Embedding Models', 'video_order': '3/5', 'topics': 'Hugging Face overview, Sentence Transformers, LangChain integrations, Embed', 'difficulty': 'Beginner → Intermediate', 'doc_type': 'video'}, page_content=\"query vs. embed documents, Model choices & dimensions — Overview — So we are going to continue the discussion with the topic that is embeddings. And in this specific video we are going to create our first embeddings. Now I hope everybody has got an idea what exactly embeddings is. You know, at the end of the day we will be taking some text data and we will be converting that into some kind of vector representation. And for this we will be using various embedding models. I will be showcasing you two different types of embedding models that is OpenAI and Hugging Face. Hugging face has some cool open source models which you can directly use it. And for OpenAI you need to have OpenAI API key. So to use the embedding models. — Exploring Hugging Face — So let's go ahead and before I go ahead, first of all, let me just go ahead and show you about hugging face. So here what I will do I will go ahead and search for hugging face. So once you go in and right hugging face.co right. And let's say that I will just go ahead and log in over here. So once I login after login here, you can actually see that this is how a hugging face looks like, right? Hugging face website. Now, if you don't know about hugging face, what exactly it has, it has huge different kind of models that is available for different different tasks, right? So let's say that if you want to perform text generation, you want to generate image text to text. So for all this kind of tasks, you know you have different different models over here, right. Not only that, let's say I will just reset all this task. Let's say you want to go ahead and use different libraries, different languages. You know, let's say I want to go ahead and use transformers. I want to use TensorFlow. Along with that, if you want to perform any kind of task, let's say the task is like audio, text to text, computer vision, you know, natural language processing like text classification and all. So this hugging face has huge number of open source libraries which are hosted in that specific platform, right? So what you can actually do is that all those libraries also you can directly use it right now in our case we are going to focus on embedding models. — Finding Embedding Models — Now with respect to embedding models, if you probably go ahead and see over here right. So here you can see that with respect to natural language processing, there are so many different categories of tasks that you can actually do. You have feature extraction. You have text generation, you have text classification, token classification. And all these are the models which you can actually use it. So what I am actually going to do, I'll go ahead and search for some of the embedding models. Right. If you want to go ahead and search there's something called as sentence transformers okay. So if I go ahead and search for sentence transformer here you can see different kind of models are over here. Like all mini lm, l6, v2. So this is the kind of embedding models. Then here you have paraphrase multilingual model. Then you have labs. So out of this you know you can use any of those embedding models. So if you also go ahead and search for embedding models in hugging face. Right? In hugging face. So here you'll be able to see various models itself, right? So if you just go ahead and click this getting started with embeddings. Here you have a lot of blogs that is available. You can directly open the code in Google Colab and you can directly use it. So if I'm just searching for this kind of models here, you can see there's so many different models that are already available which you can actually use for embedding type. So here you can see we have selected embeddings automatically. You can use all these specific models. But I want to go ahead and focus more on the open source models smaller models so that you can directly use this. And uh again we are using long chain right at the end of the day and we will try to convert our text into vectors. So for that we are going to use this all mini l6 v2. You can also go ahead and use different models based on your requirement. You can search from here and you can actually do it right. So I'm going to use this. — Installing Required Libraries — Now let's start with our first model itself. So what I'm actually going to do over here for this, we will be having some requirement with respect to the library. So I'm just going to make a simple category saying as embeddings. And we will be importing two different kind of. So we will be installing two different kind of libraries. One is lang chain hugging face and the other one is uh, let's say that after lang chain hugging face, since we also want to use the sentence transformer. So after writing lang chain hugging face, this is one of the library that we will be requiring. The other libraries that we will be using is nothing but sentence transformer, right? So sentence transformer will be my another library. Okay. Transformers okay. So these two libraries will be using and I already have Lang chain OpenAI. So with the help of this we will be able to import the OpenAI embedding models. But right now in this particular video we'll be focusing more on hugging face embedding models. Okay. So now it's time that we go ahead and install this library. So I will just go ahead and open my command prompt and quickly go ahead and write UV add minus our requirement dot txt. Okay. So here you can see that sentence transformer okay the spelling is wrong because I have to go ahead and write Transformers. Okay. Now I will just go ahead and do the installation. Now with respect to the installation here you can see that this installation has been done. I think sentence transformer has already been installed in this. Okay. So here is my sentence transformer. I think I had already done it so we don't require it. Again that basically means the installation has been completed. — Initializing a Hugging Face Embedding Model — Now what we are going to do over here is that we are going to go ahead and write our embedding code. Okay. So in the embedding code I'm going to go ahead and use my Lang chain hugging face. And we will try to use a library. Right. Uh, the same sentence transformer A model for doing the text embeddings right? So first of all, what I will do, I will go ahead and import from Lang. And there is something called as underscore hugging face. I'm going to go ahead and import hugging face embedding. Okay. So hugging face embeddings. Now this hugging face embedding library will actually help you to call any kind of models that is available in the hugging face itself. Okay. So here we will go ahead and initialize initialize a simple embedding model. So here obviously no API key is needed. Okay. So this is the most important thing since we are using the hugging face embedding library before when we are using Lang. Right. And when we used to use separately hugging face, at that point of time we required API key. But because of this library now it has become very much easy in order to call any kind of hugging face models, right? So now what we are going to do I'm going to go ahead and create my embeddings. So embeddings is equal to we will be using this hugging face embedding. And then I will just go ahead and give my model name. So model underscore name is equal to. Now remember how do I give my model name. That is really important. So I will go back to my hugging face since I'm going to use this right. So I will copy this particular model. Right sentence transformer all mini LM. Let's see. This is a sentence transformer model. It maps sentences and paragraph to a 384 dimension dense vector space and can be used for tasks like clustering or semantic search. Now see. At the end of the day, whenever we are working with vector databases, we are working with embeddings. We are going to perform this kind of task that is semantic search, right? Later on all these vectors can will be also stored in some kind of vector database. So for this we are going to use this again. It is a hugging face and at the end of the day it takes up any word or sentence. It will try to convert that into a 384 dimensional dense vector space. Uh, and then it can be used for task like this. Right. So we are going to use this. Right. And this is some of the example that you can see. Let's say if I have this specific sentence you can go ahead and execute it. Now I will show you step by step. So the model name I will just go ahead and copy it over here. Sentence transformer slash all mini lm v6 two. Right now if I just go ahead and execute this and show, let's let's see what embeddings will be able to give you. Now it is going to go ahead and initialize this for the first time. It is going to take some amount of time. But after this whenever you try to execute this it will be very much faster. Okay, so here you can see I progress not found. Please update. Okay this is fine. This is a kind of warnings. And now, uh, just in some time. In some seconds this should get executed. Um, with respect to this. Okay. So, uh, this is how you go ahead and import this now, whatever models that is available in the hugging face with respect to embeddings, you can just go ahead and write the model name over here and automatically that embedding model will be loaded. Okay. — Creating Your First Embedding — Now till this is getting executed, let me go ahead and start writing my next text. Because at the end of the day, we want to see a example of how to give a text and get an embedding vector of that right or vector representation of that. So here I'm going to go ahead and create your first embeddings okay. And let's say that I'm going to give the text which looks something like this. Hello I am learning about embedding. Now understand one thing guys okay so here we are saying this is a part of a sentence Transformers. Right. So what this model does is that it does not just take a single word word and convert that into a vector. It takes the entire sentence and convert it into a vector. Okay, this is really important. And since I'm using this particular model it will convert into a 384 dimensions. Okay, just to explain again here, we are not going to convert a word into a vector since we are using something called as a sentence transformer. So this is nothing, but we are using sentence transformer and this specific model that we have used. Right. It is going to convert a sentence into vector representation. Vector representation. Yeah. There are different models which will be able to convert a word into a vector representation. But in this particular scenario we are going to take this sentence because most of our data will be in the form of sentences chunks. And. All right. So here you can see this has got executed. Now let's say this is my text. I'm learning about embeddings okay embeddings. And I want to go ahead and see that if I give this text to my embedding model, what is the output that I'm actually going to get. So I'm going to go ahead and execute embeddings Dot embed query. See, whenever we are giving a single sentence at that point of time, we use this function which is called as embed query. If you are giving a list of sentences, then you can go ahead and give this embed document function. Right. So here we are going to go ahead and give my embed query. And here we are going to go ahead and give the text. Finally I will be able to get my embedding embedding that is my vector embeddings. Now I will go ahead and write print. Let's say initially this is what my text look like right. So text colon. Let's say this is what's my text. Right. Now we are going to go ahead and print two things. Okay. First let's say we are going to print the embedding length. Okay. So with respect to the embedding length we will be using length function. And we will give this embedding over here. Okay. So this will basically give me what is the length of the vector representation for this particular sentence. And you know the answer. It is 384 right. We saw in the documentation. Let's say I want to go ahead and print my entire embedding. Right. So what I will do, first of all, I will just let's say I want to go ahead and print it okay. So. So now I will go ahead and print my embedding. So let's go ahead and print this. Okay. Now let's execute this okay. So here you can see this is my text. Hello I am learning about embeddings. You can see this embedding length is 384. And this is how my embedding looks like right. So all these values and if you just go ahead and see this length it is nothing but 384. So that basically means this sentence has got converted in this to this particular vector representation. Right. Not a word, but the entire sentence has converted. So congratulations. This was quite amazing because at the end of the day here you are able to probably go ahead and use hugging face model, specifically embedding model. The embedding model name was all mini LM, L6, v2 and you are able to convert this into a vectors. — Embedding Multiple Sentences — Okay. Now similarly there you can also even try different different models. It is up to you. Okay, no one is stopping you that you only have to use this. You can use any of the models that is specifically available, right. But we will also go ahead and see some differences between this particular model. Like what are the basic difference? What are different kind of hugging face models are actually there, you know, so that things we will try to see it as we go ahead. And the next thing is that, you know, you can also give multiple text and probably convert that into a sentence. So let's say that I have I have some sentences okay. So let's say I have a sentence like this. The cat sat on the mat, a feline rested on the rug. The dog played in the yard. I love programming in Python. Python is my favorite programming language. Okay, now what I'm actually going to do. I will just go ahead and paste this same thing. Instead of using embed query, I'm going to go ahead and write embed documents. And instead of giving text I'm going to go ahead and give my sentences. Okay. So here I will go ahead and write embedding sentences. So it should be a list of vectors right. It should be a list of vectors. So now if I just go ahead and print this. Now here you should be able to get an output. See. So list of list right. So let's say if this is my first sentence what is my first sentence. This is how the vector looks like. If I were to go ahead and print my embedding of the second sentence that I've given in that list, I will just go ahead and print like this. Right? So similarly, you can go ahead and print the other sentences. That basically means a cat sat on the mat is nothing, but it is represented over here. If you want to see that, whether I'm going to get the same thing or not. Right. Let's see. Okay. So I will just copy the same line. I'll paste it over here to just show you. If I give the same sentence I'm going to get the same vector. Also see over here. Same vector. Same vector. Right. So now I hope you have understood what is the differences between embed query and embed documents. Right. — Quick Model Guide (HF) — So this was with respect to the hugging face where we have basically used this. What are the advantages. Definitely this is open. Like you can use it without any keys okay. And there are also different different models which I would definitely like to talk about it, you know, and I will probably give the comparison also for you okay. So let me paste some of the information over here. So see I've given all the information when you should use it. How you should use it in this entire code okay. So let's let's execute this okay. So here you can see if you are planning to use all mini lm, l6 v2. The embedding size is 384 dimension when you should use it. It should. Description is fast and efficient. Good quality use cases, general purpose and real time application. There is also one more example all MP. Net base v2. So here the embedding size is 768 dimension description best quality but it is slower than mini LM. Right. Mini LM which is their use case when quality matters more than speed. So if you have that kind of scenario where quality is important, then speed, then you can go ahead and use this. And as you know you are going to get more dimensions with respect to the vector representation. Then there is also like all mini L to L v2. So here also you are getting 384 dimensions. Description. You can see what it does slightly better than L6 bit slower okay. So if you compare it to L6 it is bit slower. Good balance of speed and quality right. So what you can actually do if you want to use this, just go ahead and change the model name. And automatically you will be able to Change the model name in hugging face embeddings automatically. You'll be able to do this right. Then you have the sentence transformer. This model also you can use it usually for Q&A systems. Semantic search. We basically use this optimized for question answering. Then you have one more model which is called as paraphrase multilingual mini lm l2 v2. Again 384 dimensions supports 50 plus languages. Let's say if you have a scenario where you have multiple languages, let's say if the text is in multiple languages, you can use this. And again multilingual application. So you can use any of them. Okay. Try to probably make a list of statement sentences with different different languages. Since it supports 50 plus languages. I think this will also be able to perform the embeddings. — Wrap-up — So this was about hugging face embeddings. I hope you like this particular video. Um, this was it. In the next video, we will also be talking about OpenAI and we will be comparing again different embedding models even in OpenAI and all. Okay. So yeah this was it. Take care. Have a great day.\"),\n",
       " Document(metadata={'section_name': 'Vector Embeddings And Vector Databases', 'section_number': '4', 'total_videos': '5', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Vector Embeddings And Vector Databases.pdf', 'video_title': 'Getting Started With OPENAI Embeddings', 'video_order': '4/5', 'topics': 'OpenAI embedding models, API key setup, pricing & quotas, model selection,', 'difficulty': 'Intermediate', 'doc_type': 'video'}, page_content='dimensions, single vs. batch embeddings, environment variables, LangChain integration Introduction So we are going to continue the discussion with respect to embeddings. Already in our previous video we created our first embedding using Hugging Face (the all-MiniLM-L6-v2 sentence transformer). We walked through multiple examples and compared a few Hugging Face embedding models like all-mpnet-base-v2 and others. Now it’s time to show you how to perform embeddings with OpenAI’s models—which are strong, accurate, and production-friendly. Exploring OpenAI Models • I’ll first search for OpenAI API key and open the OpenAI platform. • On the platform, you can find Models: GPT-4 families (text/audio/real-time), image generation (GPT-image-1, DALL·E 3/2), TTS, STT, and Embeddings. • Each model family serves a specific purpose. For RAG, we’ll focus on Embedding models (convert text → vectors), e.g.: o text-embedding-3-small o text-embedding-3-large o (Legacy) text-embedding-ada-002 When you click into a model you’ll see performance, speed, and pricing (typically listed per 1M tokens). You can also compare 3-large vs 3-small (quality vs. cost). We’ll use the embedding endpoint from code shortly. Creating Your OpenAI API Key 1. Go to Create a new secret key on the OpenAI dashboard. 2. Give it a name and generate—your key starts with sk-.... 3. You need credits in your account for usage. Under Settings → Billing, ensure you have positive balance (e.g., a few dollars is plenty to practice). In my account example, I show a balance of $0.62—I’ll top up when it hits zero. If you have zero, add at least $5. Picking an Embedding Model For embeddings we’ll choose one of: • text-embedding-3-small — balanced quality/cost, fast. • text-embedding-3-large — highest quality, more expensive. • text-embedding-ada-002 — previous-gen (legacy). Generally avoid for new builds unless you must match older systems. We’ll start with 3-small for demos. Project Setup & Environment Variables We’ll work inside a notebook/file (e.g., openai_embeddings.ipynb). Steps: 1. Create a .env file and add: o OPENAI_API_KEY=\"sk-...\" 2. Load environment variables in Python using python-dotenv: o from dotenv import load_dotenv o load_dotenv() → returns True if loaded 3. Verify key with os.getenv(\"OPENAI_API_KEY\") (don’t print secrets in real projects). 4. Set process env to ensure libraries see it: o os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") If you miss setting the env var, you’ll see errors like “API client option must be set…”. Once set, you’re good. Initializing OpenAI Embeddings in LangChain • Import: from langchain_openai import OpenAIEmbeddings • Init the model: o embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\") If the key is loaded properly, the instance initializes without error and you can call embedding methods. Single-Text Embedding (embed_query) We’ll embed a single sentence: Text \"LangChain and RAG are amazing framework and projects to work on.\" Call • vector = embeddings.embed_query(text) Result • Dimension: 1536 for text-embedding-3-small. • The returned vector is a list of 1536 floats. For display, I print the first 5 values and the total length. Output pattern • Original Input: (the sentence above) • Embedding Size: 1536 • Sample Embedding Head: [... first 5 floats ...] Batch Embeddings (embed_documents) We can embed multiple texts at once: Texts 1. \"Python is a programming language.\" 2. \"LangChain is a framework for LLM applications.\" 3. \"Embeddings convert text to numbers.\" 4. \"Vectors can be compared for similarity.\" Call • vectors = embeddings.embed_documents(texts) Result • Number of texts: 4 • Number of embeddings: 4 • Each embedding size: 1536 • You can inspect any item in vectors[i] and confirm the dimension. Model Cheat-Sheet & When To Use What text-embedding-3-small • Dimension: 1536 • Cost: ~$0.02 per 1M tokens (approx; check dashboard) • Use: General purpose, cost-effective, good performance text-embedding-3-large • Dimension: 3072 • Cost: higher (~$0.13 per 1M tokens; check dashboard) • Use: Highest quality; choose when accuracy is critical text-embedding-ada-002 (legacy) • Use: Support older/legacy setups; not recommended for new builds Wrap-Up That’s all about getting started with OpenAI Embeddings: • Create & load your API key • Initialize OpenAIEmbeddings with your chosen model • Use embed_query for single inputs and embed_documents for batches • Understand model trade-offs (dimension, quality, cost) In the next steps of the course, we’ll compare these embeddings, then store them in a vector database, and use them in the RAG retrieval pipeline.'),\n",
       " Document(metadata={'section_name': 'Vector Embeddings And Vector Databases', 'section_number': '4', 'total_videos': '5', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Vector Embeddings And Vector Databases.pdf', 'video_title': 'Semantic & Similarity Search Using OpenAI Embedding Models — Study Guide', 'video_order': '5/5', 'topics': 'OpenAI embedding models, Semantic & Similarity Search,', 'difficulty': 'Intermediate', 'doc_type': 'video'}, page_content='Overview Turn raw sentences into embeddings with OpenAI Embeddings and use cosine similarity to (1) compare pairs of sentences and (2) run a simple semantic search that returns the most relevant lines. What you’ll learn • Compute embeddings with text-embedding-3-small using LangChain’s OpenAIEmbeddings. • Implement cosine similarity from scratch. • Compare every pair of sentences and read the similarity scores. • Build a lightweight semantic search: embed a query, rank documents by similarity, return the top-K. Key concepts • Embedding: numeric vector representation of text (here, 1,536 dimensions). • Cosine similarity: dot(a, b) divided by (‖a‖ × ‖b‖). Values near 1 → very similar; near 0 → unrelated; near −1 → opposite. • Semantic search: retrieve items by meaning (vector proximity), not exact keywords. Quick setup (LangChain + OpenAI) from langchain_openai import OpenAIEmbeddings embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\") # single sentence v = embeddings.embed_query(\"Your sentence here\") # -> list[float] length 1536 # batch V = embeddings.embed_documents([\"s1\", \"s2\", \"s3\"]) # -> list[list[float]] Cosine similarity helper import numpy as np def cosine_similarity(a, b): a, b = np.array(a), np.array(b) num = np.dot(a, b) denom = np.linalg.norm(a) * np.linalg.norm(b) return num / denom Pairwise similarity (example) sentences = [ \"The cat sat on the mat.\", \"A feline rested on the rug.\", \"The dog played in the yard.\", \"I love programming in Python.\", \"Python is my favorite programming language.\", ] embs = embeddings.embed_documents(sentences) for i in range(len(sentences)): for j in range(i+1, len(sentences)): sim = cosine_similarity(embs[i], embs[j]) print(f\"{sentences[i]} vs {sentences[j]} -> {sim:.3f}\") Expected pattern: • Cat ↔ Feline ≈ ~0.65 (similar topic). • Python-love ↔ Python-favorite ≈ ~0.70+ (very similar). • Cat ↔ Python sentences ≈ low similarity. Minimal semantic search def semantic_search(query, documents, embed_model, top_k=3): qv = embed_model.embed_query(query) Dv = embed_model.embed_documents(documents) scored = [ (cosine_similarity(qv, dv), doc) for doc, dv in zip(documents, Dv) ] scored.sort(reverse=True, key=lambda x: x[0]) return scored[:top_k] docs = [ \"LangChain is a framework for developing applications powered by language models.\", \"Python is a high-level programming language.\", \"Machine learning is a subset of artificial intelligence.\", \"Embeddings convert text into numerical vectors.\", \"The weather today is sunny and warm.\", ] print(semantic_search(\"What is LangChain?\", docs, embeddings)) print(semantic_search(\"What are embeddings?\", docs, embeddings)) Expected top hits: • Query “What is LangChain?” → the LangChain sentence first. • Query “What are embeddings?” → the embeddings sentence first. Notes & pitfalls • Always normalize by vector norms when computing cosine similarity. • Embedding sizes differ across models (OpenAI small = 1536; large = 3072). Don’t mix models in one index. • For large corpora, use a vector DB (e.g., FAISS, Pinecone, Chroma) instead of brute-force Python loops. Sec 5 · Video 5 — Semantic & Similarity Search Using OpenAI Embedding Models Overview Short demo showing how to compute cosine similarity scores between sentence embeddings produced by OpenAI’s text-embedding-3-small and how to do a simple semantic search over a list of documents. What you’ll see • Problem statement & example sentences • Cosine similarity helper (NumPy) • Creating embeddings with OpenAIEmbeddings • Pairwise similarity comparisons (all-vs-all) • Building a minimal semantic search: query → top-K matches • Example outputs & interpretation (what high/low scores mean) Key snippets (for orientation only) • cosine_similarity(a, b) = (a · b) / (||a|| * ||b||) • embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\") • embeddings.embed_documents(list_of_texts) / embeddings.embed_query(query) Notes • Cosine ≈ 1.0 → very similar; ≈ 0 → unrelated; < 0 → opposite. • Semantic search = rank documents by similarity to the query embedding. Full transcript below Sematic And Similarity Search Using Open AI Embedding Models: So we are going to continue our discussion with respect to vector embeddings. Already in our previous video, we have seen that how we can use various embedding models and convert your text into vectors. Along with that, we also saw the comparison between three different embedding models that is provided by OpenAI. Now in this particular video, we are going to perform some cosine similarity with OpenAI embeddings. Now what is the problem statement that we are trying to solve over here? The problem statement is very simple here. Let\\'s consider that I have some sentences okay. So these are my sentences that the cat sat on the mat, a feline rested on the rug a the dog played in the yard. I love programming in Python. Python is my favorite programming language, so these are my sentences that are available now. What I will do is that I will try to use OpenAI embeddings. Along with that, I will try to go ahead and compare what is the similarity score, cosine similarity score between this line to this line and this sentence to this sentence. So all the comparison will be done 1 to 1. Okay. So this is the problem statement that I\\'m trying to solve over here. We are just trying to see that one sentence comparison to the other sentence. If we try to see with respect to vector representation how similar that is. So that is the problem statement that we are trying to solve okay. Now if you remember in our previous video we have already created a function which is called as cosine similarity. Right. So this is my cosine similarity here. Uh it is basically telling me to import numpy. So let\\'s go ahead and import numpy as np okay. So here in the cosine similarity the function is very simple. We\\'re just going and applying the cosine similarity score formula. That is the dot product divided by the magnitude of a multiplied by magnitude of p right. And once we do this we will be able to get the cosine similarity itself. Now let\\'s go ahead and apply all this particular sentence that we have. We will go ahead and apply the cosine similarity between each other. Okay. So quickly I will go ahead and write from line chain underscore OpenAI. So first of all I have to go ahead and import OpenAI embeddings. So I have already imported it. But again in front of you I\\'m actually trying to do it again. So embeddings is equal to we will go ahead and use the embeddings. Oh sorry. It should be OpenAI embeddings OpenAI embeddings. And we will load the model. Which model we will use the smaller one because it is quite cheap. Right. So the model that we\\'re going to use is text embedding three small. Okay. And this is how my embeddings looks like okay. Now what I am planning is that I will consider all the sentences and I\\'ll try to find out the similar sentence, or I\\'ll try to compare the similarity score between each other. Right. This is all sentences. Now what I\\'m actually going to do I will go ahead and first of all create my sentence Dense underscore embeddings okay is equal to and I\\'m going to use embeddings inside this embed underscore documents. We are going to give the value over here as sentences. So once I go ahead and see this this will basically give me my entire sentence embeddings right. So if you see over here we\\'ll be getting a list of embeddings. Right. How many different sentences are there based on that we will be getting this. Now my main over here aim is to calculate the calculate the similarity between all pairs okay. So I\\'m going to go ahead and calculate this between all pairs. So what I will do I will go ahead and run one loop for I in range off length of sentences. Right. So I\\'m going to go ahead and run this particular loop with respect to all the sentences. And then I will take another variable for j in range of I plus one. That is, I\\'m going to the next sentence and then I will be taking this tilde length of the sentences. So I\\'m comparing one sentences to all the other sentences. Yes. And then we will go ahead and compute the similarity score. So here I\\'m going to go ahead and write. Similarity is equal to cosine similarity. Cosine similarity. And we will get the sentence embeddings of I. That is the first sentence and j sentence is the nothing but the other sentences based on the loop. Right. So we here we are going to get the similarity. Now finally we\\'re going to go ahead and print f is equal to r with respect to the printing this I will just go ahead and use the simple print f statement okay. And I\\'ll use some styling over here. So let\\'s print it like this sentence of I versus sentence of Jay and we will just get the similarity score. Okay. So once I go ahead and execute this here, you can actually see all the information. The cat sat on the mat versus the feline rested on the rug. The similarity score is 0.65. The cat sat on the mat. The dog played in the yard. Here you can see the similarity score is 0.32 for the cat sat on the mat versus I love programming in Python. The similarity is 0.089. The cat sat on the mat. Python is my favorite programming language, so here you can see the similarity will be very very less because nothing is matching here. Also it is nothing is matching. So I\\'m getting 0.089 right. So with respect to this you can see all the values are less. But here you can see right I love programming in Python. Python is my favorite programming language. So here the similarity search score is also very very high. That is near to 1.708.708 is near to one. So this two sentences are almost similar. So this is how we are specifically getting the similarity score. Okay, now this is one example here with the help of similarity score. What you can also do is that you can give or you can retrieve a specific similar results. Right. So for that, what I will show is that my second example will be on. So if I go ahead and take this particular example it will be on semantic search okay. So what does semantic search mean. See my main aim is basically that whichever has the highest search try to give those similar kind of sentence. So what I will have let\\'s say that I have a list of sentences okay. And I go ahead and take one sentence and probably do a similarity search from those list of sentences. I should be getting out the similar, similar sentence itself. So now if I go ahead and do the semantic search in the semantic search, uh, first of all, let\\'s say that I have some documents. Okay. So this is my documents. My documents has like this. Right. So here I\\'m having like chain is a framework for developing application powered by language. Language models. Python is a high level programming language. Machine learning is a subset of artificial intelligence. Embeddings convert text into numerical vectors. The weather today is sunny and warm. Now what is my task? Let\\'s say if I just go ahead and take a query like what is lang? What is lang? Now what is lang? When I give this specific query from this documents, which is the similar sentence that should be retrieved, right? So with some specific score, just by directly seeing this, you know that this will be the first sentence that will be similar to each other, right? Similar to this sentence itself. Similar to this query. So this vector will match to this particular vector, you know, the vector representation. And based on that the cosine similarity will be also high. So we can retrieve this particular result directly from here. So semantic search basically means we are going to find or retrieve retrieve the similar sentence. That is what we are going to do over here. Okay. So let\\'s go ahead and execute this. This is my document. This is my query. Now I will go ahead and create a function which is called as semantic underscore search. And with respect to semantic underscore search. Here my first parameter that I\\'m actually going to take is query. So this will basically be my query. The second parameter will be nothing but documents. The third parameter is nothing but my embedding model. Let\\'s say here I\\'m going to go ahead and give my embedding model. And third I want to get how many similar sentence I have the top most three similar sentences. By default I want to give it as three. If you want four, you can go ahead and give it four also. Okay. So here this particular function is doing what. So here I will go ahead and write the sentence simple semantic search implementation. Right. So this is what we are basically going to do. And here we are going to write embed query and document. So what is first of all what we are going to do as soon as we call this particular function. Right. So first of all we will go ahead and create a embedding only for the query. So here I will go ahead and write embedding underscore models dot query or sorry dot embed query embed underscore query. So this will be the function that will convert the query into vectors okay. Similarly for documents if I go ahead and use documents for that we will be using embed documents. Right. So here you\\'ll be able to see that my embedding models dot embed documents. We are going to basically do it. And this documents will get converted into document embedding right. Now inside this function we will also go ahead and calculate the similarity score. Okay. So first of all I will go ahead and make a list. So let\\'s say similarities is equal to this will be an empty list. So I will iterate from I comma document embedding okay in enumerate. So I\\'m going to go ahead and use an enumerate. And I will go back to all the embeddings okay. So step by step we will go right. So inside this document embeddings it will be a list of embeddings. Right. So inside this embeddings when I\\'m going here I\\'m going to just go ahead and calculate my similarity by using the same function that is called as cosine similarity. Cosine similarity. And inside this I\\'m going to first of all give my query embedding. So this with respect to this particular query. This is my embedding that I\\'m actually going to get. And we are going to compare with the document embeddings right which we are iterating from here. So every document embeddings we are going to compare. So if I get the cosine similarity like some similarity score I will be able to get it over here. Right. So here I will go ahead and write. This is my similarity score. And this similarity I\\'m going to just go ahead and append inside my list. So similarity is dot append. So all the score we have first of all appending it right. So here we are going to append it right. So how do we append it. First of all we\\'ll go ahead and write the similarity score whatever similarity score we are having. And then we are also going to go ahead and provide our documents information like whichever is the document. With respect to that, we are going to put it over here right now. Since this is a list all the information will get once it is comparing with each and every sentences, right? Then we will go ahead and sort by similarity so that we get the top three records right. So here we will go ahead and write similarities dot sort. And here, while we are using the sort function we will give reverse is equal to true because whichever is the highest that needs to be shown in the top right. And then we are going to return similarities with respect to the top underscore key result. Right. So how many top underscore key result is nothing but three. So I need to only get the top three results right. So this is my function. And once we call this particular function you will be able to see that what we are going to get okay. So now I will just go ahead and display the results. So results I will call semantic search. And inside this first parameter is nothing but query. Query over here is nothing but what is launching. The second is nothing but documents. So here I\\'m going to go ahead and give my documents. The third parameter is nothing but embeddings right? So once I give all this three parameter the fourth parameter, I do not have to give it because top underscore k default value is three. Now I\\'ll go ahead and display my results. So once I display my results here you can see when I am searching for this, what is lying in the first sentence that I got right is lying in is a framework for developing application powered by language models. And here you can see the match is somewhere around 0.67. The similarity score is 0.67. Then next document that we got based on the similarity score is 0.103130. Python is a high level programming language out of all these documents. First is language, then then you have Python C. If you want to display this in a much more better way, I will print it in this way so that you will be able to see it much more clearly. Okay, so here you can see .676 language is the framework for developing application powered by language model. Then the second record that I got is this embedding converts text into vectors. Okay. Now let\\'s say I go ahead and change my sentence. My query this time should be what is embeddings? Okay, so let\\'s say I will just go ahead and search for what is embeddings. Okay. Now you know that out of all the specific records, this should be the first thing that should get displayed right? So I will use the same query and I will try to call this result. Now see this. I will try to call this result. Now you see the magic. The first sentence that you are going to get. Embedding converts text into numerical vectors. This is the maximum matching that is 0.662.62. Then machine learning is getting matched. Then long chain is a framework matched right? So this way you are specifically applying a semantic search. So based on a query based on a list of documents, what is the top match from that particular documents and how you are retrieving this specific results. This will be very, very handy because this all concepts will be reutilized in vector store and vector database. If you want to understand the differences between vector storm vector database, I will cover that in the future videos. But just understand inside our vector database or vector store, this kind of search will specifically happen in order to provide you the similar meaning sentence, right? And this is how the similar search will basically happen, right? So I hope you like this particular video. Uh, herein we discussed about two amazing things. Right. What all things we basically did over here. First of all, we tried to find out the similar sentences based on cosine similarity score. We find out we compared each and everything and we got the answer. We just calculated the similarity. This last one was with respect to the semantic search. Here we are trying to retrieve the similar sentences. Right. So both the topics we discussed in a much more depth, uh, which you can still more practice as you like. Yes. This was it from my side. I hope you liked this particular video. I will Take care.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_video_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_video_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define chunking parameters\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total chunks created: 226\n"
     ]
    }
   ],
   "source": [
    "# Split all video documents\n",
    "chunked_docs = splitter.split_documents(all_video_docs)\n",
    "\n",
    "print(f\"✅ Total chunks created: {len(chunked_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧩 SAMPLE CHUNKS PREVIEW\n",
      " ======================================================================\n",
      "\n",
      "🔹 Chunk 1\n",
      "Section: Core Components in RAG\n",
      "Video: Data Ingestion and Parsing\n",
      "Chunk length: 982 chars\n",
      "Metadata keys: ['section_name', 'section_number', 'total_videos', 'course_name', 'date_created', 'source_file', 'video_title', 'video_order', 'topics', 'difficulty', 'doc_type']\n",
      "\n",
      "Content preview:\n",
      "So we are going to continue the discussion of Retrieval-Augmented Generation (RAG). In this specific video, we’ll dive into the core components of a RAG pipeline. By now, you already have an intuition for how RAG works at a high level: we have a Large Language Model (LLM), we augment it with externa...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔹 Chunk 2\n",
      "Section: Core Components in RAG\n",
      "Video: Data Ingestion and Parsing\n",
      "Chunk length: 769 chars\n",
      "Metadata keys: ['section_name', 'section_number', 'total_videos', 'course_name', 'date_created', 'source_file', 'video_title', 'video_order', 'topics', 'difficulty', 'doc_type']\n",
      "\n",
      "Content preview:\n",
      ". From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1. Document Ingestion Phase 2. Query Processing Phase 3. Generation Phase This video focuses on Phase 1: Document Ingestion and Pre-processing. We’ll break down what data we ingest, how we clean ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔹 Chunk 3\n",
      "Section: Core Components in RAG\n",
      "Video: Data Ingestion and Parsing\n",
      "Chunk length: 976 chars\n",
      "Metadata keys: ['section_name', 'section_number', 'total_videos', 'course_name', 'date_created', 'source_file', 'video_title', 'video_order', 'topics', 'difficulty', 'doc_type']\n",
      "\n",
      "Content preview:\n",
      ". This knowledge can come from multiple sources: company policies, internal documents, PDFs, Word docs, CSVs, websites, databases, images (with extracted text), and more. The ingestion pipeline’s job is to: • Load data from these sources • Pre-process the raw text (cleaning, normalization) • Split d...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔹 Chunk 4\n",
      "Section: Core Components in RAG\n",
      "Video: Data Ingestion and Parsing\n",
      "Chunk length: 914 chars\n",
      "Metadata keys: ['section_name', 'section_number', 'total_videos', 'course_name', 'date_created', 'source_file', 'video_title', 'video_order', 'topics', 'difficulty', 'doc_type']\n",
      "\n",
      "Content preview:\n",
      ". The Ingestion Pipeline: Step by Step Step 1: Load the Documents Use appropriate loaders per file type to extract text and attach preliminary metadata (e.g., source, filename, section, url). Step 2: Pre-process the Text Light cleaning (remove boilerplate, headers/footers if noisy, fix line breaks, ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔹 Chunk 5\n",
      "Section: Core Components in RAG\n",
      "Video: Data Ingestion and Parsing\n",
      "Chunk length: 952 chars\n",
      "Metadata keys: ['section_name', 'section_number', 'total_videos', 'course_name', 'date_created', 'source_file', 'video_title', 'video_order', 'topics', 'difficulty', 'doc_type']\n",
      "\n",
      "Content preview:\n",
      ". • Think of chunks as paragraphs or sections sized for retrieval. • Overlap (e.g., 50–100 tokens) helps preserve continuity across chunk boundaries. Why chunking is necessary: If you try to hand the LLM a thousand-page book as context, it won’t fit. Even if it did, the relevant part might be buried...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- PREVIEW SAMPLE CHUNKS ---\n",
    "print(\"\\n🧩 SAMPLE CHUNKS PREVIEW\\n\", \"=\"*70)\n",
    "for i, doc in enumerate(chunked_docs[:5]):\n",
    "    print(f\"\\n🔹 Chunk {i+1}\")\n",
    "    print(f\"Section: {doc.metadata.get('section_name')}\")\n",
    "    print(f\"Video: {doc.metadata.get('video_title')}\")\n",
    "    print(f\"Chunk length: {len(doc.page_content)} chars\")\n",
    "    print(f\"Metadata keys: {list(doc.metadata.keys())}\")\n",
    "    print(f\"\\nContent preview:\\n{doc.page_content[:300]}...\")\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheking the overalp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 OVERLAP CHECK\n",
      " ======================================================================\n",
      "\n",
      "Between Chunk 1 and 2:\n",
      "------------------------------------------------------------\n",
      "🔹 End of Chunk A:\n",
      " ontext to the LLM, and then the LLM generates a summarized, grounded output using that context. From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1\n",
      "\n",
      "🔹 Start of Chunk B:\n",
      " . From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1. Document Ingestion Phase 2. Query Processing Phase 3. Generation Phase This video focuses on\n",
      "------------------------------------------------------------\n",
      "\n",
      "Between Chunk 2 and 3:\n",
      "------------------------------------------------------------\n",
      "🔹 End of Chunk A:\n",
      " rs that represent our knowledge. This knowledge can come from multiple sources: company policies, internal documents, PDFs, Word docs, CSVs, websites, databases, images (with extracted text), and more\n",
      "\n",
      "🔹 Start of Chunk B:\n",
      " . This knowledge can come from multiple sources: company policies, internal documents, PDFs, Word docs, CSVs, websites, databases, images (with extracted text), and more. The ingestion pipeline’s job \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔁 OVERLAP CHECK\\n\", \"=\"*70)\n",
    "\n",
    "for i in range(2):\n",
    "    chunk_a = chunked_docs[i].page_content[-200:]\n",
    "    chunk_b = chunked_docs[i+1].page_content[:200]\n",
    "    print(f\"\\nBetween Chunk {i+1} and {i+2}:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"🔹 End of Chunk A:\\n\", chunk_a)\n",
    "    print(\"\\n🔹 Start of Chunk B:\\n\", chunk_b)\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Total tokens across all chunks: 44,347\n",
      "Average tokens per chunk: 196\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")  # or \"text-embedding-3-large\"\n",
    "total_tokens = 0\n",
    "\n",
    "for doc in chunked_docs:\n",
    "    total_tokens += len(encoding.encode(doc.page_content))\n",
    "\n",
    "print(f\"🔢 Total tokens across all chunks: {total_tokens:,}\")\n",
    "print(f\"Average tokens per chunk: {total_tokens // len(chunked_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding + Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API key loaded: True\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Load Environment Variables ---\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads your .env file automatically\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"✅ OpenAI API key loaded:\", bool(openai_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x0000022BFFF2F3E0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x0000022BFFF565D0>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 2: Initialize OpenAI Embedding Model ---\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # accurate + cost-efficient\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Create / Persist ChromaDB Store ---\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "persist_directory = \"chroma_store\"\n",
    "\n",
    "# Create or connect to existing store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunked_docs,      # your 226 chunks\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ChromaDB store created and persisted at: chroma_store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zabo0\\AppData\\Local\\Temp\\ipykernel_33724\\66909357.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# Save to disk so you can reload later\n",
    "vectorstore.persist()\n",
    "print(\"✅ ChromaDB store created and persisted at:\", persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'difficulty': 'Beginner', 'total_videos': '2', 'source_file': 'Core Components In RAG.pdf', 'video_title': 'Data Ingestion and Parsing', 'date_created': '2024', 'topics': 'Document ingestion, Pre-processing, Chunking, Embeddings, Vector databases,', 'section_number': '2', 'section_name': 'Core Components in RAG', 'doc_type': 'video', 'video_order': '1/2'}, page_content='. From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1. Document Ingestion Phase 2. Query Processing Phase 3. Generation Phase This video focuses on Phase 1: Document Ingestion and Pre-processing. We’ll break down what data we ingest, how we clean and split it, how we embed it into vectors, and how we store it in a vector database so that later phases (query + generation) can work effectively. What is Document Ingestion and Pre-processing? To power the retriever, we first need a vector database filled with vectors that represent our knowledge. This knowledge can come from multiple sources: company policies, internal documents, PDFs, Word docs, CSVs, websites, databases, images (with extracted text), and more'),\n",
       " Document(metadata={'difficulty': 'Beginner', 'doc_type': 'video', 'topics': 'RAG fundamentals, Retrieval, Augmentation, Generation, Architecture overview', 'source_file': 'intro_to_rag.pdf', 'section_name': 'Introduction to RAG', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and Langsmith', 'video_order': '1/4', 'total_videos': '4', 'video_title': 'Introduction to RAG', 'date_created': '2024', 'section_number': '1'}, page_content=\"So I'm super excited to start the series of videos on RAG. In this video and in the upcoming series of videos, we are going to understand everything about retrieval augmented generation, a very important topic currently. We will talk about various architectures. We'll talk about how to build a RAG pipeline. We'll talk about how to build generative AI with RAG, how to build generative AI applications with RAG. If I talk about the current scenario, more than 80 percentage of the business use cases currently who are working in generative AI and AI, they are building RAG applications. RAG applications. So it is really important for anyone who is specifically interested in generative AI or genetic AI. They need to know how RAG architecture is basically implemented. RAG Architecture Overview Now, with respect to the RAG architecture here, I have shown you one type of architecture where we have three important phases: 1. Document Ingestion phase 2. Query processing phase 3\"),\n",
       " Document(metadata={'source_file': 'intro_to_rag.pdf', 'doc_type': 'video', 'topics': 'RAG workflow, Similarity search, Augmentation details, Real-world examples,', 'total_videos': '4', 'difficulty': 'Beginner-Intermediate', 'date_created': '2024', 'section_name': 'Introduction to RAG', 'section_number': '1', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and Langsmith', 'video_title': 'Some Examples and Advantages Of Using RAG', 'video_order': '2/4'}, page_content=\". So these are the three main important things with respect to any RAG architecture. Why Does RAG Actually Matter? Now let me ask one very important question. And this question is just like everybody will be probably thinking about it. Why does RAG actually matter? You know, why is RAG really helpful? I'll talk about various companies who are able to just do some amazing things with respect to RAG, you know, are they able to save huge amount of money? Bigger companies like JP Morgan and all. And I will probably share you the entire links also if you want. But amazing work with the help of RAG they are actually able to do. Restaurant Chef Analogy Now let's take one simple example and let me explain more about RAG because it is important that you understand with a simple example itself. Let's say, and then we'll try to also compare this with LLM. Let's say there is a restaurant. So I will just try to draw some restaurant over here. So let's say this is my restaurant\")]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 4: Test Retrieval ---\n",
    "query = \"What are the main phases in a RAG system?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Full RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zabo0\\AppData\\Local\\Temp\\ipykernel_33724\\637535988.py:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieved 3 relevant documents\n",
      "\n",
      "Result 1: Data Ingestion and Parsing (Core Components in RAG)\n",
      ". From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1. Document Ingestion Phase 2. Query Processing Phase 3. Generation Phase This video focuses on Phase 1: Document Ingestion and Pre-processing. We’ll break down what data we ingest, how we clean  ...\n",
      "\n",
      "Result 2: Some Examples and Advantages Of Using RAG (Introduction to RAG)\n",
      ". So these are the three main important things with respect to any RAG architecture. Why Does RAG Actually Matter? Now let me ask one very important question. And this question is just like everybody will be probably thinking about it. Why does RAG actually matter? You know, why is RAG really helpfu ...\n",
      "\n",
      "Result 3: Introduction to RAG (Introduction to RAG)\n",
      "So I'm super excited to start the series of videos on RAG. In this video and in the upcoming series of videos, we are going to understand everything about retrieval augmented generation, a very important topic currently. We will talk about various architectures. We'll talk about how to build a RAG p ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: Create the Retriever ---\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}   # number of chunks to retrieve\n",
    ")\n",
    "\n",
    "# Test the retriever alone (no LLM yet)\n",
    "query = \"What are the three main phases in a RAG system?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"✅ Retrieved {len(retrieved_docs)} relevant documents\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Result {i}: {doc.metadata.get('video_title')} ({doc.metadata.get('section_name')})\")\n",
    "    print(doc.page_content[:300], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Memory ok. Messages: [HumanMessage(content='What are the main components of a RAG pipeline?', additional_kwargs={}, response_metadata={}), AIMessage(content='Retrieval, augmentation, and generation.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Rebuild memory with the correct keys\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",   # <- matches ConversationalRetrievalChain input\n",
    "    output_key=\"answer\",    # <- the field we want stored\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Optional quick test (use matching keys!)\n",
    "memory.save_context(\n",
    "    {\"question\": \"What are the main components of a RAG pipeline?\"},\n",
    "    {\"answer\": \"Retrieval, augmentation, and generation.\"}\n",
    ")\n",
    "print(\"🧠 Memory ok. Messages:\", memory.load_memory_variables({})[\"chat_history\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prompt template created successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 3: Build Prompt Template ---\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert study assistant for the 'Ultimate RAG Bootcamp'.\n",
    "Use ONLY the information from the provided context to answer the student's question.\n",
    "If the answer is not found in the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Chat history:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a clear, educational, and concise way.\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\", \"chat_history\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Prompt template created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 4: Initialize the LLM ---\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",   # you can also try \"gpt-4-turbo\"\n",
    "    temperature=0,         # 0 = factual, deterministic answers\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(\"✅ LLM initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG Conversational Chain created successfully (with memory fix).\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 5 (fixed): Build Conversational Retrieval Chain ---\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": rag_prompt},\n",
    "    return_source_documents=True,\n",
    "    output_key=\"answer\"   # ✅ tell it to store only the answer in memory\n",
    ")\n",
    "\n",
    "print(\"✅ RAG Conversational Chain created successfully (with memory fix).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 QUESTION: How to load pdf files?\n",
      "\n",
      "💬 ANSWER:\n",
      " You can load PDF files using the following methods:\n",
      "\n",
      "1. **PyPDFLoader**:\n",
      "   - Create a loader with the file path (e.g., `data/pdf/attention.pdf`).\n",
      "   - Call `.load()` to get a list of Document objects, one for each page.\n",
      "   - Each Document contains `page_content` (extracted text) and `metadata` (fields like page number, source, creator, etc.).\n",
      "\n",
      "2. **PyMuPDFLoader**:\n",
      "   - You may need to install the `pymupdf` library first.\n",
      "   - Create the loader with the same file and call `.load()`.\n",
      "   - Inspect the returned Documents and metadata.\n",
      "   - This method is generally fast, robust in text extraction, and supports image extraction.\n",
      "\n",
      "3. **UnstructuredPDFLoader** (to be discussed later):\n",
      "   - Useful for complex layouts, PDFs with images, tables, and heuristic segmentation before chunking.\n",
      "\n",
      "After loading, you should print diagnostics, including the number of pages, a preview of the first page, and the metadata captured.\n",
      "\n",
      "📚 SOURCES:\n",
      "\n",
      "--- Source 1 ---\n",
      "From: Data Ingestion And Parsing Techniques.pdf\n",
      "Section: Data Ingestion and Data Parsing Techniques\n",
      "Video: Ingestion and Parsing PDF Documents\n",
      "Content preview: . After loading, print diagnostics: • Number of pages (e.g., 15 pages loaded) • Page 1 preview → first 100 characters • Metadata → show how much information is automatically captured This is an easy way to read a PDF and immediately get page-oriented...\n",
      "\n",
      "--- Source 2 ---\n",
      "From: Data Ingestion And Parsing Techniques.pdf\n",
      "Section: Data Ingestion and Data Parsing Techniques\n",
      "Video: Ingestion and Parsing PDF Documents\n",
      "Content preview: .g., data_parsing_pdf.ipynb, because PDF handling is a very common real-world requirement. Loading PDFs with LangChain (Approaches) We will import loaders from langchain_community.document_loaders and demonstrate three approaches (we will implement t...\n"
     ]
    }
   ],
   "source": [
    "query = \"How to load pdf files?\"\n",
    "response = rag_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"🧠 QUESTION:\", query)\n",
    "print(\"\\n💬 ANSWER:\\n\", response[\"answer\"])\n",
    "\n",
    "print(\"\\n📚 SOURCES:\")\n",
    "for i, doc in enumerate(response[\"source_documents\"][:2]):\n",
    "    print(f\"\\n--- Source {i+1} ---\")\n",
    "    print(f\"From: {doc.metadata.get('source_file')}\")\n",
    "    print(f\"Section: {doc.metadata.get('section_name')}\")\n",
    "    print(f\"Video: {doc.metadata.get('video_title')}\")\n",
    "    print(f\"Content preview: {doc.page_content[:250]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
