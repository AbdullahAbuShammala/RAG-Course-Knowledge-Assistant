{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs found: ['Core Components In RAG.pdf', 'Data Ingestion And Parsing Techniques.pdf', 'intro_to_rag.pdf', 'Vector Embeddings And Vector Databases.pdf', 'Vector Stores Vs Vector Databases.pdf']\n",
      "\n",
      "Loaded 7 pages from Core Components In RAG.pdf\n",
      "First page metadata: {'producer': 'Microsoft¬Æ Word for Microsoft 365', 'creator': 'Microsoft¬Æ Word for Microsoft 365', 'creationdate': '2025-10-19T20:24:26+03:00', 'source': 'c:\\\\Users\\\\zabo0\\\\OneDrive\\\\Desktop\\\\RAG-Course-Knowledge-Assistant\\\\RAG-Course-Knowledge-Assistant\\\\pdf\\\\Core Components In RAG.pdf', 'file_path': 'c:\\\\Users\\\\zabo0\\\\OneDrive\\\\Desktop\\\\RAG-Course-Knowledge-Assistant\\\\RAG-Course-Knowledge-Assistant\\\\pdf\\\\Core Components In RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:24:26+03:00', 'trapped': '', 'modDate': \"D:20251019202426+03'00'\", 'creationDate': \"D:20251019202426+03'00'\", 'page': 0}\n",
      "\n",
      "First 500 chars:\n",
      "Section 2: Core Components in RAG \n",
      "Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith \n",
      "Section Number: 2 \n",
      "Total Videos: 2 \n",
      "Date Created: 2024 \n",
      " \n",
      "Video 1: Data Ingestion and Parsing \n",
      "Video Order: 1/2 \n",
      "Topics: Document ingestion, Pre-processing, Chunking, Embeddings, Vector databases, \n",
      "Similarity search, Cosine/Euclidean distance \n",
      "Difficulty: Beginner \n",
      "Content \n",
      "Hello guys. \n",
      "So we are going to continue the discussion of Retrieval-Augmented Generation (RAG). In \n",
      "this specific vid\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "# Go up one level from the notebooks folder to reach the main project folder\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "pdf_folder = os.path.join(base_dir, \"pdf\")\n",
    "\n",
    "# Get first PDF file\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "print(f\"PDFs found: {pdf_files}\")\n",
    "\n",
    "# Load first PDF\n",
    "first_pdf_path = os.path.join(pdf_folder, pdf_files[0])\n",
    "loader = PyMuPDFLoader(first_pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"\\nLoaded {len(docs)} pages from {pdf_files[0]}\")\n",
    "print(f\"First page metadata: {docs[0].metadata}\")\n",
    "print(f\"\\nFirst 500 chars:\\n{docs[0].page_content[:500]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Microsoft¬Æ Word for Microsoft 365', 'creator': 'Microsoft¬Æ Word for Microsoft 365', 'creationdate': '2025-10-19T20:24:26+03:00', 'source': 'c:\\\\Users\\\\zabo0\\\\OneDrive\\\\Desktop\\\\RAG-Course-Knowledge-Assistant\\\\RAG-Course-Knowledge-Assistant\\\\pdf\\\\Core Components In RAG.pdf', 'file_path': 'c:\\\\Users\\\\zabo0\\\\OneDrive\\\\Desktop\\\\RAG-Course-Knowledge-Assistant\\\\RAG-Course-Knowledge-Assistant\\\\pdf\\\\Core Components In RAG.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': 'Abdullah Salah', 'subject': '', 'keywords': '', 'moddate': '2025-10-19T20:24:26+03:00', 'trapped': '', 'modDate': \"D:20251019202426+03'00'\", 'creationDate': \"D:20251019202426+03'00'\", 'page': 0}, page_content='Section 2: Core Components in RAG \\nCourse: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith \\nSection Number: 2 \\nTotal Videos: 2 \\nDate Created: 2024 \\n \\nVideo 1: Data Ingestion and Parsing \\nVideo Order: 1/2 \\nTopics: Document ingestion, Pre-processing, Chunking, Embeddings, Vector databases, \\nSimilarity search, Cosine/Euclidean distance \\nDifficulty: Beginner \\nContent \\nHello guys. \\nSo we are going to continue the discussion of Retrieval-Augmented Generation (RAG). In \\nthis specific video, we‚Äôll dive into the core components of a RAG pipeline. By now, you \\nalready have an intuition for how RAG works at a high level: we have a Large Language \\nModel (LLM), we augment it with external knowledge stored in a vector database, and the \\nLLM uses retrieved context from that database to generate better answers. \\nAt a glance, when I provide an input to a plain LLM, it just generates an output from its \\ninternal knowledge. In RAG, however, the LLM is connected to a vector database. We \\nstore information in that database as vectors (numerical representations). When we send \\na question, we first retrieve relevant information from the vector database, pass that \\nretrieved context to the LLM, and then the LLM generates a summarized, grounded output \\nusing that context. \\nFrom the architectural diagram we discussed earlier, there are three main phases in a full \\nRAG system: \\n1. Document Ingestion Phase \\n2. Query Processing Phase \\n3. Generation Phase \\nThis video focuses on Phase 1: Document Ingestion and Pre-processing. We‚Äôll break \\ndown what data we ingest, how we clean and split it, how we embed it into vectors, and \\nhow we store it in a vector database so that later phases (query + generation) can work \\neffectively.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "\n",
      "Page 1 - Last 200 chars:\n",
      "‚Äôll break \n",
      "down what data we ingest, how we clean and split it, how we embed it into vectors, and \n",
      "how we store it in a vector database so that later phases (query + generation) can work \n",
      "effectively.\n",
      "\n",
      "Page 1 - First 200 chars:\n",
      "Section 2: Core Components in RAG \n",
      "Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith \n",
      "Section Number: 2 \n",
      "Total Videos: 2 \n",
      "Date Created: 2024 \n",
      " \n",
      "Video 1: Data Ingestion and Parsing\n",
      "------------------------------\n",
      "\n",
      "Page 2 - Last 200 chars:\n",
      "ine. \n",
      " \n",
      "The Ingestion Pipeline: Step by Step \n",
      "Step 1: Load the Documents \n",
      "Use appropriate loaders per file type to extract text and attach preliminary metadata (e.g., \n",
      "source, filename, section, url).\n",
      "\n",
      "Page 2 - First 200 chars:\n",
      "What is Document Ingestion and Pre-processing? \n",
      "To power the retriever, we first need a vector database filled with vectors that represent \n",
      "our knowledge. This knowledge can come from multiple sources\n",
      "------------------------------\n",
      "\n",
      "Page 3 - Last 200 chars:\n",
      "maDB \n",
      "‚Ä¢ \n",
      "FAISS (library) \n",
      "‚Ä¢ \n",
      "Pinecone (managed) \n",
      "‚Ä¢ \n",
      "DataStax / Cassandra with vector support \n",
      "This database supports similarity search so we can find the most relevant chunks for a \n",
      "given query later.\n",
      "\n",
      "Page 3 - First 200 chars:\n",
      "Step 2: Pre-process the Text \n",
      "Light cleaning (remove boilerplate, headers/footers if noisy, fix line breaks, normalize \n",
      "whitespace, optionally remove very long tables or binary debris) so the chunks a\n",
      "------------------------------\n",
      "\n",
      "‚úó Video 2 not in first page\n"
     ]
    }
   ],
   "source": [
    "# Check how content flows across pages\n",
    "print(\"=\"*50)\n",
    "for i in range(min(3, len(docs))):  # First 3 pages\n",
    "    print(f\"\\nPage {i+1} - Last 200 chars:\")\n",
    "    print(docs[i].page_content[-200:])\n",
    "    print(f\"\\nPage {i+1} - First 200 chars:\")\n",
    "    print(docs[i].page_content[:200])\n",
    "    print(\"-\"*30)\n",
    "\n",
    "# Look for video boundaries\n",
    "full_text = docs[0].page_content\n",
    "if \"Video 2:\" in full_text or \"Video Order: 2\" in full_text:\n",
    "    print(\"\\n‚úì Multiple videos found in same page\")\n",
    "else:\n",
    "    print(\"\\n‚úó Video 2 not in first page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for video markers across all pages...\n",
      "\n",
      "Page 1: Found video marker\n",
      "  Video 1: Data Ingestion and Parsing \n",
      "  Video Order: 1/2 \n",
      "Page 5: Found video marker\n",
      "  Video 2: Query Processing and Output Generation Phase \n",
      "  Video Order: 2/2 \n",
      "\n",
      "Total 'Video Order:' found: 2\n",
      "\n",
      "Video 2 found at character position: 5882\n",
      "Context around Video 2:\n",
      "nto the LLM for the Generation Phase. \n",
      "Thank you! Video 2: Query Processing and Output Generation Phase \n",
      "Video Order: 2/2 \n",
      "Topics: Query processing, Query embedding, Similarity search, Retrieval, Context \n",
      "enrichment, Generation, LLMs (OpenAI, Llama, \n"
     ]
    }
   ],
   "source": [
    "# Find all video markers across all pages\n",
    "print(\"Searching for video markers across all pages...\\n\")\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    if \"Video Order:\" in doc.page_content:\n",
    "        # Find the video header\n",
    "        lines = doc.page_content.split('\\n')\n",
    "        for j, line in enumerate(lines[:10]):  # Check first 10 lines\n",
    "            if \"Video\" in line and \"Order:\" in lines[j+1] if j+1 < len(lines) else False:\n",
    "                print(f\"Page {i+1}: Found video marker\")\n",
    "                print(f\"  {line}\")\n",
    "                print(f\"  {lines[j+1]}\")\n",
    "                break\n",
    "    \n",
    "# Check total content length\n",
    "total_content = \" \".join([doc.page_content for doc in docs])\n",
    "video_count = total_content.count(\"Video Order:\")\n",
    "print(f\"\\nTotal 'Video Order:' found: {video_count}\")\n",
    "\n",
    "# Find where Video 2 starts\n",
    "if \"Video 2:\" in total_content:\n",
    "    video2_index = total_content.find(\"Video 2:\")\n",
    "    print(f\"\\nVideo 2 found at character position: {video2_index}\")\n",
    "    print(\"Context around Video 2:\")\n",
    "    print(total_content[video2_index-50:video2_index+200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCANNING ALL PDFs FOR STRUCTURE PATTERNS\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìÑ Core Components In RAG.pdf\n",
      "   Pages: 7\n",
      "   Videos found: 2\n",
      "   Structure preview:\n",
      "   Section 2: Core Components in RAG  Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith  Section Number: 2  Total Videos: 2  Date Created: 2024    Video 1: Data Ingestion and Parsing\n",
      "   Patterns: ['Has Section:', 'Has Video:', 'Has Topics:', 'Has Difficulty:']\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Data Ingestion And Parsing Techniques.pdf\n",
      "   Pages: 36\n",
      "   Videos found: 9\n",
      "   Structure preview:\n",
      "   Section 3: Data Ingestion and Data Parsing Techniques  Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith  Section Number: 3  Total Videos: 9  Date Created: 2024    Video 1: Docume\n",
      "   Patterns: ['Has Section:', 'Has Video:', 'Has Topics:', 'Has Difficulty:']\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ intro_to_rag.pdf\n",
      "   Pages: 20\n",
      "   Videos found: 4\n",
      "   Structure preview:\n",
      "   Section 1: Introduction to RAG  Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and Langsmith  Section Number: 1  Total Videos: 4  Date Created: 2024    Video 1: Introduction to RAG  Video Or\n",
      "   Patterns: ['Has Section:', 'Has Video:', 'Has Topics:', 'Has Difficulty:']\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Vector Embeddings And Vector Databases.pdf\n",
      "   Pages: 38\n",
      "   Videos found: 5\n",
      "   Structure preview:\n",
      "   Section 4: Vector Embeddings And Vector Databases  Course: Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith  Section Number: 4  Total Videos: 5  Date Created: 2024    Video 1: Introducti\n",
      "   Patterns: ['Has Section:', 'Has Video:', 'Has Topics:', 'Has Difficulty:']\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Vector Stores Vs Vector Databases.pdf\n",
      "   Pages: 6\n",
      "   Videos found: 0\n",
      "   Structure preview:\n",
      "   Vector Stores Vs Vector Databases  Section: Vector Stores And Vector Databases  Overview  In previous lessons we generated embeddings using models like OpenAI and Hugging Face  and ran semantic search\n",
      "   Patterns: ['Has Section:']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Quick scan of all PDFs to understand their different structures\n",
    "print(\"SCANNING ALL PDFs FOR STRUCTURE PATTERNS\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    print(f\"\\nüìÑ {pdf_file}\")\n",
    "    print(f\"   Pages: {len(docs)}\")\n",
    "    \n",
    "    # Check for video markers\n",
    "    full_text = \" \".join([doc.page_content for doc in docs])\n",
    "    video_count = full_text.count(\"Video Order:\")\n",
    "    \n",
    "    print(f\"   Videos found: {video_count}\")\n",
    "    \n",
    "    # Check first 300 chars to see structure\n",
    "    print(f\"   Structure preview:\")\n",
    "    print(f\"   {docs[0].page_content[:200].replace(chr(10), ' ')}\")\n",
    "    \n",
    "    # Look for common patterns\n",
    "    patterns = {\n",
    "        \"Has Section:\": \"Section\" in full_text[:500],\n",
    "        \"Has Video:\": \"Video\" in full_text[:500], \n",
    "        \"Has Topics:\": \"Topics:\" in full_text[:500],\n",
    "        \"Has Difficulty:\": \"Difficulty:\" in full_text[:500]\n",
    "    }\n",
    "    print(f\"   Patterns: {[k for k,v in patterns.items() if v]}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT QUALITY ANALYSIS\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìÑ Core Components In RAG.pdf\n",
      "\n",
      "üìä Quality Metrics:\n",
      "   Total characters: 10,320\n",
      "   Total words: 1,646\n",
      "   Avg words per page: 235\n",
      "\n",
      "‚ö†Ô∏è  Potential Issues:\n",
      "   Pages ending mid-sentence: 1/7\n",
      "   Possible code blocks: False\n",
      "   Non-ASCII characters: 58\n",
      "   Double spaces: 0, Triple newlines: 0\n",
      "   Timestamp patterns: 0\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ Data Ingestion And Parsing Techniques.pdf\n",
      "\n",
      "üìä Quality Metrics:\n",
      "   Total characters: 58,233\n",
      "   Total words: 8,912\n",
      "   Avg words per page: 247\n",
      "\n",
      "‚ö†Ô∏è  Potential Issues:\n",
      "   Pages ending mid-sentence: 13/36\n",
      "   Possible code blocks: True\n",
      "   Non-ASCII characters: 444\n",
      "   Double spaces: 0, Triple newlines: 0\n",
      "   Timestamp patterns: 0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Document Quality Analysis\n",
    "import re\n",
    "\n",
    "print(\"DOCUMENT QUALITY ANALYSIS\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for pdf_file in pdf_files[:2]:  # Start with first 2 PDFs\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    full_text = \" \".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    print(f\"\\nüìÑ {pdf_file}\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    print(\"\\nüìä Quality Metrics:\")\n",
    "    print(f\"   Total characters: {len(full_text):,}\")\n",
    "    print(f\"   Total words: {len(full_text.split()):,}\")\n",
    "    print(f\"   Avg words per page: {len(full_text.split())//len(docs)}\")\n",
    "    \n",
    "    # Check for common issues\n",
    "    print(\"\\n‚ö†Ô∏è  Potential Issues:\")\n",
    "    \n",
    "    # 1. Broken sentences across pages?\n",
    "    broken_sentences = 0\n",
    "    for doc in docs[:-1]:\n",
    "        if doc.page_content.strip()[-1] not in '.!?':\n",
    "            broken_sentences += 1\n",
    "    print(f\"   Pages ending mid-sentence: {broken_sentences}/{len(docs)}\")\n",
    "    \n",
    "    # 2. Code blocks?\n",
    "    code_indicators = full_text.count('```') + full_text.count('import ') + full_text.count('def ')\n",
    "    print(f\"   Possible code blocks: {code_indicators > 0}\")\n",
    "    \n",
    "    # 3. Special characters/encoding issues?\n",
    "    weird_chars = len(re.findall(r'[^\\x00-\\x7F]+', full_text))\n",
    "    print(f\"   Non-ASCII characters: {weird_chars}\")\n",
    "    \n",
    "    # 4. Excessive whitespace?\n",
    "    double_spaces = full_text.count('  ')\n",
    "    triple_newlines = full_text.count('\\n\\n\\n')\n",
    "    print(f\"   Double spaces: {double_spaces}, Triple newlines: {triple_newlines}\")\n",
    "    \n",
    "    # 5. Timestamps in transcript?\n",
    "    timestamps = len(re.findall(r'\\d{1,2}:\\d{2}', full_text))\n",
    "    print(f\"   Timestamp patterns: {timestamps}\")\n",
    "    \n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEP DIVE: Data Ingestion And Parsing Techniques.pdf\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ BROKEN SENTENCE EXAMPLE:\n",
      "\n",
      "Page 2 ends with:\n",
      "... embedded and searched.\" \n",
      "‚Ä¢ \n",
      "metadata: a dictionary like: \n",
      "o source: \"example.txt\" \n",
      "o page_number: 1\n",
      "\n",
      "Page 3 starts with:\n",
      "o author: \"Krish\" \n",
      "o date_created: \"2024-01-01\" \n",
      "o (any other relevant fields you want) \n",
      "If you prin...\n",
      "\n",
      "2Ô∏è‚É£ CODE BLOCK EXAMPLE:\n",
      "Found at position 2086:\n",
      " reading different kinds of data. For this, we‚Äôll import some libraries. \n",
      "If a library like pandas is missing, install it (for example: uv add pandas or your chosen \n",
      "package manager). As we add libraries, your pyproject.toml (or requirements) will up\n",
      "\n",
      "3Ô∏è‚É£ NON-ASCII CHARACTERS:\n",
      "\n",
      "Page 1 has: {'‚Äù‚Äî', '‚Äô', '‚Äú', '‚Üí'}\n"
     ]
    }
   ],
   "source": [
    "# Deep dive into ONE document to understand the issues\n",
    "pdf_path = os.path.join(pdf_folder, \"Data Ingestion And Parsing Techniques.pdf\")\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"DEEP DIVE: Data Ingestion And Parsing Techniques.pdf\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Find the broken sentence issue\n",
    "print(\"\\n1Ô∏è‚É£ BROKEN SENTENCE EXAMPLE:\")\n",
    "for i in range(len(docs)-1):\n",
    "    if not docs[i].page_content.strip().endswith(('.', '!', '?', ':')):\n",
    "        print(f\"\\nPage {i+1} ends with:\")\n",
    "        print(f\"...{docs[i].page_content.strip()[-100:]}\")\n",
    "        print(f\"\\nPage {i+2} starts with:\")\n",
    "        print(f\"{docs[i+1].page_content.strip()[:100]}...\")\n",
    "        break\n",
    "\n",
    "# 2. Find code blocks\n",
    "print(\"\\n2Ô∏è‚É£ CODE BLOCK EXAMPLE:\")\n",
    "full_text = \" \".join([doc.page_content for doc in docs])\n",
    "if \"import \" in full_text:\n",
    "    idx = full_text.find(\"import \")\n",
    "    print(f\"Found at position {idx}:\")\n",
    "    print(full_text[idx-50:idx+200])\n",
    "\n",
    "# 3. Find non-ASCII characters\n",
    "print(\"\\n3Ô∏è‚É£ NON-ASCII CHARACTERS:\")\n",
    "import re\n",
    "for i, doc in enumerate(docs[:3]):  # Check first 3 pages\n",
    "    non_ascii = re.findall(r'[^\\x00-\\x7F]+', doc.page_content)\n",
    "    if non_ascii:\n",
    "        print(f\"\\nPage {i+1} has: {set(non_ascii[:5])}\")  # Show first 5 unique\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello guys üëã\n",
      "SEMANTIC FLOW CHECK\n",
      "============================================================\n",
      "\n",
      "Cross-references found:\n",
      "  Forward refs: 1\n",
      "  Back refs: 0\n",
      "  Cross refs: 0\n",
      "\n",
      "Topic markers found:\n",
      "  'Introduction': 2 times\n",
      "  'Overview': 1 times\n",
      "  'Summary': 1 times\n"
     ]
    }
   ],
   "source": [
    "# Check semantic boundaries - How topics flow\n",
    "print(\"Hello guys üëã\")   # <-- added line\n",
    "print(\"SEMANTIC FLOW CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pdf_path = os.path.join(pdf_folder, \"intro_to_rag.pdf\")\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Check if topics are self-contained or reference each other\n",
    "full_text = \" \".join([doc.page_content for doc in docs[:5]])\n",
    "\n",
    "# Look for references to other parts\n",
    "references = {\n",
    "    \"Forward refs\": len(re.findall(r'(later|next video|upcoming|we will see)', full_text, re.I)),\n",
    "    \"Back refs\": len(re.findall(r'(earlier|previous|as mentioned|we saw)', full_text, re.I)),\n",
    "    \"Cross refs\": len(re.findall(r'(see also|refer to|check out)', full_text, re.I))\n",
    "}\n",
    "\n",
    "print(f\"\\nCross-references found:\")\n",
    "for ref_type, count in references.items():\n",
    "    print(f\"  {ref_type}: {count}\")\n",
    "\n",
    "# Check topic density - are there clear topic shifts?\n",
    "print(f\"\\nTopic markers found:\")\n",
    "topic_markers = ['Introduction', 'Overview', 'Summary', 'Conclusion', 'Step', 'Phase']\n",
    "for marker in topic_markers:\n",
    "    count = full_text.count(marker)\n",
    "    if count > 0:\n",
    "        print(f\"  '{marker}': {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned Core Components In RAG.pdf (7 pages)\n",
      "‚úÖ Cleaned Data Ingestion And Parsing Techniques.pdf (36 pages)\n",
      "‚úÖ Cleaned intro_to_rag.pdf (20 pages)\n",
      "‚úÖ Cleaned Vector Embeddings And Vector Databases.pdf (38 pages)\n",
      "‚úÖ Cleaned Vector Stores Vs Vector Databases.pdf (6 pages)\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Define folder (go up one level from notebooks to main project folder)\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "pdf_folder = os.path.join(base_dir, \"pdf\")\n",
    "\n",
    "# Greeting & closing patterns (expandable)\n",
    "greeting_patterns = [\n",
    "    r\"(?i)\\bhello\\s+guys[.! ]*\", \n",
    "    r\"(?i)\\bhi\\s+(everyone|guys|folks)[.! ]*\",\n",
    "    r\"(?i)\\bhey\\s+(everyone|guys)[.! ]*\"\n",
    "]\n",
    "\n",
    "closing_patterns = [\n",
    "    r\"(?i)\\bthank\\s+you[.! ]*\",\n",
    "    r\"(?i)\\bthanks\\s+(for\\s+watching|everyone|guys)[.! ]*\",\n",
    "    r\"(?i)\\bsee\\s+you\\s+(in\\s+the\\s+next\\s+video|soon)[.! ]*\",\n",
    "    r\"(?i)\\bthat'?s\\s+it\\s+for\\s+(this|today'?s)\\s+(video|lesson)[.! ]*\"\n",
    "]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove greetings and closings.\"\"\"\n",
    "    for pattern in greeting_patterns + closing_patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    # Normalize extra spaces\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Loop through all PDFs and clean their text\n",
    "cleaned_docs = {}\n",
    "\n",
    "for pdf_file in os.listdir(pdf_folder):\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        cleaned_texts = [clean_text(doc.page_content) for doc in docs]\n",
    "        cleaned_docs[pdf_file] = \" \".join(cleaned_texts)\n",
    "\n",
    "        print(f\"‚úÖ Cleaned {pdf_file} ({len(docs)} pages)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core Components In RAG.pdf: 2 videos extracted\n",
      "‚úÖ Data Ingestion And Parsing Techniques.pdf: 9 videos extracted\n",
      "‚úÖ intro_to_rag.pdf: 4 videos extracted\n",
      "‚úÖ Vector Embeddings And Vector Databases.pdf: 5 videos extracted\n",
      "‚úÖ Vector Stores Vs Vector Databases.pdf: 0 videos extracted\n",
      "\n",
      "üìò SAMPLE DOCUMENTS PREVIEW\n",
      " ======================================================================\n",
      "\n",
      "=== Core Components In RAG.pdf ===\n",
      "\n",
      "üéûÔ∏è Video Title: Data Ingestion and Parsing\n",
      "   Video Order: 1/2\n",
      "   Topics: Document ingestion, Pre-processing, Chunking, Embeddings, Vector databases,\n",
      "   Difficulty: Beginner\n",
      "   Section: Core Components in RAG\n",
      "   Section Number: 2\n",
      "   Total Videos: 2\n",
      "   Date Created: 2024\n",
      "   Source File: Core Components In RAG.pdf\n",
      "   Characters: 5357\n",
      "\n",
      "üìù Content Preview:\n",
      "So we are going to continue the discussion of Retrieval-Augmented Generation (RAG). In this specific video, we‚Äôll dive into the core components of a RAG pipeline. By now, you already have an intuition for how RAG works at a high level: we have a Large Language Model (LLM), we augment it with externa...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéûÔ∏è Video Title: Query Processing and Output Generation Phase\n",
      "   Video Order: 2/2\n",
      "   Topics: Query processing, Query embedding, Similarity search, Retrieval, Context\n",
      "   Difficulty: Beginner\n",
      "   Section: Core Components in RAG\n",
      "   Section Number: 2\n",
      "   Total Videos: 2\n",
      "   Date Created: 2024\n",
      "   Source File: Core Components In RAG.pdf\n",
      "   Characters: 4103\n",
      "\n",
      "üìù Content Preview:\n",
      "So we are going to continue our discussion with respect to our core components in RAG. We have already discussed about document ingestion and pre-processing. We understood what all specific steps we take in the document ingestion phase, wherein we focus on finding the sources of the data. Then we us...\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Data Ingestion And Parsing Techniques.pdf ===\n",
      "\n",
      "üéûÔ∏è Video Title: Document Structure in LangChain\n",
      "   Video Order: 1/9\n",
      "   Topics: LangChain document structure, Page content vs. metadata, Document loaders,\n",
      "   Difficulty: Beginner\n",
      "   Section: Data Ingestion and Data Parsing Techniques\n",
      "   Section Number: 3\n",
      "   Total Videos: 9\n",
      "   Date Created: 2024\n",
      "   Source File: Data Ingestion And Parsing Techniques.pdf\n",
      "   Characters: 4360\n",
      "\n",
      "üìù Content Preview:\n",
      "vs. metadata, Document loaders, Text splitters, Project setup notes, Why metadata matters Difficulty: Beginner Content So we are going to continue the discussion with respect to data ingestion. Already in our previous video, we created our virtual environment and set up the project structure. Now, i...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéûÔ∏è Video Title: Ingesting and Parsing Text Data Using Document Loaders\n",
      "   Video Order: 2/9\n",
      "   Topics: Text files, TextLoader, DirectoryLoader, Document structure (page_content &\n",
      "   Difficulty: Beginner\n",
      "   Section: Data Ingestion and Data Parsing Techniques\n",
      "   Section Number: 3\n",
      "   Total Videos: 9\n",
      "   Date Created: 2024\n",
      "   Source File: Data Ingestion And Parsing Techniques.pdf\n",
      "   Characters: 4663\n",
      "\n",
      "üìù Content Preview:\n",
      "So we are going to continue the discussion with respect to RAG. Already in our previous video, we have understood the entire document structure inside data ingestion. So first of all, one type of data source files that we are going to read is a text file. What we are going to do here is the simplest...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéûÔ∏è Video Title: Text Splitting Techniques\n",
      "   Video Order: 3/9\n",
      "   Topics: Text splitters, CharacterTextSplitter, RecursiveCharacterTextSplitter,\n",
      "   Difficulty: Beginner\n",
      "   Section: Data Ingestion and Data Parsing Techniques\n",
      "   Section Number: 3\n",
      "   Total Videos: 9\n",
      "   Date Created: 2024\n",
      "   Source File: Data Ingestion And Parsing Techniques.pdf\n",
      "   Characters: 5783\n",
      "\n",
      "üìù Content Preview:\n",
      "So we are going to continue the discussion with respect to our data ingestion pipeline. Already in our previous video, we have seen how to read a .txt file and convert it into a Document using the document loaders available in LangChain. Now, let me go back to the RAG architecture diagram we saw ear...\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== intro_to_rag.pdf ===\n",
      "\n",
      "üéûÔ∏è Video Title: Introduction to RAG\n",
      "   Video Order: 1/4\n",
      "   Topics: RAG fundamentals, Retrieval, Augmentation, Generation, Architecture overview\n",
      "   Difficulty: Beginner\n",
      "   Section: Introduction to RAG\n",
      "   Section Number: 1\n",
      "   Total Videos: 4\n",
      "   Date Created: 2024\n",
      "   Source File: intro_to_rag.pdf\n",
      "   Characters: 9893\n",
      "\n",
      "üìù Content Preview:\n",
      "So I'm super excited to start the series of videos on RAG. In this video and in the upcoming series of videos, we are going to understand everything about retrieval augmented generation, a very important topic currently. We will talk about various architectures. We'll talk about how to build a RAG p...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéûÔ∏è Video Title: Some Examples and Advantages Of Using RAG\n",
      "   Video Order: 2/4\n",
      "   Topics: RAG workflow, Similarity search, Augmentation details, Real-world examples,\n",
      "   Difficulty: Beginner-Intermediate\n",
      "   Section: Introduction to RAG\n",
      "   Section Number: 1\n",
      "   Total Videos: 4\n",
      "   Date Created: 2024\n",
      "   Source File: intro_to_rag.pdf\n",
      "   Characters: 12404\n",
      "\n",
      "üìù Content Preview:\n",
      "So we are going to continue the discussion with respect to RAG already. We got some idea at least to make you just understand what is RAG and how does RAG actually work. One is retrieval augmented generation. One very important thing that I missed out in the previous video is that whenever you make ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéûÔ∏è Video Title: Business Impact Use Cases With RAG\n",
      "   Video Order: 3/4\n",
      "   Topics: Business impact, Cost savings, ROI, Industry adoption, Real case studies\n",
      "   Difficulty: Intermediate\n",
      "   Section: Introduction to RAG\n",
      "   Section Number: 1\n",
      "   Total Videos: 4\n",
      "   Date Created: 2024\n",
      "   Source File: intro_to_rag.pdf\n",
      "   Characters: 2836\n",
      "\n",
      "üìù Content Preview:\n",
      "So we are going to continue our discussion with respect to RAG. Already in our previous video, you know, I gave some of the points where I told that how JPMorgan were able to, you know, save $150 million annually using RAG and all. So what I thought is that why RAG actually matters, you know, and wh...\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Vector Embeddings And Vector Databases.pdf ===\n",
      "\n",
      "üéûÔ∏è Video Title: Introduction To Embeddings And Vector Databases\n",
      "   Video Order: 1/5\n",
      "   Topics: Embedding fundamentals, Vector databases vs traditional databases, Similarity\n",
      "   Difficulty: Beginner ‚Üí Intermediate\n",
      "   Section: Vector Embeddings And Vector Databases\n",
      "   Section Number: 4\n",
      "   Total Videos: 5\n",
      "   Date Created: 2024\n",
      "   Source File: Vector Embeddings And Vector Databases.pdf\n",
      "   Characters: 15621\n",
      "\n",
      "üìù Content Preview:\n",
      "search, Cosine similarity, Feature representations, Model choices (OpenAI & Hugging Face) ‚Äî Overview ‚Äî So we are going to continue the discussion with respect to Rag. ‚Äî Recap: Ingestion & Splitting ‚Äî In our previous video we have seen various data ingestion and parsing techniques. Uh, and the last o...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéûÔ∏è Video Title: Visualization Of Embedding And Cosine Similarity\n",
      "   Video Order: 2/5\n",
      "   Topics: Visualizing embeddings, 2D plots, Cosine similarity, Interpreting similarity scores,\n",
      "   Difficulty: Beginner ‚Üí Intermediate\n",
      "   Section: Vector Embeddings And Vector Databases\n",
      "   Section Number: 4\n",
      "   Total Videos: 5\n",
      "   Date Created: 2024\n",
      "   Source File: Vector Embeddings And Vector Databases.pdf\n",
      "   Characters: 11880\n",
      "\n",
      "üìù Content Preview:\n",
      "High-dimensional embeddings ‚Äî Overview ‚Äî So we are going to continue the discussion with respect to embeddings. Now, uh, already we have understood some of the theoretical explanation about what exactly embeddings is, what exactly embedding models is. And, uh, you know, we also got a brief idea abou...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéûÔ∏è Video Title: Creating Your First Embeddings With HuggingFace Embedding Models\n",
      "   Video Order: 3/5\n",
      "   Topics: Hugging Face overview, Sentence Transformers, LangChain integrations, Embed\n",
      "   Difficulty: Beginner ‚Üí Intermediate\n",
      "   Section: Vector Embeddings And Vector Databases\n",
      "   Section Number: 4\n",
      "   Total Videos: 5\n",
      "   Date Created: 2024\n",
      "   Source File: Vector Embeddings And Vector Databases.pdf\n",
      "   Characters: 17925\n",
      "\n",
      "üìù Content Preview:\n",
      "query vs. embed documents, Model choices & dimensions ‚Äî Overview ‚Äî So we are going to continue the discussion with the topic that is embeddings. And in this specific video we are going to create our first embeddings. Now I hope everybody has got an idea what exactly embeddings is. You know, at the e...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# ‚úÖ Fix: go up one directory from 'notebooks' to main project folder\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "pdf_folder = os.path.join(base_dir, \"pdf\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove greetings, closings, normalize whitespace.\"\"\"\n",
    "    patterns = [\n",
    "        r\"(?i)\\bhello\\s+guys[.! ]*\",\n",
    "        r\"(?i)\\bhi\\s+(everyone|guys|folks)[.! ]*\",\n",
    "        r\"(?i)\\bthank\\s+you[.! ]*\",\n",
    "        r\"(?i)\\bthanks\\s+(for\\s+watching|everyone|guys)[.! ]*\",\n",
    "        r\"(?i)\\bsee\\s+you\\s+(in\\s+the\\s+next\\s+video|soon)[.! ]*\",\n",
    "        r\"(?i)\\bthat'?s\\s+it\\s+for\\s+(this|today'?s)\\s+(video|lesson)[.! ]*\",\n",
    "    ]\n",
    "    for p in patterns:\n",
    "        text = re.sub(p, \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_section_metadata(text):\n",
    "    \"\"\"Extract metadata at the section level.\"\"\"\n",
    "    section_meta = {}\n",
    "    patterns = {\n",
    "        \"section_name\": r\"Section\\s*\\d*:\\s*(.*?)(?:Course|Section\\s*Number|Total|Date|$)\",\n",
    "        \"section_number\": r\"Section\\s*Number:\\s*(\\d+)\",\n",
    "        \"total_videos\": r\"Total\\s*Videos:\\s*(\\d+)\",\n",
    "        \"course_name\": r\"Course:\\s*(.*?)(?:Section|$)\",\n",
    "        \"date_created\": r\"Date\\s*Created:\\s*(\\d+)\",\n",
    "    }\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            section_meta[key] = match.group(1).strip()\n",
    "    return section_meta\n",
    "\n",
    "\n",
    "def extract_video_metadata(text):\n",
    "    \"\"\"Extract metadata inside a single video block.\"\"\"\n",
    "    meta = {}\n",
    "    patterns = {\n",
    "        \"video_title\": r\"Video\\s*\\d+:\\s*([^\\n]*)\",\n",
    "        \"video_order\": r\"Video\\s*Order:\\s*([^\\n]*)\",\n",
    "        \"topics\": r\"Topics:\\s*([^\\n]*)\",\n",
    "        \"difficulty\": r\"Difficulty:\\s*([^\\n]*)\",\n",
    "    }\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            meta[key] = match.group(1).strip()\n",
    "    return meta\n",
    "\n",
    "\n",
    "def strip_metadata_from_content(text):\n",
    "    \"\"\"\n",
    "    Remove any leading metadata section (Video title, Topics, Difficulty, etc.)\n",
    "    including the 'Content' label if present.\n",
    "    \"\"\"\n",
    "    content_match = re.search(r\"(?i)\\bcontent\\b[:\\-]?\", text)\n",
    "    if content_match:\n",
    "        text = text[content_match.end():]\n",
    "    else:\n",
    "        lines = text.splitlines()\n",
    "        cleaned_lines = []\n",
    "        skip_keywords = [\"video\", \"video order\", \"topics\", \"difficulty\"]\n",
    "        for line in lines:\n",
    "            if any(re.match(fr\"(?i)^{kw}\", line.strip()) for kw in skip_keywords):\n",
    "                continue\n",
    "            cleaned_lines.append(line)\n",
    "        text = \"\\n\".join(cleaned_lines)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def load_pdf_videos(pdf_path):\n",
    "    \"\"\"Extract structured Document objects from PDF (only videos).\"\"\"\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    full_text = \" \".join([doc.page_content for doc in docs])\n",
    "    section_meta = extract_section_metadata(full_text)\n",
    "    section_meta[\"source_file\"] = os.path.basename(pdf_path)\n",
    "\n",
    "    # Split by video markers and skip the first (section header)\n",
    "    video_blocks = re.split(r\"(?=Video\\s*\\d+:)\", full_text)[1:]\n",
    "\n",
    "    video_docs = []\n",
    "    for block in video_blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "        meta = extract_video_metadata(block)\n",
    "        text = clean_text(strip_metadata_from_content(block))\n",
    "        metadata = {**section_meta, **meta, \"doc_type\": \"video\"}\n",
    "        video_docs.append(Document(page_content=text, metadata=metadata))\n",
    "\n",
    "    return video_docs\n",
    "\n",
    "\n",
    "# --- PROCESS ALL PDFs ---\n",
    "all_video_docs = []\n",
    "for pdf_file in os.listdir(pdf_folder):\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        video_docs = load_pdf_videos(pdf_path)\n",
    "        all_video_docs.extend(video_docs)\n",
    "        print(f\"‚úÖ {pdf_file}: {len(video_docs)} videos extracted\")\n",
    "\n",
    "\n",
    "# --- PREVIEW SAMPLE DOCUMENTS ---\n",
    "print(\"\\nüìò SAMPLE DOCUMENTS PREVIEW\\n\", \"=\"*70)\n",
    "by_source = {}\n",
    "for doc in all_video_docs:\n",
    "    src = doc.metadata[\"source_file\"]\n",
    "    by_source.setdefault(src, []).append(doc)\n",
    "\n",
    "for src, docs in by_source.items():\n",
    "    print(f\"\\n=== {src} ===\")\n",
    "    for d in docs[:3]:\n",
    "        print(f\"\\nüéûÔ∏è Video Title: {d.metadata.get('video_title', 'N/A')}\")\n",
    "        print(f\"   Video Order: {d.metadata.get('video_order', 'N/A')}\")\n",
    "        print(f\"   Topics: {d.metadata.get('topics', 'N/A')}\")\n",
    "        print(f\"   Difficulty: {d.metadata.get('difficulty', 'N/A')}\")\n",
    "        print(f\"   Section: {d.metadata.get('section_name', 'N/A')}\")\n",
    "        print(f\"   Section Number: {d.metadata.get('section_number', 'N/A')}\")\n",
    "        print(f\"   Total Videos: {d.metadata.get('total_videos', 'N/A')}\")\n",
    "        print(f\"   Date Created: {d.metadata.get('date_created', 'N/A')}\")\n",
    "        print(f\"   Source File: {d.metadata.get('source_file', 'N/A')}\")\n",
    "        print(f\"   Characters: {len(d.page_content)}\")\n",
    "        print(f\"\\nüìù Content Preview:\\n{d.page_content[:300]}...\")\n",
    "        print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'section_name': 'Core Components in RAG', 'section_number': '2', 'total_videos': '2', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'date_created': '2024', 'source_file': 'Core Components In RAG.pdf', 'video_title': 'Data Ingestion and Parsing', 'video_order': '1/2', 'topics': 'Document ingestion, Pre-processing, Chunking, Embeddings, Vector databases,', 'difficulty': 'Beginner', 'doc_type': 'video'}, page_content='So we are going to continue the discussion of Retrieval-Augmented Generation (RAG). In this specific video, we‚Äôll dive into the core components of a RAG pipeline. By now, you already have an intuition for how RAG works at a high level: we have a Large Language Model (LLM), we augment it with external knowledge stored in a vector database, and the LLM uses retrieved context from that database to generate better answers. At a glance, when I provide an input to a plain LLM, it just generates an output from its internal knowledge. In RAG, however, the LLM is connected to a vector database. We store information in that database as vectors (numerical representations). When we send a question, we first retrieve relevant information from the vector database, pass that retrieved context to the LLM, and then the LLM generates a summarized, grounded output using that context. From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1. Document Ingestion Phase 2. Query Processing Phase 3. Generation Phase This video focuses on Phase 1: Document Ingestion and Pre-processing. We‚Äôll break down what data we ingest, how we clean and split it, how we embed it into vectors, and how we store it in a vector database so that later phases (query + generation) can work effectively. What is Document Ingestion and Pre-processing? To power the retriever, we first need a vector database filled with vectors that represent our knowledge. This knowledge can come from multiple sources: company policies, internal documents, PDFs, Word docs, CSVs, websites, databases, images (with extracted text), and more. The ingestion pipeline‚Äôs job is to: ‚Ä¢ Load data from these sources ‚Ä¢ Pre-process the raw text (cleaning, normalization) ‚Ä¢ Split documents into chunks ‚Ä¢ Embed each chunk into a vector ‚Ä¢ Store the vectors (with metadata) in a vector database That‚Äôs the core of Phase 1. Common Data Sources Your data might be: ‚Ä¢ PDFs ‚Ä¢ Word Documents (.docx) ‚Ä¢ Text files (.txt) ‚Ä¢ Spreadsheets (CSV/Excel) ‚Ä¢ Web pages (scraped or via loaders) ‚Ä¢ Databases (records turned into text fields) We will read all of these using loaders, normalize them into a consistent text representation, and then pass them through a standardized pipeline. The Ingestion Pipeline: Step by Step Step 1: Load the Documents Use appropriate loaders per file type to extract text and attach preliminary metadata (e.g., source, filename, section, url). Step 2: Pre-process the Text Light cleaning (remove boilerplate, headers/footers if noisy, fix line breaks, normalize whitespace, optionally remove very long tables or binary debris) so the chunks are clean and meaningful. Step 3: Split into Chunks We don‚Äôt store entire documents as a single vector. We split documents into smaller chunks (e.g., ~500 tokens with some overlap). This matters because every LLM has a context window limit‚Äîonly so much text can be fed to the model at once. Smaller, coherent chunks improve both retrieval accuracy and downstream answer quality. ‚Ä¢ Think of chunks as paragraphs or sections sized for retrieval. ‚Ä¢ Overlap (e.g., 50‚Äì100 tokens) helps preserve continuity across chunk boundaries. Why chunking is necessary: If you try to hand the LLM a thousand-page book as context, it won‚Äôt fit. Even if it did, the relevant part might be buried. Chunking allows the retriever to pinpoint just the passages most relevant to the question, keeping prompts concise and precise. Step 4: Embed Each Chunk We pass each chunk to an embedding model that converts text into a vector (a list of numbers). For example, a sentence might turn into a vector like [0.6, 0.5, 0.4, 0.1, 0.7, ‚Ä¶]. These vectors capture semantic meaning, enabling similarity comparisons. ‚Ä¢ You can use OpenAI embeddings or Hugging Face open-source models. ‚Ä¢ The embedding model‚Äôs job: map semantically similar text to nearby points in vector space. Step 5: Store in a Vector Database We store each vector along with its metadata (e.g., source, section, page_number, chunk_id, timestamp, keywords). Popular vector stores include: ‚Ä¢ ChromaDB ‚Ä¢ FAISS (library) ‚Ä¢ Pinecone (managed) ‚Ä¢ DataStax / Cassandra with vector support This database supports similarity search so we can find the most relevant chunks for a given query later. Similarity Search and Distance Metrics Once vectors are stored, the retriever uses similarity search to find chunks related to a query. Common distance/similarity measures: ‚Ä¢ Cosine similarity (most common for embeddings) ‚Ä¢ Euclidean distance Given a user question like, ‚ÄúWhat is an LLM?‚Äù, the retriever embeds the question into a vector and searches for the nearest vectors in the database. The top-k matching chunks are returned as context for the LLM. End-to-End View of Phase 1 1. Data Sources ‚Üí PDFs, docs, CSVs, websites, databases 2. Loaders + Cleaning ‚Üí normalize and prepare text 3. Split into Chunks ‚Üí manageable, overlapping units 4. Embed Chunks ‚Üí text ‚Üí vectors using an embedding model 5. Store in Vector DB ‚Üí vectors + metadata (for retrieval) When this pipeline completes, you have a ready-to-query vector database. In the next video, we‚Äôll cover Query Processing: how the system embeds an incoming user question, how it performs the similarity search, how we select and format the retrieved context, and how that context flows into the LLM for the Generation Phase.')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_video_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_video_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define chunking parameters\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total chunks created: 226\n"
     ]
    }
   ],
   "source": [
    "# Split all video documents\n",
    "chunked_docs = splitter.split_documents(all_video_docs)\n",
    "\n",
    "print(f\"‚úÖ Total chunks created: {len(chunked_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© SAMPLE CHUNKS PREVIEW\n",
      " ======================================================================\n",
      "\n",
      "üîπ Chunk 1\n",
      "Section: Core Components in RAG\n",
      "Video: Data Ingestion and Parsing\n",
      "Chunk length: 982 chars\n",
      "Metadata keys: ['section_name', 'section_number', 'total_videos', 'course_name', 'date_created', 'source_file', 'video_title', 'video_order', 'topics', 'difficulty', 'doc_type']\n",
      "\n",
      "Content preview:\n",
      "So we are going to continue the discussion of Retrieval-Augmented Generation (RAG). In this specific video, we‚Äôll dive into the core components of a RAG pipeline. By now, you already have an intuition for how RAG works at a high level: we have a Large Language Model (LLM), we augment it with externa...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîπ Chunk 2\n",
      "Section: Core Components in RAG\n",
      "Video: Data Ingestion and Parsing\n",
      "Chunk length: 769 chars\n",
      "Metadata keys: ['section_name', 'section_number', 'total_videos', 'course_name', 'date_created', 'source_file', 'video_title', 'video_order', 'topics', 'difficulty', 'doc_type']\n",
      "\n",
      "Content preview:\n",
      ". From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1. Document Ingestion Phase 2. Query Processing Phase 3. Generation Phase This video focuses on Phase 1: Document Ingestion and Pre-processing. We‚Äôll break down what data we ingest, how we clean ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîπ Chunk 3\n",
      "Section: Core Components in RAG\n",
      "Video: Data Ingestion and Parsing\n",
      "Chunk length: 976 chars\n",
      "Metadata keys: ['section_name', 'section_number', 'total_videos', 'course_name', 'date_created', 'source_file', 'video_title', 'video_order', 'topics', 'difficulty', 'doc_type']\n",
      "\n",
      "Content preview:\n",
      ". This knowledge can come from multiple sources: company policies, internal documents, PDFs, Word docs, CSVs, websites, databases, images (with extracted text), and more. The ingestion pipeline‚Äôs job is to: ‚Ä¢ Load data from these sources ‚Ä¢ Pre-process the raw text (cleaning, normalization) ‚Ä¢ Split d...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîπ Chunk 4\n",
      "Section: Core Components in RAG\n",
      "Video: Data Ingestion and Parsing\n",
      "Chunk length: 914 chars\n",
      "Metadata keys: ['section_name', 'section_number', 'total_videos', 'course_name', 'date_created', 'source_file', 'video_title', 'video_order', 'topics', 'difficulty', 'doc_type']\n",
      "\n",
      "Content preview:\n",
      ". The Ingestion Pipeline: Step by Step Step 1: Load the Documents Use appropriate loaders per file type to extract text and attach preliminary metadata (e.g., source, filename, section, url). Step 2: Pre-process the Text Light cleaning (remove boilerplate, headers/footers if noisy, fix line breaks, ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîπ Chunk 5\n",
      "Section: Core Components in RAG\n",
      "Video: Data Ingestion and Parsing\n",
      "Chunk length: 952 chars\n",
      "Metadata keys: ['section_name', 'section_number', 'total_videos', 'course_name', 'date_created', 'source_file', 'video_title', 'video_order', 'topics', 'difficulty', 'doc_type']\n",
      "\n",
      "Content preview:\n",
      ". ‚Ä¢ Think of chunks as paragraphs or sections sized for retrieval. ‚Ä¢ Overlap (e.g., 50‚Äì100 tokens) helps preserve continuity across chunk boundaries. Why chunking is necessary: If you try to hand the LLM a thousand-page book as context, it won‚Äôt fit. Even if it did, the relevant part might be buried...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- PREVIEW SAMPLE CHUNKS ---\n",
    "print(\"\\nüß© SAMPLE CHUNKS PREVIEW\\n\", \"=\"*70)\n",
    "for i, doc in enumerate(chunked_docs[:5]):\n",
    "    print(f\"\\nüîπ Chunk {i+1}\")\n",
    "    print(f\"Section: {doc.metadata.get('section_name')}\")\n",
    "    print(f\"Video: {doc.metadata.get('video_title')}\")\n",
    "    print(f\"Chunk length: {len(doc.page_content)} chars\")\n",
    "    print(f\"Metadata keys: {list(doc.metadata.keys())}\")\n",
    "    print(f\"\\nContent preview:\\n{doc.page_content[:300]}...\")\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheking the overalp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ OVERLAP CHECK\n",
      " ======================================================================\n",
      "\n",
      "Between Chunk 1 and 2:\n",
      "------------------------------------------------------------\n",
      "üîπ End of Chunk A:\n",
      " ontext to the LLM, and then the LLM generates a summarized, grounded output using that context. From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1\n",
      "\n",
      "üîπ Start of Chunk B:\n",
      " . From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1. Document Ingestion Phase 2. Query Processing Phase 3. Generation Phase This video focuses on\n",
      "------------------------------------------------------------\n",
      "\n",
      "Between Chunk 2 and 3:\n",
      "------------------------------------------------------------\n",
      "üîπ End of Chunk A:\n",
      " rs that represent our knowledge. This knowledge can come from multiple sources: company policies, internal documents, PDFs, Word docs, CSVs, websites, databases, images (with extracted text), and more\n",
      "\n",
      "üîπ Start of Chunk B:\n",
      " . This knowledge can come from multiple sources: company policies, internal documents, PDFs, Word docs, CSVs, websites, databases, images (with extracted text), and more. The ingestion pipeline‚Äôs job \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîÅ OVERLAP CHECK\\n\", \"=\"*70)\n",
    "\n",
    "for i in range(2):\n",
    "    chunk_a = chunked_docs[i].page_content[-200:]\n",
    "    chunk_b = chunked_docs[i+1].page_content[:200]\n",
    "    print(f\"\\nBetween Chunk {i+1} and {i+2}:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"üîπ End of Chunk A:\\n\", chunk_a)\n",
    "    print(\"\\nüîπ Start of Chunk B:\\n\", chunk_b)\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Total tokens across all chunks: 44,347\n",
      "Average tokens per chunk: 196\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")  # or \"text-embedding-3-large\"\n",
    "total_tokens = 0\n",
    "\n",
    "for doc in chunked_docs:\n",
    "    total_tokens += len(encoding.encode(doc.page_content))\n",
    "\n",
    "print(f\"üî¢ Total tokens across all chunks: {total_tokens:,}\")\n",
    "print(f\"Average tokens per chunk: {total_tokens // len(chunked_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding + Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API key loaded: True\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Load Environment Variables ---\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads your .env file automatically\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"‚úÖ OpenAI API key loaded:\", bool(openai_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x0000023036132120>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x0000023038A5E690>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 2: Initialize OpenAI Embedding Model ---\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # accurate + cost-efficient\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Create / Persist ChromaDB Store ---\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "persist_directory = \"chroma_store\"\n",
    "\n",
    "# Create or connect to existing store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunked_docs,      # your 226 chunks\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ChromaDB store created and persisted at: chroma_store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zabo0\\AppData\\Local\\Temp\\ipykernel_24856\\66909357.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# Save to disk so you can reload later\n",
    "vectorstore.persist()\n",
    "print(\"‚úÖ ChromaDB store created and persisted at:\", persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'video_title': 'Data Ingestion and Parsing', 'topics': 'Document ingestion, Pre-processing, Chunking, Embeddings, Vector databases,', 'date_created': '2024', 'video_order': '1/2', 'source_file': 'Core Components In RAG.pdf', 'section_number': '2', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and LangSmith', 'section_name': 'Core Components in RAG', 'difficulty': 'Beginner', 'doc_type': 'video', 'total_videos': '2'}, page_content='. From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1. Document Ingestion Phase 2. Query Processing Phase 3. Generation Phase This video focuses on Phase 1: Document Ingestion and Pre-processing. We‚Äôll break down what data we ingest, how we clean and split it, how we embed it into vectors, and how we store it in a vector database so that later phases (query + generation) can work effectively. What is Document Ingestion and Pre-processing? To power the retriever, we first need a vector database filled with vectors that represent our knowledge. This knowledge can come from multiple sources: company policies, internal documents, PDFs, Word docs, CSVs, websites, databases, images (with extracted text), and more'),\n",
       " Document(metadata={'video_order': '1/4', 'section_name': 'Introduction to RAG', 'date_created': '2024', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and Langsmith', 'difficulty': 'Beginner', 'source_file': 'intro_to_rag.pdf', 'topics': 'RAG fundamentals, Retrieval, Augmentation, Generation, Architecture overview', 'section_number': '1', 'total_videos': '4', 'doc_type': 'video', 'video_title': 'Introduction to RAG'}, page_content=\"So I'm super excited to start the series of videos on RAG. In this video and in the upcoming series of videos, we are going to understand everything about retrieval augmented generation, a very important topic currently. We will talk about various architectures. We'll talk about how to build a RAG pipeline. We'll talk about how to build generative AI with RAG, how to build generative AI applications with RAG. If I talk about the current scenario, more than 80 percentage of the business use cases currently who are working in generative AI and AI, they are building RAG applications. RAG applications. So it is really important for anyone who is specifically interested in generative AI or genetic AI. They need to know how RAG architecture is basically implemented. RAG Architecture Overview Now, with respect to the RAG architecture here, I have shown you one type of architecture where we have three important phases: 1. Document Ingestion phase 2. Query processing phase 3\"),\n",
       " Document(metadata={'video_order': '2/4', 'total_videos': '4', 'section_number': '1', 'topics': 'RAG workflow, Similarity search, Augmentation details, Real-world examples,', 'doc_type': 'video', 'course_name': 'Ultimate RAG Bootcamp Using LangChain, LangGraph and Langsmith', 'difficulty': 'Beginner-Intermediate', 'source_file': 'intro_to_rag.pdf', 'date_created': '2024', 'video_title': 'Some Examples and Advantages Of Using RAG', 'section_name': 'Introduction to RAG'}, page_content=\". So these are the three main important things with respect to any RAG architecture. Why Does RAG Actually Matter? Now let me ask one very important question. And this question is just like everybody will be probably thinking about it. Why does RAG actually matter? You know, why is RAG really helpful? I'll talk about various companies who are able to just do some amazing things with respect to RAG, you know, are they able to save huge amount of money? Bigger companies like JP Morgan and all. And I will probably share you the entire links also if you want. But amazing work with the help of RAG they are actually able to do. Restaurant Chef Analogy Now let's take one simple example and let me explain more about RAG because it is important that you understand with a simple example itself. Let's say, and then we'll try to also compare this with LLM. Let's say there is a restaurant. So I will just try to draw some restaurant over here. So let's say this is my restaurant\")]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 4: Test Retrieval ---\n",
    "query = \"What are the main phases in a RAG system?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Full RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zabo0\\AppData\\Local\\Temp\\ipykernel_24856\\637535988.py:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 3 relevant documents\n",
      "\n",
      "Result 1: Data Ingestion and Parsing (Core Components in RAG)\n",
      ". From the architectural diagram we discussed earlier, there are three main phases in a full RAG system: 1. Document Ingestion Phase 2. Query Processing Phase 3. Generation Phase This video focuses on Phase 1: Document Ingestion and Pre-processing. We‚Äôll break down what data we ingest, how we clean  ...\n",
      "\n",
      "Result 2: Some Examples and Advantages Of Using RAG (Introduction to RAG)\n",
      ". So these are the three main important things with respect to any RAG architecture. Why Does RAG Actually Matter? Now let me ask one very important question. And this question is just like everybody will be probably thinking about it. Why does RAG actually matter? You know, why is RAG really helpfu ...\n",
      "\n",
      "Result 3: Introduction to RAG (Introduction to RAG)\n",
      "So I'm super excited to start the series of videos on RAG. In this video and in the upcoming series of videos, we are going to understand everything about retrieval augmented generation, a very important topic currently. We will talk about various architectures. We'll talk about how to build a RAG p ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: Create the Retriever ---\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}   # number of chunks to retrieve\n",
    ")\n",
    "\n",
    "# Test the retriever alone (no LLM yet)\n",
    "query = \"What are the three main phases in a RAG system?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"‚úÖ Retrieved {len(retrieved_docs)} relevant documents\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Result {i}: {doc.metadata.get('video_title')} ({doc.metadata.get('section_name')})\")\n",
    "    print(doc.page_content[:300], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Memory ok. Messages: [HumanMessage(content='What are the main components of a RAG pipeline?', additional_kwargs={}, response_metadata={}), AIMessage(content='Retrieval, augmentation, and generation.', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zabo0\\AppData\\Local\\Temp\\ipykernel_24856\\2636491970.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Rebuild memory with the correct keys\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",   # <- matches ConversationalRetrievalChain input\n",
    "    output_key=\"answer\",    # <- the field we want stored\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Optional quick test (use matching keys!)\n",
    "memory.save_context(\n",
    "    {\"question\": \"What are the main components of a RAG pipeline?\"},\n",
    "    {\"answer\": \"Retrieval, augmentation, and generation.\"}\n",
    ")\n",
    "print(\"üß† Memory ok. Messages:\", memory.load_memory_variables({})[\"chat_history\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt template created successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 3: Build Prompt Template ---\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert study assistant for the 'Ultimate RAG Bootcamp'.\n",
    "Use ONLY the information from the provided context to answer the student's question.\n",
    "If the answer is not found in the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Chat history:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a clear, educational, and concise way.\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\", \"chat_history\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Prompt template created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 4: Initialize the LLM ---\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",   # you can also try \"gpt-4-turbo\"\n",
    "    temperature=0,         # 0 = factual, deterministic answers\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG Conversational Chain created successfully (with memory fix).\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 5 (fixed): Build Conversational Retrieval Chain ---\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": rag_prompt},\n",
    "    return_source_documents=True,\n",
    "    output_key=\"answer\"   # ‚úÖ tell it to store only the answer in memory\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG Conversational Chain created successfully (with memory fix).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† QUESTION: How to load pdf files?\n",
      "\n",
      "üí¨ ANSWER:\n",
      " You can load PDF files using different methods in LangChain. Here are two common approaches:\n",
      "\n",
      "1. **PyPDFLoader**:\n",
      "   - Create a loader with the file path (e.g., `data/pdf/attention.pdf`).\n",
      "   - Call `.load()` to get a list of Document objects, one for each page.\n",
      "   - Each Document contains:\n",
      "     - `page_content`: extracted text for that page.\n",
      "     - `metadata`: includes fields like page number, source, creator, producer, author, timestamps, etc.\n",
      "   - After loading, you can print diagnostics such as the number of pages and a preview of the first page.\n",
      "\n",
      "2. **PyMuPDFLoader**:\n",
      "   - You may need to install the `pymupdf` library first.\n",
      "   - Create the loader with the same file and call `.load()`.\n",
      "   - Inspect the returned Documents and metadata.\n",
      "   - This method is generally fast, offers robust text extraction, and supports image extraction and richer PDF features.\n",
      "\n",
      "A third method, **UnstructuredPDFLoader**, will be discussed later and is useful for handling messy PDFs with complex layouts.\n",
      "\n",
      "üìö SOURCES:\n",
      "\n",
      "--- Source 1 ---\n",
      "From: Data Ingestion And Parsing Techniques.pdf\n",
      "Section: Data Ingestion and Data Parsing Techniques\n",
      "Video: Ingestion and Parsing PDF Documents\n",
      "Content preview: . After loading, print diagnostics: ‚Ä¢ Number of pages (e.g., 15 pages loaded) ‚Ä¢ Page 1 preview ‚Üí first 100 characters ‚Ä¢ Metadata ‚Üí show how much information is automatically captured This is an easy way to read a PDF and immediately get page-oriented...\n",
      "\n",
      "--- Source 2 ---\n",
      "From: Data Ingestion And Parsing Techniques.pdf\n",
      "Section: Data Ingestion and Data Parsing Techniques\n",
      "Video: Ingestion and Parsing PDF Documents\n",
      "Content preview: .g., data_parsing_pdf.ipynb, because PDF handling is a very common real-world requirement. Loading PDFs with LangChain (Approaches) We will import loaders from langchain_community.document_loaders and demonstrate three approaches (we will implement t...\n"
     ]
    }
   ],
   "source": [
    "query = \"How to load pdf files?\"\n",
    "response = rag_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"üß† QUESTION:\", query)\n",
    "print(\"\\nüí¨ ANSWER:\\n\", response[\"answer\"])\n",
    "\n",
    "print(\"\\nüìö SOURCES:\")\n",
    "for i, doc in enumerate(response[\"source_documents\"][:2]):\n",
    "    print(f\"\\n--- Source {i+1} ---\")\n",
    "    print(f\"From: {doc.metadata.get('source_file')}\")\n",
    "    print(f\"Section: {doc.metadata.get('section_name')}\")\n",
    "    print(f\"Video: {doc.metadata.get('video_title')}\")\n",
    "    print(f\"Content preview: {doc.page_content[:250]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
